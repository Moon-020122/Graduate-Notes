# 卷积网络

## 1.1-卷积运算

### 边缘检测

​	边缘检测就是从一个小边缘到整体图像，比如要在一个图片中进行垂直检测或者水平检测，绘画出图像的轮廓。

![img](images/clip_image002-17204413950371.jpg)

### 边缘检测的步骤

1. **输入图像**：

   - 假设我们有一个 ($6 \times 6$) 的灰度图像，表示为 ($6 \times 6 \times 1$)，其中每个像素点的值表示灰度级别。

2. **创建过滤器（滤波器）**：

   - 为了检测垂直边缘，我们需要创建一个 ($3 \times 3$) 的过滤器（也称为核）。一个常见的垂直边缘检测过滤器是Sobel过滤器：
     $$
     \begin{bmatrix}
     -1 & 0 & 1 \\
     -2 & 0 & 2 \\
     -1 & 0 & 1
     \end{bmatrix}
     $$
     
- 类似地，水平边缘检测过滤器可以是：
     $$
     \begin{bmatrix}
     -1 & -2 & -1 \\
      0 &  0 &  0 \\
      1 &  2 &  1
     \end{bmatrix}
     $$
     **卷积操作**：
   
  - 将 ($3 \times 3$) 的过滤器与 ($6 \times 6$) 的图像进行卷积操作。卷积的过程是将过滤器在图像上滑动，并计算每个位置的点积。
     - 具体来说，将过滤器放在图像的一个位置，计算过滤器与图像对应区域的点积，然后将结果作为输出图像的一个像素值。
  
  **得到边缘检测结果**：
  
  - 完成卷积操作后，我们得到一个新的图像，这个图像包含了垂直边缘的信息。同样地，我们可以使用水平边缘检测过滤器进行卷积，得到水平边缘的信息。

![img](images/clip_image004-17204413950372.jpg)

​	水平和竖直过滤器

![img](images/clip_image006-17204413950383.jpg)

### 正边缘和负边缘

即由亮变暗和由暗变量的边缘过渡。

1. **正边缘**：
   - 正边缘是指从亮到暗的过渡。在图像中，这种边缘通常表示亮的区域突然变成暗的区域。
   - 例如，在一个白色背景上有一个黑色物体的边缘，这个边缘就是正边缘。
2. **负边缘**：
   - 负边缘是指从暗到亮的过渡。在图像中，这种边缘通常表示暗的区域突然变成亮的区域。
   - 例如，在一个黑色背景上有一个白色物体的边缘，这个边缘就是负边缘。

![img](images/clip_image008-17204413950385.jpg)

### 几种其他的过滤器

$Sobel filter$：给予了中间行赋予了更大的权重，从而使得其更加稳定,在上文中有提及。

$Scharr filter$：$Scharr$过滤器是$Sobel$过滤器的改进版本，特别适用于检测图像中的细微边缘。它在计算梯度时更加精确，尤其是在噪声较多的图像中表现更好。典型的$Scharr$过滤器如下：

垂直边缘检测:
$$
\begin{bmatrix}
-3 & 0 & 3 \\
-10 & 0 & 10 \\
-3 & 0 & 3
\end{bmatrix}
$$
水平边缘检测：
$$
\begin{bmatrix}
-3 & -10 & -3 \\
 0 &   0 &  0 \\
 3 &  10 &  3
\end{bmatrix}
$$


### 反向传播学习过滤器参数

​	如果想要检测一些复杂的图形边界，可能并不需要挑选出这9个矩阵元素，可以把矩阵里的这9个元素当做参数，通过反向传播学习，得到他们的值，目的是要获得这9个参数使得对于这个6*6的图片卷积3*3的过滤器可以得到一个优良的边检检测器，反向传播学习选择得到上述3个过滤器，除了垂直边界和水平边界，也可以学习到检测45°边界等多个随意度数的边界过滤器，因此这样使得通告反向传播可以学习到任何所需的3*3的过滤器。

在卷积神经网络（CNN）中，我们可以通过反向传播算法来学习过滤器的参数，而不需要手动设计过滤器。这种方法的优势在于：

1. **自动化学习**：通过反向传播，神经网络可以自动调整过滤器的参数，以最小化损失函数，从而学习到最优的边缘检测器。
2. **灵活性**：除了垂直和水平边缘，网络还可以学习到检测任意角度的边缘（如45°边缘）以及复杂的图形边界。
3. **适应性**：网络可以根据具体的任务和数据集，学习到最适合的过滤器，从而提高检测精度。

​	初始的 ($3 \times 3$) 过滤器可能是随机初始化的：
$$
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
$$


通过训练，网络会调整这些参数 $(a, b, c, d, e, f, g, h, i) $以最小化损失函数，从而学习到最优的边缘检测器。

![img](images/clip_image010-17204413950384.jpg)

### 填充

​	为构建深层神经网络，对基本的卷积操作的改进是填充$(padding)$,当一个$n*n$的图片被一个$f*f$的过滤器卷积，得到的是一个$(n-f+1)*(n-f+1)$的图像。而这会导致两个缺陷，每一次使用一次卷积操作，则图片会缩小；通过在图像边缘添加填充，我们可以保持图像的尺寸。使用大小为$P$像素的填充，可以使卷积后的图像尺寸变为$((n+2p-f+1) \times (n+2p-f+1))$。这样，填充可以有效地防止图像尺寸过度缩小。

​	在没有填充的情况下，图像边缘和角落的像素只会被卷积核覆盖一次，而图像中间的像素会被卷积核多次覆盖。这会导致边界和角落的信息在卷积过程中被忽略或丢失。通过添加填充，我们可以确保卷积核在图像边缘和角落也能多次覆盖这些像素，从而保留更多的边界信息。	

![image-20240709190641503](images/image-20240709190641503.png)

关于填充的数量，我们通常有两个选择，则vaild卷积和same卷积。 

通常有两种填充方式：valid卷积和same卷积。

- **Valid卷积**：没有填充，输出图像尺寸会缩小。
- **Same卷积**：填充使得输出图像尺寸与输入图像尺寸相同。对于大小为$(f \times f)$的卷积核，填充的数量$(p)$可以通过公式$p = \frac{f-1}{2}$计算得出。当(f)为奇数时，(p)是一个整数，这样可以确保输入和输出图像尺寸相同。

卷积核通常选择奇数大小（如3x3、5x5等），主要有以下几个原因：

- **中心点的存在**：奇数大小的卷积核有一个明确的中心点，使得卷积操作在图像的每个位置都能对称地应用卷积核。

- **填充计算的简化**：当卷积核大小为奇数时，填充数量(p)是整数，计算更加简便。

- **对称性和边界处理**：奇数大小的卷积核在处理图像边界时更加对称，有助于保留边界信息。

### 卷积步长

​	带步长卷积是指在进行卷积操作时，卷积核每次移动的步长（stride）大于1。假设输入图像的尺寸为$(n \times n)$，卷积核的尺寸为$(f \times f)$，填充为$(p)$，步长为$(S)$，在计算输出尺寸时，如果分数的结果不是整数，我们需要向下取整。因为卷积核在图像上移动时，必须完全覆盖图像的区域。如果不向下取整，卷积核可能会超出图像的边界，导致无法进行有效的卷积操作。则输出图像的尺寸可以通过以下公式计算：
$$
\text{输出尺寸} = \left\lfloor \frac{n + 2p - f}{S} \right\rfloor + 1
$$
![image-20240709191624573](images/image-20240709191624573.png)

![image-20240709191629416](images/image-20240709191629416.png)

### 交叉相关与卷积

- **卷积（Convolution）**：在信号处理中，卷积操作通常包括将一个信号（或图像）与一个过滤器（或核）进行卷积。这个过程包括将过滤器翻转，然后在信号上滑动并计算点积。
- **交叉相关（Cross-Correlation）**：在深度学习中，交叉相关操作与卷积类似，但没有翻转过滤器。过滤器直接在输入信号上滑动并计算点积。

在深度学习中，卷积层的主要目的是提取特征，而不是进行信号处理中的严格数学操作而在深度学习中更常用交叉相关,而且使用交叉相关而不是严格的卷积并不会影响深度学习模型的性能：

- **计算效率**：交叉相关省略了过滤器翻转的步骤，计算上更为简单和高效。
- **实现简便**：在实际实现中，交叉相关更容易理解和实现，特别是在使用深度学习框架（如$TensorFlow$、$PyTorch$）时。
- **特征提取一致性**：在深度学习中，卷积层的主要目的是提取特征，而不是进行严格的数学卷积操作。交叉相关在特征提取方面表现良好，并且在实践中效果显著。

​	尽管技术上深度学习中进行的是交叉相关操作，但由于历史原因和约定俗成的命名，我们仍然称之为“卷积”。这种命名方式已经被广泛接受，并且在文献和实践中普遍使用。

### 三维卷积

​	在二维卷积中，我们处理的是单通道的灰度图像，而在三维卷积中，我们处理的是多通道的彩色图像。彩色图像通常有三个通道：红色（R）、绿色（G）和蓝色（B）。三维卷积的过滤器不仅在空间维度上滑动，还在通道维度上进行操作。过滤器的通道数必须与输入图像的通道数相同。这是因为每个通道都包含不同的颜色信息，过滤器需要在每个通道上进行卷积操作，以提取特征。例如，对于一个RGB图像，过滤器的通道数也必须是3，以便在红色、绿色和蓝色通道上分别进行卷积。

​	通过设定RGB三个维度的不同数字，我们可以调节需要检测的颜色。例如，如果我们希望检测红色特征，可以在过滤器的红色通道上设置较高的权重，而在绿色和蓝色通道上设置较低的权重。这样，过滤器在卷积操作中会更关注红色通道的信息。如果我们希望仅仅检测边界而不在意颜色，可以在所有通道上设置相同的权重。这样，过滤器在卷积操作中会均匀地处理所有颜色通道，从而提取边界特征，而不偏向任何特定的颜色。尽管输入图像有多个通道，三维卷积的输出通常是一维的。这是因为卷积操作会将多通道的信息融合在一起，生成一个综合的特征图。这个特征图包含了所有通道的信息，并且可以用于后续的处理和分析。

​	假设我们有一个大小为$(32 \times 32 \times 3)$的RGB图像，使用一个大小为$(5 \times 5 \times 3)$的过滤器进行卷积。过滤器在每个通道上分别进行卷积操作，然后将结果相加，生成一个大小为$(28 \times 28 \times 1)$的特征图。

![image-20240709192253829](images/image-20240709192253829.png)

​	在卷积神经网络中，我们可以使用多个过滤器来检测不同的特征。例如，一个过滤器可以用来检测垂直边缘，另一个过滤器可以用来检测水平边缘，还有其他过滤器可以用来检测不同角度的边缘。每个过滤器在图像上滑动并计算点积，生成一个特征图，将这些特征图叠加在一起，我们可以得到一个包含所有过滤器输出的多通道特征图。这些特征图的叠加可以捕捉到图像中的不同特征，从而提高模型的识别能力。

​	假设输入图像的尺寸为$(n \times n \times n_c)$（其中$(n_c)$是通道数），过滤器的尺寸为$(f \times f \times n_c)$，步长为$(S)$，填充为$(p)$。每个过滤器会生成一个大小为$(\frac {(n+2p-f)}S + 1 \times \frac {(n+2p-f)}S + 1)$的特征图。

​	如果我们使用(k)个过滤器，每个过滤器都会生成一个特征图。将这些特征图叠加在一起，输出的特征图的尺寸为$((\frac {(n+2p-f)}S + 1) \times (\frac {(n+2p-f)}S + 1) \times k)$。

![image-20240709192854875](images/image-20240709192854875.png)

## 1.2-卷积网络层

​	在卷积层中，过滤器的作用和$w^{[1]}$的作用相似，之前我们在卷积计算中，我们有27个输入，或者确切的说，是两组27个输入，因为我们有两个过滤器，我们需要把上面这些数字相乘，其实就是通过一个线性方程计算得到了一个$4\times4$的矩阵，因此这里通过卷积得到的的$4\times4$矩阵的过程和$w^{[1]}*a^{[0]}$的过程类似，然后添加偏差值$b$，因此下图方框中，添加了偏差的式子的作用和$z$类似，最后应用非线性方程，此处的输出就成了下一层神经网络的激活函数。

![image-20240709193352999](images/image-20240709193352999.png)

### 卷积层的符号标识

​	1. 卷积层的参数

- **$( f^{[l]} )$**：过滤器的大小（filter size），即卷积核的尺寸。

- **$( p^{[l]} )$**：填充（padding），即在输入图像的边缘添加的像素数。

- **$( s^{[l]} )$**：步长（stride），即卷积核在图像上滑动的步长。

- **$( n_c^{[l]} )$**：过滤器的数量（number of filters），即卷积层中使用的卷积核的数量。

​	2. 激活函数的维度

- **高度（Height）**：$( n_H^{[l]} = \left\lfloor \frac{n_{H_{prev}} + 2p^{[l]} - f{[l]}}{s{[l]}} + 1 \right\rfloor )$
- **宽度（Width）**：$( n_W^{[l]} = \left\lfloor \frac{n_{W_{prev}} + 2p^{[l]} - f{[l]}}{s{[l]}} + 1 \right\rfloor )$

​	3. 过滤器的尺寸

- 每个过滤器的尺寸为 $( f^{[l]} \times f^{[l]} \times n_c^{[l-1]} )$，其中$ ( n_c^{[l-1]} ) $是前一层的通道数。

​	4. 权重和偏置

- **权重（Weights）**：$( W_1, W_2, …, W_{n_c^{[l]}} )$，表示每个过滤器的权重。
- **偏置（biases）**：$( b_1, b_2, …, b_{n_c^{[l]}} )$，表示每个过滤器的偏置项。

​	**输入和输出维度**

- **输入维度**：$( n_H^{[l-1]} \times n_W^{[l-1]} \times n_c^{[l-1]} )$
- **输出维度**：$( n_H^{[l]} \times n_W^{[l]} \times n_c^{[l]} )$

​		这些维度表示输入图像和输出特征图的高度、宽度和通道数。

- $A^{[L]}$是进行批量化或者矩阵化后的激活函数。

![image-20240709193540568 ](images/image-20240709193540568.png)

### 卷积网络的简单示例

​	每个卷积层通常会使用多个过滤器，每个过滤器生成一个特征图。随着卷积层的增加，过滤器的数量（即通道数）也会增加。

​	在卷积层和池化层之后，特征图会被扁平化为一个向量，然后输入到全连接层（Fully Connected Layer）中进行分类。全连接层通过线性变换和激活函数，将特征向量映射到输出类别。

​	随着卷积神经网络层数的增加，图像的空间尺寸（高度和宽度）会逐渐减小，而通道数（深度）会逐渐增加。这是因为卷积和池化操作会减小特征图的尺寸，而更多的过滤器会提取更多的特征。最终，特征图会被扁平化为一个向量，输入到全连接层中进行分类。

![image-20240709194053666](images/image-20240709194053666.png)

**1. 卷积层（Convolutional Layer, Conv）**

**作用**：卷积层是CNN的核心组件，用于提取输入图像的特征。通过卷积操作，卷积层能够捕捉到图像中的局部特征，如边缘、纹理和形状。

**命名原因**：卷积层之所以称为“卷积层”，是因为它执行的主要操作是卷积（convolution）。卷积操作包括使用多个过滤器（filters）在输入图像上滑动，计算局部区域的加权和，从而生成特征图（feature maps）。

**2. 池化层（Pooling Layer, Pool）**

**作用**：池化层用于减小特征图的尺寸，同时保留重要的特征。常见的池化操作包括最大池化（Max Pooling）和平均池化（Average Pooling）。池化层有助于减少计算量和参数数量，并引入一定程度的平移不变性。

**命名原因**：池化层之所以称为“池化层”，是因为它执行的主要操作是池化（pooling）。池化操作通过在局部区域内取最大值或平均值，来减小特征图的尺寸。

**3. 全连接层（Fully Connected Layer, FC）**

**作用**：全连接层通常位于卷积神经网络的末端，用于高层次的特征表示和分类。全连接层中的每个神经元与前一层的所有神经元相连，类似于传统的多层感知器（MLP）。

**命名原因**：全连接层之所以称为“全连接层”，是因为它的每个神经元与前一层的所有神经元都有连接。这种全连接的结构使得全连接层能够综合前面卷积层和池化层提取的特征，用于最终的分类任务。

![image-20240709194506060](images/image-20240709194506060.png)
