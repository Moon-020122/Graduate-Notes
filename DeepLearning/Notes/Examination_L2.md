# ç¥ç»ç½‘ç»œåˆå§‹åŒ–

æ¬¢è¿æ¥åˆ°â€œæ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œâ€çš„ç¬¬ä¸€é¡¹ä½œä¸šã€‚

è®­ç»ƒç¥ç»ç½‘ç»œéœ€è¦æŒ‡å®šæƒé‡çš„åˆå§‹å€¼ï¼Œè€Œä¸€ä¸ªå¥½çš„åˆå§‹åŒ–æ–¹æ³•å°†æœ‰åŠ©äºç½‘ç»œå­¦ä¹ ã€‚

å¦‚æœä½ å®Œæˆäº†æœ¬ç³»åˆ—çš„ä¸Šä¸€è¯¾ç¨‹ï¼Œåˆ™å¯èƒ½å·²ç»æŒ‰ç…§æˆ‘ä»¬çš„è¯´æ˜å®Œæˆäº†æƒé‡åˆå§‹åŒ–ã€‚ä½†æ˜¯ï¼Œå¦‚ä½•ä¸ºæ–°çš„ç¥ç»ç½‘ç»œé€‰æ‹©åˆå§‹åŒ–ï¼Ÿåœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œä½ èƒ½å­¦ä¹ çœ‹åˆ°ä¸åŒçš„åˆå§‹åŒ–å¯¼è‡´çš„ä¸åŒç»“æœã€‚

å¥½çš„åˆå§‹åŒ–å¯ä»¥ï¼š

- åŠ å¿«æ¢¯åº¦ä¸‹é™ã€æ¨¡å‹æ”¶æ•›
- å‡å°æ¢¯åº¦ä¸‹é™æ”¶æ•›è¿‡ç¨‹ä¸­è®­ç»ƒï¼ˆå’Œæ³›åŒ–ï¼‰å‡ºç°è¯¯å·®çš„å‡ ç‡

é¦–å…ˆï¼Œè¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ä»¥åŠ è½½åŒ…å’Œç”¨äºåˆ†ç±»çš„äºŒç»´æ•°æ®é›†ã€‚

å¯¹äºload_dataset()å‡½æ•°è§£æ

```PYTHON
def load_dataset():
    np.random.seed(1)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)# ç”Ÿæˆ300ä¸ªè®­ç»ƒæ ·æœ¬
    np.random.seed(2)
    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)
    # train_X[:, 0] å’Œ train_X[:, 1] ä»£è¡¨æ•°æ®ç‚¹çš„ x å’Œ y åæ ‡ï¼Œc=train_Y è¡¨ç¤ºé¢œè‰²ï¼Œæ ¹æ®train_Yä¸ºä¸åŒç±»åˆ«çš„ç‚¹ç€è‰²ï¼Œs=40ç‚¹çš„å¤§å°ï¼Œcmap=plt.cm.Spectral æŒ‡å®šäº†ä¸€ä¸ªé¢œè‰²æ˜ å°„ï¼Œç”¨äºç»™ä¸åŒç±»åˆ«çš„ç‚¹ç€ä¸åŒçš„é¢œè‰²ã€‚
    #train_X[:, 0] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬0åˆ—çš„å…ƒç´ ã€‚è¿™é‡Œçš„ : è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œè€Œ 0 è¡¨ç¤ºé€‰æ‹©ç¬¬0åˆ—ã€‚train_X æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œç¬¬0åˆ—å’Œç¬¬1åˆ—åˆ†åˆ«ä»£è¡¨æ•°æ®ç‚¹çš„xåæ ‡å’Œyåæ ‡ã€‚
 #    [[x1, y1],
 #    [x2, y2],
 #    [x3, y3],
 #        ...]
    #é‚£ä¹ˆ train_X[:, 0] å°†ä¼šæ˜¯ [x1, x2, x3, ...]ï¼Œè€Œ train_X[:, 1] å°†ä¼šæ˜¯ [y1, y2, y3, ...]ã€‚
train_X[:, 1] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬1åˆ—çš„å…ƒç´ ã€‚åŒæ ·åœ°ï¼Œ: è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œ1 è¡¨ç¤ºé€‰æ‹©ç¬¬1åˆ—ã€‚
    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);
    train_X = train_X.T
    train_Y = train_Y.reshape((1, train_Y.shape[0]))
    test_X = test_X.T
    test_Y = test_Y.reshape((1, test_Y.shape[0]))
    return train_X, train_Y, test_X, test_Y
```

inï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
from lib.init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation
from lib.init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec


plt.rcParams['figure.figsize'] = (7.0, 4.0) # å›¾è¡¨çš„é»˜è®¤å¤§å°è®¾ç½®ä¸ºå®½7è‹±å¯¸ã€é«˜4è‹±å¯¸ã€‚
plt.rcParams['image.interpolation'] = 'nearest'#å°†å›¾åƒç¼©æ”¾æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•è®¾ç½®ä¸ºâ€™nearestâ€™ï¼Œæ„å‘³ç€åœ¨ç¼©æ”¾å›¾åƒçš„åŒºåŸŸå†…ï¼Œå°†ä½¿ç”¨åŸå§‹åƒç´ å€¼ï¼Œè¿™ä¼šå¯¼è‡´å›¾åƒçœ‹èµ·æ¥åƒå—çŠ¶ï¼Œæ²¡æœ‰å¹³æ»‘å¤„ç†ã€‚
plt.rcParams['image.cmap'] = 'gray' #å°†å›¾åƒçš„é»˜è®¤è‰²å½©æ˜ å°„è®¾ç½®ä¸ºâ€™grayâ€™ï¼Œå³ä»¥ç°åº¦æ¨¡å¼æ˜¾ç¤ºå›¾åƒã€‚

# load image dataset: blue/red dots in circles
train_X, train_Y, test_X, test_Y = load_dataset() #ä¸Šæ–¹è§£æ
plt.show()
```

output:

![image-20240610135848779](images/image-20240610135848779.png)

ä½¿ç”¨åˆ†ç±»å™¨å¸Œæœ›å°†è“ç‚¹å’Œçº¢ç‚¹åˆ†å¼€ã€‚

## 1-ç¥ç»ç½‘ç»œæ¨¡å‹

ä½¿ç”¨å·²ç»å®ç°äº†çš„3å±‚ç¥ç»ç½‘ç»œã€‚å°†å°è¯•çš„åˆå§‹åŒ–æ–¹æ³•ï¼š

- *é›¶åˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "zeros"`ã€‚
- *éšæœºåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "random"`ï¼Œè¿™ä¼šå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ã€‚
- *Heåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "he"`ï¼ŒHeåˆå§‹åŒ–ã€‚

**è¯´æ˜**ï¼šè¯·å¿«é€Ÿé˜…è¯»å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œåœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œä½ å°†å®ç°æ­¤`model()`è°ƒç”¨çš„ä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ã€‚

```PYTHON
def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
    learning_rate -- learning rate for gradient descent 
    num_iterations -- number of iterations to run gradient descent
    print_cost -- if True, print the cost every 1000 iterations
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
    Returns:
    parameters -- parameters learnt by the model
    """
        
    grads = {} # to keep track of the gradients æ¢¯åº¦
    costs = [] # to keep track of the loss æŸå¤±å‡½æ•°
    m = X.shape[1] # number of examples  æ ·æœ¬æ•°
    layers_dims = [X.shape[0], 10, 5, 1] #ä¸‰å±‚ç¥ç»ç½‘ç»œ
    
    # Initialize parameters dictionary.
    if initialization == "zeros":
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == "random":
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == "he":
        parameters = initialize_parameters_he(layers_dims)

    # Loop (gradient descent)
    #æ¢¯åº¦ä¸‹é™ç®—æ³•
    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        a3, cache = forward_propagation(X, parameters) #å‰å‘ä¼ æ’­
        
        # Loss
        cost = compute_loss(a3, Y)#æŸå¤±å‡½æ•°

        # Backward propagation.
        grads = backward_propagation(X, Y, cache) #åå‘ä¼ æ’­
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)#æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
            costs.append(cost)
            
    # plot the loss
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```

### 1.1-é›¶åˆå§‹åŒ–

åœ¨ç¥ç»ç½‘ç»œä¸­æœ‰ä¸¤ç§ç±»å‹çš„å‚æ•°è¦åˆå§‹åŒ–ï¼š

- æƒé‡çŸ©é˜µ$(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$
- åå·®å‘é‡ $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$

**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ä»¥å°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ã€‚ ç¨åä½ ä¼šçœ‹åˆ°æ­¤æ–¹æ³•ä¼šæŠ¥é”™ï¼Œå› ä¸ºå®ƒæ— æ³•â€œæ‰“ç ´å¯¹ç§°æ€§â€ã€‚æ€»ä¹‹å…ˆå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç¡®ä¿ä½¿ç”¨æ­£ç¡®ç»´åº¦çš„np.zerosï¼ˆï¼ˆ..ï¼Œ..ï¼‰ï¼‰ã€‚

â€‹	åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœå°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¼šå¯¼è‡´ä¸€ä¸ªé—®é¢˜ç§°ä¸ºâ€œå¯¹ç§°æ€§ç ´åâ€ã€‚æ„å‘³ç€åœ¨ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œæ‰€æœ‰ç¥ç»å…ƒçš„æƒé‡æ›´æ–°å°†æ˜¯ç›¸åŒçš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¦‚æœæƒé‡ç›¸åŒï¼Œé‚£ä¹ˆåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒæ¥æ”¶åˆ°çš„è¾“å…¥å’Œæ¢¯åº¦å°†æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­æ—¶å®ƒä»¬çš„æƒé‡æ›´æ–°ä¹Ÿä¼šç›¸åŒã€‚è¿™æ ·ï¼Œæ— è®ºç½‘ç»œæœ‰å¤šå°‘å±‚æˆ–å¤šå°‘ç¥ç»å…ƒï¼Œæ¯ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½ä¼šæ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œç›¸å½“äºç½‘ç»œæ²¡æœ‰å¤šä¸ªç¥ç»å…ƒçš„å­¦ä¹ èƒ½åŠ›ã€‚

```PYTHON
def initialize_parameters_zeros(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    
    for l in range(1, L): #åˆ°L-1
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1])) #ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œå‰ä¸€å±‚]ï¼Œé€šå¸¸ä¾é é¢„å¤„ç†åçš„Xæ¥ç¡®å®š
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))#å‚æ•°bçš„ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œ1]
        ### END CODE HERE ###
    return parameters

parameters = initialize_parameters_zeros([3,2,1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨é›¶åˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "zeros")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```python
Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599455
Cost after iteration 11000: 0.6931471805599453
Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453
On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5
```

![image-20240610160011776](images/image-20240610160011776.png)

æ€§èƒ½ç¡®å®å¾ˆå·®ï¼ŒæŸå¤±ä¹Ÿæ²¡æœ‰é™ä½ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é¢„æµ‹çš„è¯¦ç»†ä¿¡æ¯å’Œå†³ç­–è¾¹ç•Œï¼š

```python
print ("predictions_train = " + str(predictions_train))
print ("predictions_test = " + str(predictions_test))
```

output:

```python
predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
```

```python
plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)#ç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚
```

output:

![image-20240610160400535](images/image-20240610160400535.png)

è¯¥æ¨¡å‹é¢„æµ‹çš„æ¯ä¸ªç¤ºä¾‹éƒ½ä¸º0ã€‚

èƒŒæ™¯è¢«æ¶‚æˆé»„è‰²ï¼Œä»£è¡¨å†³ç­–è¾¹ç•ŒåŒºåŸŸ,ç”±äºæ²¡æœ‰å¯è§çš„çº¿æ¡æˆ–æ›²çº¿æ¥åˆ†éš”è¿™ä¸¤ç±»æ•°æ®ç‚¹ï¼Œè¿™è¡¨æ˜æ¨¡å‹æ²¡æœ‰å­¦ä¼šåŒºåˆ†å®ƒä»¬ã€‚

é€šå¸¸ï¼Œå°†æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸ºé›¶ä¼šå¯¼è‡´ç½‘ç»œæ— æ³•æ‰“ç ´å¯¹ç§°æ€§ã€‚ è¿™æ„å‘³ç€æ¯ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒéƒ½å°†å­¦ä¹ ç›¸åŒçš„ä¸œè¥¿ï¼Œå¹¶ä¸”ä½ ä¸å¦¨è®­ç»ƒæ¯ä¸€å±‚$n^{[l]}=1$çš„ç¥ç»ç½‘ç»œï¼Œä¸”è¯¥ç½‘ç»œçš„æ€§èƒ½ä¸å¦‚çº¿æ€§åˆ†ç±»å™¨ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ã€‚

**å› æ­¤ç»“è®ºå¦‚ä¸‹**ï¼š

- æƒé‡$W^{[l]}$åº”è¯¥éšæœºåˆå§‹åŒ–ä»¥æ‰“ç ´å¯¹ç§°æ€§ã€‚
- å°†åå·®$b^{[l]}$åˆå§‹åŒ–ä¸ºé›¶æ˜¯å¯ä»¥çš„ã€‚åªè¦éšæœºåˆå§‹åŒ–äº†$W^{[l]}$ï¼Œå¯¹ç§°æ€§ä»ç„¶ä¼šç ´åã€‚

### 1.2-éšæœºåˆå§‹åŒ–

â€‹	ä¸ºäº†æ‰“ç ´å¯¹ç§°æ€§ï¼Œè®©æˆ‘ä»¬éšæœºè®¾ç½®æƒé‡ã€‚ åœ¨éšæœºåˆå§‹åŒ–ä¹‹åï¼Œæ¯ä¸ªç¥ç»å…ƒå¯ä»¥ç»§ç»­å­¦ä¹ å…¶è¾“å…¥çš„ä¸åŒç‰¹å¾ã€‚ åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†çœ‹åˆ°å¦‚æœå°†æƒé‡éšæœºåˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„å€¼ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ï¼ˆæŒ‰*10ç¼©æ”¾ï¼‰ï¼Œå¹¶å°†åå·®è®¾ä¸º0ã€‚ å°† `np.random.randn(..,..) * 10`ç”¨äºæƒé‡ï¼Œå°†`np.zeros((.., ..))`ç”¨äºåå·®ã€‚

```python
# GRADED FUNCTION: initialize_parameters_random

def initialize_parameters_random(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
    parameters = {}
    L = len(layers_dims)            # integer representing the number of layers
    
    for l in range(1, L):
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*10
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###

    return parameters

parameters = initialize_parameters_random([3, 2, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

output:

```python
W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "random")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```PYTHON
Cost after iteration 0: inf
Cost after iteration 1000: 0.6247924745506072
Cost after iteration 2000: 0.5980258056061102
Cost after iteration 3000: 0.5637539062842213
Cost after iteration 4000: 0.5501256393526495
Cost after iteration 5000: 0.5443826306793814
Cost after iteration 6000: 0.5373895855049121
Cost after iteration 7000: 0.47157999220550006
Cost after iteration 8000: 0.39770475516243037
Cost after iteration 9000: 0.3934560146692851
Cost after iteration 10000: 0.3920227137490125
Cost after iteration 11000: 0.38913700035966736
Cost after iteration 12000: 0.3861358766546214
Cost after iteration 13000: 0.38497629552893475
Cost after iteration 14000: 0.38276694641706693
On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86
```

![image-20240614115011370](images/image-20240614115011370.png)

å› ä¸ºæ•°å€¼èˆå…¥ï¼Œä½ å¯èƒ½åœ¨0è¿­ä»£ä¹‹åçœ‹åˆ°æŸå¤±ä¸º"inf"(infinite æ— é™)ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¹‹åç”¨æ›´å¤æ‚çš„æ•°å­—å®ç°è§£å†³æ­¤é—®é¢˜ã€‚

```python
print (predictions_train)
print (predictions_test)
```
outputï¼š
```python
[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]
```

```python
plt.title("Model with large random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

outputï¼š

![image-20240614115411087](images/image-20240614115411087.png)

- æŸå¤±ä¸€å¼€å§‹å¾ˆé«˜æ˜¯å› ä¸ºè¾ƒå¤§çš„éšæœºæƒé‡å€¼ï¼Œå¯¹äºæŸäº›æ•°æ®ï¼Œæœ€åä¸€å±‚æ¿€æ´»å‡½æ•°sigmoidè¾“å‡ºçš„ç»“æœéå¸¸æ¥è¿‘0æˆ–1ï¼Œå¹¶ä¸”å½“è¯¥ç¤ºä¾‹æ•°æ®é¢„æµ‹é”™è¯¯æ—¶ï¼Œå°†å¯¼è‡´éå¸¸é«˜çš„æŸå¤±ã€‚å½“$\log(a^{[3]}) = \log(0)$æ—¶ï¼ŒæŸå¤±è¾¾åˆ°æ— ç©·å¤§ã€‚
- åˆå§‹åŒ–ä¸å½“ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŒæ—¶ä¹Ÿä¼šå‡æ…¢ä¼˜åŒ–ç®—æ³•çš„é€Ÿåº¦ã€‚
- è®­ç»ƒè¾ƒé•¿æ—¶é—´çš„ç½‘ç»œï¼Œå°†ä¼šçœ‹åˆ°æ›´å¥½çš„ç»“æœï¼Œä½†æ˜¯ä½¿ç”¨å¤ªå¤§çš„éšæœºæ•°è¿›è¡Œåˆå§‹åŒ–ä¼šé™ä½ä¼˜åŒ–é€Ÿåº¦ã€‚

**æ€»ç»“**ï¼š

- å°†æƒé‡åˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„éšæœºå€¼æ•ˆæœä¸ä½³ã€‚
- åˆå§‹åŒ–ä¸ºè¾ƒå°çš„éšæœºå€¼ä¼šæ›´å¥½ã€‚é‡è¦çš„é—®é¢˜æ˜¯ï¼šè¿™äº›éšæœºå€¼åº”ä¸ºå¤šå°ï¼Ÿè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼

### 1.3-Heåˆå§‹åŒ–

â€‹	å°è¯•â€œHe åˆå§‹åŒ–â€ï¼Œè¯¥åç§°ä»¥Heç­‰äººçš„åå­—å‘½åï¼ˆç±»ä¼¼äºâ€œXavieråˆå§‹åŒ–â€ï¼Œä½†Xavieråˆå§‹åŒ–ä½¿ç”¨æ¯”ä¾‹å› å­ `sqrt(1./layers_dims[l-1])`æ¥è¡¨ç¤ºæƒé‡ğ‘Š[ğ‘™] ï¼Œè€ŒHeåˆå§‹åŒ–ä½¿ç”¨`sqrt(2./layers_dims[l-1])`ï¼‰ã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œä»¥Heåˆå§‹åŒ–æ¥åˆå§‹åŒ–å‚æ•°ã€‚

â€‹	**æç¤º**ï¼šæ­¤å‡½æ•°ç±»ä¼¼äºå…ˆå‰çš„`initialize_parameters_random(...)`ã€‚ å”¯ä¸€çš„ä¸åŒæ˜¯ï¼Œæ— éœ€å°†`np.random.randn(..,..)`ä¹˜ä»¥10ï¼Œè€Œæ˜¯å°†å…¶ä¹˜ä»¥2dimension of the previous layerï¼Œè¿™æ˜¯Heåˆå§‹åŒ–å»ºè®®ä½¿ç”¨çš„ReLUæ¿€æ´»å±‚ã€‚

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer. åŒ…å«æ¯å±‚å¤§å°çš„ python æ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1 # integer representing the number of layers  æ­¤å¤„æ˜¯åŒ…å«æ¯å±‚å¤§å°çš„pythonæ•°ç»„ï¼Œè€Œå¹¶éç›´æ¥å®šä¹‰çš„å±‚æ•°ã€‚
     
    for l in range(1, L + 1): #ä»1åˆ°L
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###
        
    return parameters
parameters = initialize_parameters_he([2, 4, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```python
W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä½¿ç”¨Heåˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒä½ çš„æ¨¡å‹ã€‚

```py
parameters = model(train_X, train_Y, initialization = "he")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```python
Cost after iteration 0: 0.8830537463419761
Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
Cost after iteration 3000: 0.6526117768893805
Cost after iteration 4000: 0.6082958970572938
Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.4138645817071794
Cost after iteration 7000: 0.3117803464844441
Cost after iteration 8000: 0.23696215330322562
Cost after iteration 9000: 0.1859728720920684
Cost after iteration 10000: 0.15015556280371808
Cost after iteration 11000: 0.12325079292273551
Cost after iteration 12000: 0.09917746546525937
Cost after iteration 13000: 0.08457055954024283
Cost after iteration 14000: 0.07357895962677366
On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96
```

![image-20240617231530976](images/image-20240617231530976.png)

```python
plt.title("Model with He initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
# plot_decision_boundary çš„å‡½æ•°ï¼Œå®ƒç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚è¿™ä¸ªå‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š
	#ä¸€ä¸ª lambda å‡½æ•°ï¼šlambda x: predict_dec(parameters, x.T)ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ¿åå‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥çš„ x åº”ç”¨ predict_dec å‡½æ•°ã€‚è¿™é‡Œçš„ parameters æ˜¯é¢„å…ˆå®šä¹‰çš„æ¨¡å‹å‚æ•°ï¼Œè€Œ x.T æ˜¯å°†è¾“å…¥çš„ x è½¬ç½®ã€‚
    #train_Xï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®ç‰¹å¾çš„æ•°ç»„ã€‚
	#train_Yï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®æ ‡ç­¾çš„æ•°ç»„ã€‚
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

![image-20240617234257865](images/image-20240617234257865.png)

# ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–

â€‹	æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§å’Œèƒ½åŠ›ï¼Œå¦‚æœè®­ç»ƒæ•°æ®é›†ä¸å¤Ÿå¤§ï¼Œ**å°†ä¼šé€ æˆä¸€ä¸ªä¸¥é‡çš„é—®é¢˜--è¿‡æ‹Ÿåˆ**ã€‚å°½ç®¡å®ƒåœ¨è®­ç»ƒé›†ä¸Šæ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å­¦åˆ°çš„ç½‘ç»œ**ä¸èƒ½åº”ç”¨åˆ°æµ‹è¯•é›†ä¸­ï¼**

â€‹	é¦–å…ˆå¯¼å…¥è¦ä½¿ç”¨çš„åŒ…ã€‚

```python
# import packages
import numpy as np
import matplotlib.pyplot as plt
from lib.reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec
from lib.reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters
import sklearn
import sklearn.datasets
import scipy.io
from lib.testCases import *

plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'
```

**é—®é¢˜é™ˆè¿°**ï¼šä½ åˆšåˆšè¢«æ³•å›½è¶³çƒå…¬å¸è˜ä¸ºAIä¸“å®¶ã€‚ä»–ä»¬å¸Œæœ›ä½ æ¨èé¢„æµ‹æ³•å›½å®ˆé—¨å‘˜å°†çƒè¸¢å‡ºçš„ä½ç½®ï¼Œä»¥ä¾¿æ³•å›½é˜Ÿçš„çƒå‘˜å¯ä»¥ç”¨å¤´å°†çƒå‡»ä¸­ã€‚

![image-20240619213902974](images/image-20240619213902974.png)
	å®ˆé—¨å‘˜å°†çƒè¸¢åˆ°ç©ºä¸­ï¼Œæ¯æ”¯çƒé˜Ÿçš„çƒå‘˜éƒ½åœ¨å°½åŠ›ç”¨å¤´å‡»çƒ

â€‹	ä»¥ä¸‹æä¾›äº†æ³•å›½è¿‡å»10åœºæ¯”èµ›çš„äºŒç»´æ•°æ®é›†ã€‚

```python
train_X, train_Y, test_X, test_Y = load_2D_dataset()
```

output:

![image-20240619214954850](images/image-20240619214954850.png)

æ•°æ®ä¸­æ¯ä¸ªç‚¹å¯¹åº”äºè¶³çƒåœºä¸Šçš„ä½ç½®ï¼Œåœ¨è¯¥ä½ç½®ä¸Šï¼Œæ³•å›½å®ˆé—¨å‘˜ä»è¶³çƒåœºå·¦ä¾§å°„å‡ºçƒåï¼Œè¶³çƒè¿åŠ¨å‘˜ç”¨ä»–/å¥¹çš„å¤´éƒ¨å‡»ä¸­äº†çƒã€‚

- å¦‚æœåœ†ç‚¹ä¸ºè“è‰²ï¼Œåˆ™è¡¨ç¤ºæ³•å›½çƒå‘˜è®¾æ³•ç”¨å¤´éƒ¨å°†çƒå‡»ä¸­
- å¦‚æœåœ†ç‚¹ä¸ºçº¢è‰²ï¼Œåˆ™è¡¨ç¤ºå¦ä¸€æ”¯çƒé˜Ÿçš„çƒå‘˜ç”¨å¤´æ’çƒ

**ä½ çš„ç›®æ ‡**ï¼šè¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹å®ˆé—¨å‘˜åº”å°†çƒè¸¢åˆ°çƒåœºä¸Šçš„ä½ç½®ã€‚

**æ•°æ®é›†åˆ†æ**ï¼šè¯¥æ•°æ®é›†å«æœ‰å™ªå£°ï¼Œä½†ä¸€æ¡å°†å·¦ä¸ŠåŠéƒ¨åˆ†ï¼ˆè“è‰²ï¼‰ä¸å³ä¸‹åŠéƒ¨åˆ†ï¼ˆçº¢è‰²ï¼‰åˆ†å¼€çš„å¯¹è§’çº¿ä¼šå¾ˆæ¯”è¾ƒæœ‰æ•ˆã€‚

é¦–å…ˆå°è¯•éæ­£åˆ™åŒ–æ¨¡å‹ã€‚ç„¶åå­¦ä¹ å¦‚ä½•å¯¹å…¶è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶å†³å®šé€‰æ‹©å“ªç§æ¨¡å‹æ¥è§£å†³æ³•å›½è¶³çƒå…¬å¸çš„é—®é¢˜ã€‚

## 1-éæ­£åˆ™åŒ–æ¨¡å‹

ä½ å°†ä½¿ç”¨ä»¥ä¸‹ç¥ç»ç½‘ç»œï¼ˆå·²ä¸ºä½ å®ç°ï¼‰ï¼Œå¯ä»¥å¦‚ä¸‹ä½¿ç”¨æ­¤æ¨¡å‹ï¼š

- åœ¨*regularization mode*ä¸­ï¼Œé€šè¿‡`lambd`å°†è¾“å…¥è®¾ç½®ä¸ºéé›¶å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨`lambd`ä»£æ›¿`lambda`ï¼Œå› ä¸º`lambda`æ˜¯Pythonä¸­çš„ä¿ç•™å…³é”®å­—ã€‚
- åœ¨*dropout mode*ä¸­ï¼Œå°†`keep_prob`è®¾ç½®ä¸ºå°äº1çš„å€¼

é¦–å…ˆï¼Œä½ å°†å°è¯•ä¸è¿›è¡Œä»»ä½•æ­£åˆ™åŒ–çš„æ¨¡å‹ã€‚ç„¶åï¼Œä½ å°†å®ç°ï¼š

- *L2 æ­£åˆ™åŒ–* å‡½æ•°ï¼š`compute_cost_with_regularization()`å’Œ`backward_propagation_with_regularization()`
- *Dropout* å‡½æ•°ï¼š`forward_propagation_with_dropout()`å’Œ`backward_propagation_with_dropout()`

```PYTHON
def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
    learning_rate -- learning rate of the optimization
    num_iterations -- number of iterations of the optimization loop
    print_cost -- If True, print the cost every 10000 iterations
    lambd -- regularization hyperparameter, scalar
    keep_prob - probability of keeping a neuron active during drop-out, scalar.
    
    Returns:
    parameters -- parameters learned by the model. They can then be used to predict.
    """
        
    grads = {}
    costs = []                            # to keep track of the cost
    m = X.shape[1]                        # number of examples æ ·æœ¬æ•°
    layers_dims = [X.shape[0], 20, 3, 1] #ä¸‰å±‚ç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€å±‚ä¸ºé¢„å¤„ç†åçš„æ ·æœ¬
    
    # Initialize parameters dictionary.
    parameters = initialize_parameters(layers_dims) #åˆå§‹åŒ–å‚æ•°wå’Œbï¼ˆå¤šä¸ªï¼‰

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
        elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob) #a3æ˜¯æœ€åä¸€å±‚çš„è¾“å‡º
        
        # Cost function
        if lambd == 0: #è¡¥å¿å­¦ä¹ ç‡
            cost = compute_cost(a3, Y)
        else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)  #æ˜¯å¯¹ç”¨wå¯¹æˆæœ¬å‡½æ•°è¿›è¡Œä¿®æ­£ï¼Œå› æ­¤ä¼ å…¥parametersè·å–wå‚æ•°
            
        # Backward propagation.
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache) #æ­£å¸¸åå‘ä¼ æ’­
        elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
        elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 10000 iterations
        if print_cost and i % 10000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
        if print_cost and i % 1000 == 0:
            costs.append(cost)
    
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (x1,000)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
parameters = model(train_X, train_Y)
print ("On the training set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
plt.title("Model without regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
plt.show()
```



åœ¨ä¸è¿›è¡Œä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿè®­ç»ƒ/æµ‹è¯•é›†çš„å‡†ç¡®æ€§ã€‚

outputï¼š

```PYTHON
Cost after iteration 0: 0.6557412523481002
Cost after iteration 10000: 0.1632998752572417
Cost after iteration 20000: 0.13851642423284755
```

![image-20240619220547567](images/image-20240619220547567.png)



![image-20240619220641508](images/image-20240619220641508.png)

éæ­£åˆ™åŒ–æ¨¡å‹æ˜¾ç„¶è¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒé›†ï¼Œæ‹Ÿåˆäº†ä¸€äº›å™ªå£°ç‚¹ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å‡å°‘è¿‡æ‹Ÿåˆçš„ä¸¤ç§æ‰‹æ®µã€‚

## 2-L2æ­£åˆ™åŒ–	

é¿å…è¿‡æ‹Ÿåˆçš„æ ‡å‡†æ–¹æ³•ç§°ä¸º **L2æ­£åˆ™åŒ–**ï¼Œå®ƒå°†æŸå¤±å‡½æ•°ä»ï¼š
$$
J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}
$$
ä¿®æ”¹åˆ°ï¼š
$$
J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}
$$
åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™ä¸ªå…¬å¼ä¸­çš„ **k** å’Œ **j** ä»£è¡¨ä»¥ä¸‹å†…å®¹ï¼š

- **k** è¡¨ç¤ºç¥ç»ç½‘ç»œä¸­çš„ç¬¬ **k** ä¸ªç¥ç»å…ƒæˆ–èŠ‚ç‚¹ã€‚
- **j** è¡¨ç¤ºä¸ç¬¬ **k** ä¸ªç¥ç»å…ƒç›¸è¿çš„æƒé‡ã€‚

å‡è®¾æœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œéšè—å±‚æœ‰3ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸ªç¥ç»å…ƒä¸è¾“å…¥å±‚çš„4ä¸ªç‰¹å¾ç›¸è¿ã€‚æˆ‘ä»¬å¯ä»¥è¡¨ç¤ºè¿™ä¸ªéšè—å±‚çš„æƒé‡çŸ©é˜µä¸ºï¼š
$$
W^{[1]} = \begin{bmatrix}
0.5  ; -0.2 ; 0.8 ; 0.3 \\
-0.1 ; 0.6  ; -0.4; 0.7 \\
0.2  ; 0.4  ; 0.1 ; -0.5 \\
\end{bmatrix}
$$
ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæƒé‡çš„å¹³æ–¹å¹¶æ±‚å’Œï¼š
$$
\sum_{k}\sum_{j} W_{k,j}^{[1]2} = (0.5)^2 + (-0.2)^2 + 0.8^2 + 0.3^2 + (-0.1)^2 + 0.6^2 + (-0.4)^2 + 0.7^2 + 0.2^2 + 0.4^2 + 0.1^2 + (-0.5)^2
$$
è®¡ç®—ç»“æœä¸ºï¼š
$$
\sum_{k}\sum_{j} W_{k,j}^{[1]2} = 2.63
$$
â€‹	**ç»ƒä¹ **ï¼šå®ç°`compute_cost_with_regularizationï¼ˆï¼‰`ï¼Œä»¥è®¡ç®—å…¬å¼ï¼ˆ2ï¼‰çš„æŸå¤±ã€‚è¦è®¡ç®—$\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$ ï¼Œè¯·ä½¿ç”¨ï¼š

```python
np.sum(np.square(Wl))
```

å¿…é¡»å¯¹$W^{[1]}$ï¼Œ$W^{[2]}$å’Œ$W^{[3]}$æ‰§è¡Œæ­¤æ“ä½œï¼Œç„¶åå°†ä¸‰ä¸ªé¡¹ç›¸åŠ å¹¶ä¹˜ä»¥$\frac{1}{m}\frac{\lambda}{2}$ã€‚

```PYTHON
def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost è®¡ç®—æŸå¤±å‡½æ•°
    
    ### START CODE HERE ### (approx. 1 line)
    L2_regularization_cost = (1./m*lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) #è®¡ç®—L2æ­£åˆ™åŒ–é¡¹
    ### END CODER HERE ###
    
    cost = cross_entropy_cost + L2_regularization_cost
    
    return cost

A3, Y_assess, parameters = compute_cost_with_regularization_test_case()

print("cost = " + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))
```

output:

```PYTHON
cost = 1.7864859451590758
```

å½“ç„¶ï¼Œå› ä¸ºä½ æ›´æ”¹äº†æŸå¤±ï¼Œæ‰€ä»¥è¿˜å¿…é¡»æ›´æ”¹åå‘ä¼ æ’­ï¼ å¿…é¡»é’ˆå¯¹æ–°æŸå¤±å‡½æ•°è®¡ç®—æ‰€æœ‰æ¢¯åº¦ã€‚

**ç»ƒä¹ **ï¼šå®ç°æ­£åˆ™åŒ–åçš„åå‘ä¼ æ’­ã€‚æ›´æ”¹ä»…æ¶‰åŠdW1ï¼ŒdW2å’ŒdW3ã€‚å¯¹äºæ¯ä¸€ä¸ªï¼Œä½ å¿…é¡»æ·»åŠ æ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$ã€‚

éå¸¸å®¹æ˜“ã€æ ¹æ®å¯¼æ•°å››åˆ™è¿ç®—ã€æ±‚å¯¼åä¾æ—§ç›¸åŠ ï¼Œå³åœ¨dwåæ–¹æ·»åŠ $\frac{\lambda}{m} W$å³å¯ã€‚

```PYTHON

def backward_propagation_with_regularization(X, Y, cache, lambd):
    #å¯¼æ•°éƒ½æ˜¯ä»ä¼ å¯¼å›¾ä¸­ç›´æ¥æ¨ç®—å¾—åˆ°
    """
    Implements the backward propagation of our baseline model to which we added an L2 regularization.

    Arguments:
    X -- input dataset, of shape (input size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation()
    lambd -- regularization hyperparameter, scalar

    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """

    m = X.shape[1] #æ ·æœ¬æ•°
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache

    dZ3 = A3 - Y

    ### START CODE HERE ### (approx. 1 line)
    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3
    ### END CODE HERE ###
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)

    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2
    ### END CODE HERE ###
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)

    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m * W1
    ### END CODE HERE ###
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)

    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}

    return gradients

X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()

grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)
print ("dW1 = "+ str(grads["dW1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("dW3 = "+ str(grads["dW3"]))
```

outputï¼š

```PYTHON
dW1 = [[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 = [[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 = [[-1.77691347 -0.11832879 -0.09397446]]
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨L2æ­£åˆ™åŒ–(ğœ†=0.7)è¿è¡Œçš„æ¨¡å‹ã€‚`modelï¼ˆï¼‰`å‡½æ•°å°†è°ƒç”¨ï¼š

- `compute_cost_with_regularization`ä»£æ›¿`compute_cost`
- `backward_propagation_with_regularization`ä»£æ›¿`backward_propagation`

```PYTHON
parameters = model(train_X, train_Y, lambd = 0.7)
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```python
Cost after iteration 0: 0.6974484493131264
Cost after iteration 10000: 0.2684918873282239
Cost after iteration 20000: 0.2680916337127301
On the train set:
Accuracy: 0.9383886255924171
On the test set:
Accuracy: 0.93
```

![image-20240620222750656](images/image-20240620222750656.png)

å†³ç­–è¾¹ç•Œï¼š

```python
plt.title("Model with L2-regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

outputï¼š

![image-20240620222850603](images/image-20240620222850603.png)

- ğœ†  çš„å€¼æ˜¯ä½ å¯ä»¥è°ƒæ•´å¼€å‘é›†çš„è¶…å‚æ•°ã€‚
- L2æ­£åˆ™åŒ–ä½¿å†³ç­–è¾¹ç•Œæ›´å¹³æ»‘ã€‚å¦‚æœğœ† å¤ªå¤§ï¼Œåˆ™ä¹Ÿå¯èƒ½â€œè¿‡åº¦å¹³æ»‘â€ï¼Œä»è€Œä½¿æ¨¡å‹åå·®è¾ƒé«˜ã€‚

### 2.1-L2æ­£åˆ™åŒ–çš„åŸç†

L2æ­£åˆ™åŒ–åŸºäºä»¥ä¸‹å‡è®¾ï¼šæƒé‡è¾ƒå°çš„æ¨¡å‹æ¯”æƒé‡è¾ƒå¤§çš„æ¨¡å‹æ›´ç®€å•ã€‚å› æ­¤ï¼Œé€šè¿‡å¯¹æŸå¤±å‡½æ•°ä¸­æƒé‡çš„å¹³æ–¹å€¼è¿›è¡Œæƒ©ç½šï¼Œå¯ä»¥å°†æ‰€æœ‰æƒé‡é©±åŠ¨ä¸ºè¾ƒå°çš„å€¼ã€‚æ¯”é‡å¤ªå¤§ä¼šä½¿æŸå¤±è¿‡é«˜ï¼è¿™å°†å¯¼è‡´æ¨¡å‹æ›´å¹³æ»‘ï¼Œè¾“å‡ºéšç€è¾“å…¥çš„å˜åŒ–è€Œå˜åŒ–å¾—æ›´æ…¢ã€‚

 L2æ­£åˆ™åŒ–çš„å½±å“ï¼š

- æŸå¤±è®¡ç®—ï¼š
    \- æ­£åˆ™åŒ–æ¡ä»¶ä¼šæ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­
- åå‘ä¼ æ’­å‡½æ•°ï¼š
    \- æœ‰å…³æƒé‡çŸ©é˜µçš„æ¸å˜ä¸­è¿˜æœ‰å…¶ä»–æœ¯è¯­
- æƒé‡æœ€ç»ˆå˜å°ï¼ˆâ€œæƒé‡è¡°å‡â€ï¼‰ï¼š
    \- æƒé‡è¢«æ¨åˆ°è¾ƒå°çš„å€¼ã€‚

