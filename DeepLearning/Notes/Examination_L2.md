# ç¥ç»ç½‘ç»œåˆå§‹åŒ–

æ¬¢è¿æ¥åˆ°â€œæ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œâ€çš„ç¬¬ä¸€é¡¹ä½œä¸šã€‚

è®­ç»ƒç¥ç»ç½‘ç»œéœ€è¦æŒ‡å®šæƒé‡çš„åˆå§‹å€¼ï¼Œè€Œä¸€ä¸ªå¥½çš„åˆå§‹åŒ–æ–¹æ³•å°†æœ‰åŠ©äºç½‘ç»œå­¦ä¹ ã€‚

å¦‚æœä½ å®Œæˆäº†æœ¬ç³»åˆ—çš„ä¸Šä¸€è¯¾ç¨‹ï¼Œåˆ™å¯èƒ½å·²ç»æŒ‰ç…§æˆ‘ä»¬çš„è¯´æ˜å®Œæˆäº†æƒé‡åˆå§‹åŒ–ã€‚ä½†æ˜¯ï¼Œå¦‚ä½•ä¸ºæ–°çš„ç¥ç»ç½‘ç»œé€‰æ‹©åˆå§‹åŒ–ï¼Ÿåœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œä½ èƒ½å­¦ä¹ çœ‹åˆ°ä¸åŒçš„åˆå§‹åŒ–å¯¼è‡´çš„ä¸åŒç»“æœã€‚

å¥½çš„åˆå§‹åŒ–å¯ä»¥ï¼š

- åŠ å¿«æ¢¯åº¦ä¸‹é™ã€æ¨¡å‹æ”¶æ•›
- å‡å°æ¢¯åº¦ä¸‹é™æ”¶æ•›è¿‡ç¨‹ä¸­è®­ç»ƒï¼ˆå’Œæ³›åŒ–ï¼‰å‡ºç°è¯¯å·®çš„å‡ ç‡

é¦–å…ˆï¼Œè¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ä»¥åŠ è½½åŒ…å’Œç”¨äºåˆ†ç±»çš„äºŒç»´æ•°æ®é›†ã€‚

å¯¹äºload_dataset()å‡½æ•°è§£æ

```PYTHON
def load_dataset():
    np.random.seed(1)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)# ç”Ÿæˆ300ä¸ªè®­ç»ƒæ ·æœ¬
    np.random.seed(2)
    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)
    # train_X[:, 0] å’Œ train_X[:, 1] ä»£è¡¨æ•°æ®ç‚¹çš„ x å’Œ y åæ ‡ï¼Œc=train_Y è¡¨ç¤ºé¢œè‰²ï¼Œæ ¹æ®train_Yä¸ºä¸åŒç±»åˆ«çš„ç‚¹ç€è‰²ï¼Œs=40ç‚¹çš„å¤§å°ï¼Œcmap=plt.cm.Spectral æŒ‡å®šäº†ä¸€ä¸ªé¢œè‰²æ˜ å°„ï¼Œç”¨äºç»™ä¸åŒç±»åˆ«çš„ç‚¹ç€ä¸åŒçš„é¢œè‰²ã€‚
    #train_X[:, 0] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬0åˆ—çš„å…ƒç´ ã€‚è¿™é‡Œçš„ : è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œè€Œ 0 è¡¨ç¤ºé€‰æ‹©ç¬¬0åˆ—ã€‚train_X æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œç¬¬0åˆ—å’Œç¬¬1åˆ—åˆ†åˆ«ä»£è¡¨æ•°æ®ç‚¹çš„xåæ ‡å’Œyåæ ‡ã€‚
 #    [[x1, y1],
 #    [x2, y2],
 #    [x3, y3],
 #        ...]
    #é‚£ä¹ˆ train_X[:, 0] å°†ä¼šæ˜¯ [x1, x2, x3, ...]ï¼Œè€Œ train_X[:, 1] å°†ä¼šæ˜¯ [y1, y2, y3, ...]ã€‚
train_X[:, 1] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬1åˆ—çš„å…ƒç´ ã€‚åŒæ ·åœ°ï¼Œ: è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œ1 è¡¨ç¤ºé€‰æ‹©ç¬¬1åˆ—ã€‚
    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);
    train_X = train_X.T
    train_Y = train_Y.reshape((1, train_Y.shape[0]))
    test_X = test_X.T
    test_Y = test_Y.reshape((1, test_Y.shape[0]))
    return train_X, train_Y, test_X, test_Y
```

inï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
from lib.init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation
from lib.init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec


plt.rcParams['figure.figsize'] = (7.0, 4.0) # å›¾è¡¨çš„é»˜è®¤å¤§å°è®¾ç½®ä¸ºå®½7è‹±å¯¸ã€é«˜4è‹±å¯¸ã€‚
plt.rcParams['image.interpolation'] = 'nearest'#å°†å›¾åƒç¼©æ”¾æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•è®¾ç½®ä¸ºâ€™nearestâ€™ï¼Œæ„å‘³ç€åœ¨ç¼©æ”¾å›¾åƒçš„åŒºåŸŸå†…ï¼Œå°†ä½¿ç”¨åŸå§‹åƒç´ å€¼ï¼Œè¿™ä¼šå¯¼è‡´å›¾åƒçœ‹èµ·æ¥åƒå—çŠ¶ï¼Œæ²¡æœ‰å¹³æ»‘å¤„ç†ã€‚
plt.rcParams['image.cmap'] = 'gray' #å°†å›¾åƒçš„é»˜è®¤è‰²å½©æ˜ å°„è®¾ç½®ä¸ºâ€™grayâ€™ï¼Œå³ä»¥ç°åº¦æ¨¡å¼æ˜¾ç¤ºå›¾åƒã€‚

# load image dataset: blue/red dots in circles
train_X, train_Y, test_X, test_Y = load_dataset() #ä¸Šæ–¹è§£æ
plt.show()
```

output:

![image-20240610135848779](images/image-20240610135848779.png)

ä½¿ç”¨åˆ†ç±»å™¨å¸Œæœ›å°†è“ç‚¹å’Œçº¢ç‚¹åˆ†å¼€ã€‚

## 1-ç¥ç»ç½‘ç»œæ¨¡å‹

ä½¿ç”¨å·²ç»å®ç°äº†çš„3å±‚ç¥ç»ç½‘ç»œã€‚å°†å°è¯•çš„åˆå§‹åŒ–æ–¹æ³•ï¼š

- *é›¶åˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "zeros"`ã€‚
- *éšæœºåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "random"`ï¼Œè¿™ä¼šå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ã€‚
- *Heåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "he"`ï¼ŒHeåˆå§‹åŒ–ã€‚

**è¯´æ˜**ï¼šè¯·å¿«é€Ÿé˜…è¯»å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œåœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œä½ å°†å®ç°æ­¤`model()`è°ƒç”¨çš„ä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ã€‚

```PYTHON
def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
    learning_rate -- learning rate for gradient descent 
    num_iterations -- number of iterations to run gradient descent
    print_cost -- if True, print the cost every 1000 iterations
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
    Returns:
    parameters -- parameters learnt by the model
    """
        
    grads = {} # to keep track of the gradients æ¢¯åº¦
    costs = [] # to keep track of the loss æŸå¤±å‡½æ•°
    m = X.shape[1] # number of examples  æ ·æœ¬æ•°
    layers_dims = [X.shape[0], 10, 5, 1] #ä¸‰å±‚ç¥ç»ç½‘ç»œ
    
    # Initialize parameters dictionary.
    if initialization == "zeros":
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == "random":
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == "he":
        parameters = initialize_parameters_he(layers_dims)

    # Loop (gradient descent)
    #æ¢¯åº¦ä¸‹é™ç®—æ³•
    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        a3, cache = forward_propagation(X, parameters) #å‰å‘ä¼ æ’­
        
        # Loss
        cost = compute_loss(a3, Y)#æŸå¤±å‡½æ•°

        # Backward propagation.
        grads = backward_propagation(X, Y, cache) #åå‘ä¼ æ’­
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)#æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
            costs.append(cost)
            
    # plot the loss
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```

### 1.1-é›¶åˆå§‹åŒ–

åœ¨ç¥ç»ç½‘ç»œä¸­æœ‰ä¸¤ç§ç±»å‹çš„å‚æ•°è¦åˆå§‹åŒ–ï¼š

- æƒé‡çŸ©é˜µ$(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$
- åå·®å‘é‡ $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$

**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ä»¥å°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ã€‚ ç¨åä½ ä¼šçœ‹åˆ°æ­¤æ–¹æ³•ä¼šæŠ¥é”™ï¼Œå› ä¸ºå®ƒæ— æ³•â€œæ‰“ç ´å¯¹ç§°æ€§â€ã€‚æ€»ä¹‹å…ˆå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç¡®ä¿ä½¿ç”¨æ­£ç¡®ç»´åº¦çš„np.zerosï¼ˆï¼ˆ..ï¼Œ..ï¼‰ï¼‰ã€‚

â€‹	åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœå°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¼šå¯¼è‡´ä¸€ä¸ªé—®é¢˜ç§°ä¸ºâ€œå¯¹ç§°æ€§ç ´åâ€ã€‚æ„å‘³ç€åœ¨ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œæ‰€æœ‰ç¥ç»å…ƒçš„æƒé‡æ›´æ–°å°†æ˜¯ç›¸åŒçš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¦‚æœæƒé‡ç›¸åŒï¼Œé‚£ä¹ˆåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒæ¥æ”¶åˆ°çš„è¾“å…¥å’Œæ¢¯åº¦å°†æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­æ—¶å®ƒä»¬çš„æƒé‡æ›´æ–°ä¹Ÿä¼šç›¸åŒã€‚è¿™æ ·ï¼Œæ— è®ºç½‘ç»œæœ‰å¤šå°‘å±‚æˆ–å¤šå°‘ç¥ç»å…ƒï¼Œæ¯ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½ä¼šæ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œç›¸å½“äºç½‘ç»œæ²¡æœ‰å¤šä¸ªç¥ç»å…ƒçš„å­¦ä¹ èƒ½åŠ›ã€‚

```PYTHON
def initialize_parameters_zeros(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    
    for l in range(1, L): #åˆ°L-1
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1])) #ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œå‰ä¸€å±‚]ï¼Œé€šå¸¸ä¾é é¢„å¤„ç†åçš„Xæ¥ç¡®å®š
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))#å‚æ•°bçš„ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œ1]
        ### END CODE HERE ###
    return parameters

parameters = initialize_parameters_zeros([3,2,1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨é›¶åˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "zeros")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```python
Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599455
Cost after iteration 11000: 0.6931471805599453
Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453
On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5
```

![image-20240610160011776](images/image-20240610160011776.png)

æ€§èƒ½ç¡®å®å¾ˆå·®ï¼ŒæŸå¤±ä¹Ÿæ²¡æœ‰é™ä½ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é¢„æµ‹çš„è¯¦ç»†ä¿¡æ¯å’Œå†³ç­–è¾¹ç•Œï¼š

```python
print ("predictions_train = " + str(predictions_train))
print ("predictions_test = " + str(predictions_test))
```

output:

```python
predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
```

```python
plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)#ç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚
```

output:

![image-20240610160400535](images/image-20240610160400535.png)

è¯¥æ¨¡å‹é¢„æµ‹çš„æ¯ä¸ªç¤ºä¾‹éƒ½ä¸º0ã€‚

èƒŒæ™¯è¢«æ¶‚æˆé»„è‰²ï¼Œä»£è¡¨å†³ç­–è¾¹ç•ŒåŒºåŸŸ,ç”±äºæ²¡æœ‰å¯è§çš„çº¿æ¡æˆ–æ›²çº¿æ¥åˆ†éš”è¿™ä¸¤ç±»æ•°æ®ç‚¹ï¼Œè¿™è¡¨æ˜æ¨¡å‹æ²¡æœ‰å­¦ä¼šåŒºåˆ†å®ƒä»¬ã€‚

é€šå¸¸ï¼Œå°†æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸ºé›¶ä¼šå¯¼è‡´ç½‘ç»œæ— æ³•æ‰“ç ´å¯¹ç§°æ€§ã€‚ è¿™æ„å‘³ç€æ¯ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒéƒ½å°†å­¦ä¹ ç›¸åŒçš„ä¸œè¥¿ï¼Œå¹¶ä¸”ä½ ä¸å¦¨è®­ç»ƒæ¯ä¸€å±‚$n^{[l]}=1$çš„ç¥ç»ç½‘ç»œï¼Œä¸”è¯¥ç½‘ç»œçš„æ€§èƒ½ä¸å¦‚çº¿æ€§åˆ†ç±»å™¨ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ã€‚

**å› æ­¤ç»“è®ºå¦‚ä¸‹**ï¼š

- æƒé‡$W^{[l]}$åº”è¯¥éšæœºåˆå§‹åŒ–ä»¥æ‰“ç ´å¯¹ç§°æ€§ã€‚
- å°†åå·®$b^{[l]}$åˆå§‹åŒ–ä¸ºé›¶æ˜¯å¯ä»¥çš„ã€‚åªè¦éšæœºåˆå§‹åŒ–äº†$W^{[l]}$ï¼Œå¯¹ç§°æ€§ä»ç„¶ä¼šç ´åã€‚

### 1.2-éšæœºåˆå§‹åŒ–

â€‹	ä¸ºäº†æ‰“ç ´å¯¹ç§°æ€§ï¼Œè®©æˆ‘ä»¬éšæœºè®¾ç½®æƒé‡ã€‚ åœ¨éšæœºåˆå§‹åŒ–ä¹‹åï¼Œæ¯ä¸ªç¥ç»å…ƒå¯ä»¥ç»§ç»­å­¦ä¹ å…¶è¾“å…¥çš„ä¸åŒç‰¹å¾ã€‚ åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†çœ‹åˆ°å¦‚æœå°†æƒé‡éšæœºåˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„å€¼ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ï¼ˆæŒ‰*10ç¼©æ”¾ï¼‰ï¼Œå¹¶å°†åå·®è®¾ä¸º0ã€‚ å°† `np.random.randn(..,..) * 10`ç”¨äºæƒé‡ï¼Œå°†`np.zeros((.., ..))`ç”¨äºåå·®ã€‚

```python
# GRADED FUNCTION: initialize_parameters_random

def initialize_parameters_random(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
    parameters = {}
    L = len(layers_dims)            # integer representing the number of layers
    
    for l in range(1, L):
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*10
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###

    return parameters

parameters = initialize_parameters_random([3, 2, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

output:

```python
W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "random")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```PYTHON
Cost after iteration 0: inf
Cost after iteration 1000: 0.6247924745506072
Cost after iteration 2000: 0.5980258056061102
Cost after iteration 3000: 0.5637539062842213
Cost after iteration 4000: 0.5501256393526495
Cost after iteration 5000: 0.5443826306793814
Cost after iteration 6000: 0.5373895855049121
Cost after iteration 7000: 0.47157999220550006
Cost after iteration 8000: 0.39770475516243037
Cost after iteration 9000: 0.3934560146692851
Cost after iteration 10000: 0.3920227137490125
Cost after iteration 11000: 0.38913700035966736
Cost after iteration 12000: 0.3861358766546214
Cost after iteration 13000: 0.38497629552893475
Cost after iteration 14000: 0.38276694641706693
On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86
```

![image-20240614115011370](images/image-20240614115011370.png)

å› ä¸ºæ•°å€¼èˆå…¥ï¼Œä½ å¯èƒ½åœ¨0è¿­ä»£ä¹‹åçœ‹åˆ°æŸå¤±ä¸º"inf"(infinite æ— é™)ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¹‹åç”¨æ›´å¤æ‚çš„æ•°å­—å®ç°è§£å†³æ­¤é—®é¢˜ã€‚

```python
print (predictions_train)
print (predictions_test)
```
outputï¼š
```python
[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]
```

```python
plt.title("Model with large random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

outputï¼š

![image-20240614115411087](images/image-20240614115411087.png)

- æŸå¤±ä¸€å¼€å§‹å¾ˆé«˜æ˜¯å› ä¸ºè¾ƒå¤§çš„éšæœºæƒé‡å€¼ï¼Œå¯¹äºæŸäº›æ•°æ®ï¼Œæœ€åä¸€å±‚æ¿€æ´»å‡½æ•°sigmoidè¾“å‡ºçš„ç»“æœéå¸¸æ¥è¿‘0æˆ–1ï¼Œå¹¶ä¸”å½“è¯¥ç¤ºä¾‹æ•°æ®é¢„æµ‹é”™è¯¯æ—¶ï¼Œå°†å¯¼è‡´éå¸¸é«˜çš„æŸå¤±ã€‚å½“$\log(a^{[3]}) = \log(0)$æ—¶ï¼ŒæŸå¤±è¾¾åˆ°æ— ç©·å¤§ã€‚
- åˆå§‹åŒ–ä¸å½“ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŒæ—¶ä¹Ÿä¼šå‡æ…¢ä¼˜åŒ–ç®—æ³•çš„é€Ÿåº¦ã€‚
- è®­ç»ƒè¾ƒé•¿æ—¶é—´çš„ç½‘ç»œï¼Œå°†ä¼šçœ‹åˆ°æ›´å¥½çš„ç»“æœï¼Œä½†æ˜¯ä½¿ç”¨å¤ªå¤§çš„éšæœºæ•°è¿›è¡Œåˆå§‹åŒ–ä¼šé™ä½ä¼˜åŒ–é€Ÿåº¦ã€‚

**æ€»ç»“**ï¼š

- å°†æƒé‡åˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„éšæœºå€¼æ•ˆæœä¸ä½³ã€‚
- åˆå§‹åŒ–ä¸ºè¾ƒå°çš„éšæœºå€¼ä¼šæ›´å¥½ã€‚é‡è¦çš„é—®é¢˜æ˜¯ï¼šè¿™äº›éšæœºå€¼åº”ä¸ºå¤šå°ï¼Ÿè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼

### 1.3-Heåˆå§‹åŒ–

â€‹	å°è¯•â€œHe åˆå§‹åŒ–â€ï¼Œè¯¥åç§°ä»¥Heç­‰äººçš„åå­—å‘½åï¼ˆç±»ä¼¼äºâ€œXavieråˆå§‹åŒ–â€ï¼Œä½†Xavieråˆå§‹åŒ–ä½¿ç”¨æ¯”ä¾‹å› å­ `sqrt(1./layers_dims[l-1])`æ¥è¡¨ç¤ºæƒé‡ğ‘Š[ğ‘™] ï¼Œè€ŒHeåˆå§‹åŒ–ä½¿ç”¨`sqrt(2./layers_dims[l-1])`ï¼‰ã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œä»¥Heåˆå§‹åŒ–æ¥åˆå§‹åŒ–å‚æ•°ã€‚

â€‹	**æç¤º**ï¼šæ­¤å‡½æ•°ç±»ä¼¼äºå…ˆå‰çš„`initialize_parameters_random(...)`ã€‚ å”¯ä¸€çš„ä¸åŒæ˜¯ï¼Œæ— éœ€å°†`np.random.randn(..,..)`ä¹˜ä»¥10ï¼Œè€Œæ˜¯å°†å…¶ä¹˜ä»¥2dimension of the previous layerï¼Œè¿™æ˜¯Heåˆå§‹åŒ–å»ºè®®ä½¿ç”¨çš„ReLUæ¿€æ´»å±‚ã€‚

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer. åŒ…å«æ¯å±‚å¤§å°çš„ python æ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1 # integer representing the number of layers  æ­¤å¤„æ˜¯åŒ…å«æ¯å±‚å¤§å°çš„pythonæ•°ç»„ï¼Œè€Œå¹¶éç›´æ¥å®šä¹‰çš„å±‚æ•°ã€‚
     
    for l in range(1, L + 1): #ä»1åˆ°L
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###
        
    return parameters
parameters = initialize_parameters_he([2, 4, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```python
W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä½¿ç”¨Heåˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒä½ çš„æ¨¡å‹ã€‚

```py
parameters = model(train_X, train_Y, initialization = "he")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```python
Cost after iteration 0: 0.8830537463419761
Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
Cost after iteration 3000: 0.6526117768893805
Cost after iteration 4000: 0.6082958970572938
Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.4138645817071794
Cost after iteration 7000: 0.3117803464844441
Cost after iteration 8000: 0.23696215330322562
Cost after iteration 9000: 0.1859728720920684
Cost after iteration 10000: 0.15015556280371808
Cost after iteration 11000: 0.12325079292273551
Cost after iteration 12000: 0.09917746546525937
Cost after iteration 13000: 0.08457055954024283
Cost after iteration 14000: 0.07357895962677366
On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96
```

![image-20240617231530976](images/image-20240617231530976.png)

```python
plt.title("Model with He initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
# plot_decision_boundary çš„å‡½æ•°ï¼Œå®ƒç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚è¿™ä¸ªå‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š
	#ä¸€ä¸ª lambda å‡½æ•°ï¼šlambda x: predict_dec(parameters, x.T)ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ¿åå‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥çš„ x åº”ç”¨ predict_dec å‡½æ•°ã€‚è¿™é‡Œçš„ parameters æ˜¯é¢„å…ˆå®šä¹‰çš„æ¨¡å‹å‚æ•°ï¼Œè€Œ x.T æ˜¯å°†è¾“å…¥çš„ x è½¬ç½®ã€‚
    #train_Xï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®ç‰¹å¾çš„æ•°ç»„ã€‚
	#train_Yï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®æ ‡ç­¾çš„æ•°ç»„ã€‚
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

![image-20240617234257865](images/image-20240617234257865.png)

2-æ€»ç»“

# ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–