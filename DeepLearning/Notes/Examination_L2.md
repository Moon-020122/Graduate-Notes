# ç¥ç»ç½‘ç»œåˆå§‹åŒ–

æ¬¢è¿æ¥åˆ°â€œæ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œâ€çš„ç¬¬ä¸€é¡¹ä½œä¸šã€‚

è®­ç»ƒç¥ç»ç½‘ç»œéœ€è¦æŒ‡å®šæƒé‡çš„åˆå§‹å€¼ï¼Œè€Œä¸€ä¸ªå¥½çš„åˆå§‹åŒ–æ–¹æ³•å°†æœ‰åŠ©äºç½‘ç»œå­¦ä¹ ã€‚

å¦‚æœä½ å®Œæˆäº†æœ¬ç³»åˆ—çš„ä¸Šä¸€è¯¾ç¨‹ï¼Œåˆ™å¯èƒ½å·²ç»æŒ‰ç…§æˆ‘ä»¬çš„è¯´æ˜å®Œæˆäº†æƒé‡åˆå§‹åŒ–ã€‚ä½†æ˜¯ï¼Œå¦‚ä½•ä¸ºæ–°çš„ç¥ç»ç½‘ç»œé€‰æ‹©åˆå§‹åŒ–ï¼Ÿåœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œä½ èƒ½å­¦ä¹ çœ‹åˆ°ä¸åŒçš„åˆå§‹åŒ–å¯¼è‡´çš„ä¸åŒç»“æœã€‚

å¥½çš„åˆå§‹åŒ–å¯ä»¥ï¼š

- åŠ å¿«æ¢¯åº¦ä¸‹é™ã€æ¨¡å‹æ”¶æ•›
- å‡å°æ¢¯åº¦ä¸‹é™æ”¶æ•›è¿‡ç¨‹ä¸­è®­ç»ƒï¼ˆå’Œæ³›åŒ–ï¼‰å‡ºç°è¯¯å·®çš„å‡ ç‡

é¦–å…ˆï¼Œè¿è¡Œä»¥ä¸‹å•å…ƒæ ¼ä»¥åŠ è½½åŒ…å’Œç”¨äºåˆ†ç±»çš„äºŒç»´æ•°æ®é›†ã€‚

å¯¹äºload_dataset()å‡½æ•°è§£æ

```PYTHON
def load_dataset():
    np.random.seed(1)  # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœçš„å¯é‡å¤æ€§
    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)# ç”Ÿæˆ300ä¸ªè®­ç»ƒæ ·æœ¬
    np.random.seed(2)
    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)
    # train_X[:, 0] å’Œ train_X[:, 1] ä»£è¡¨æ•°æ®ç‚¹çš„ x å’Œ y åæ ‡ï¼Œc=train_Y è¡¨ç¤ºé¢œè‰²ï¼Œæ ¹æ®train_Yä¸ºä¸åŒç±»åˆ«çš„ç‚¹ç€è‰²ï¼Œs=40ç‚¹çš„å¤§å°ï¼Œcmap=plt.cm.Spectral æŒ‡å®šäº†ä¸€ä¸ªé¢œè‰²æ˜ å°„ï¼Œç”¨äºç»™ä¸åŒç±»åˆ«çš„ç‚¹ç€ä¸åŒçš„é¢œè‰²ã€‚
    #train_X[:, 0] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬0åˆ—çš„å…ƒç´ ã€‚è¿™é‡Œçš„ : è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œè€Œ 0 è¡¨ç¤ºé€‰æ‹©ç¬¬0åˆ—ã€‚train_X æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œç¬¬0åˆ—å’Œç¬¬1åˆ—åˆ†åˆ«ä»£è¡¨æ•°æ®ç‚¹çš„xåæ ‡å’Œyåæ ‡ã€‚
 #    [[x1, y1],
 #    [x2, y2],
 #    [x3, y3],
 #        ...]
    #é‚£ä¹ˆ train_X[:, 0] å°†ä¼šæ˜¯ [x1, x2, x3, ...]ï¼Œè€Œ train_X[:, 1] å°†ä¼šæ˜¯ [y1, y2, y3, ...]ã€‚
train_X[:, 1] è¡¨ç¤ºè·å– train_X æ•°ç»„ä¸­æ‰€æœ‰è¡Œçš„ç¬¬1åˆ—çš„å…ƒç´ ã€‚åŒæ ·åœ°ï¼Œ: è¡¨ç¤ºé€‰æ‹©æ‰€æœ‰è¡Œï¼Œ1 è¡¨ç¤ºé€‰æ‹©ç¬¬1åˆ—ã€‚
    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);
    train_X = train_X.T
    train_Y = train_Y.reshape((1, train_Y.shape[0]))
    test_X = test_X.T
    test_Y = test_Y.reshape((1, test_Y.shape[0]))
    return train_X, train_Y, test_X, test_Y
```

inï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
from lib.init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation
from lib.init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec


plt.rcParams['figure.figsize'] = (7.0, 4.0) # å›¾è¡¨çš„é»˜è®¤å¤§å°è®¾ç½®ä¸ºå®½7è‹±å¯¸ã€é«˜4è‹±å¯¸ã€‚
plt.rcParams['image.interpolation'] = 'nearest'#å°†å›¾åƒç¼©æ”¾æ—¶ä½¿ç”¨çš„æ’å€¼æ–¹æ³•è®¾ç½®ä¸ºâ€™nearestâ€™ï¼Œæ„å‘³ç€åœ¨ç¼©æ”¾å›¾åƒçš„åŒºåŸŸå†…ï¼Œå°†ä½¿ç”¨åŸå§‹åƒç´ å€¼ï¼Œè¿™ä¼šå¯¼è‡´å›¾åƒçœ‹èµ·æ¥åƒå—çŠ¶ï¼Œæ²¡æœ‰å¹³æ»‘å¤„ç†ã€‚
plt.rcParams['image.cmap'] = 'gray' #å°†å›¾åƒçš„é»˜è®¤è‰²å½©æ˜ å°„è®¾ç½®ä¸ºâ€™grayâ€™ï¼Œå³ä»¥ç°åº¦æ¨¡å¼æ˜¾ç¤ºå›¾åƒã€‚

# load image dataset: blue/red dots in circles
train_X, train_Y, test_X, test_Y = load_dataset() #ä¸Šæ–¹è§£æ
plt.show()
```

output:

![image-20240610135848779](images/image-20240610135848779.png)

ä½¿ç”¨åˆ†ç±»å™¨å¸Œæœ›å°†è“ç‚¹å’Œçº¢ç‚¹åˆ†å¼€ã€‚

## 1-ç¥ç»ç½‘ç»œæ¨¡å‹

ä½¿ç”¨å·²ç»å®ç°äº†çš„3å±‚ç¥ç»ç½‘ç»œã€‚å°†å°è¯•çš„åˆå§‹åŒ–æ–¹æ³•ï¼š

- *é›¶åˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "zeros"`ã€‚
- *éšæœºåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "random"`ï¼Œè¿™ä¼šå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ã€‚
- *Heåˆå§‹åŒ–* ï¼šåœ¨è¾“å…¥å‚æ•°ä¸­è®¾ç½®`initialization = "he"`ï¼ŒHeåˆå§‹åŒ–ã€‚

**è¯´æ˜**ï¼šè¯·å¿«é€Ÿé˜…è¯»å¹¶è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œåœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œä½ å°†å®ç°æ­¤`model()`è°ƒç”¨çš„ä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ã€‚

```PYTHON
def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
    learning_rate -- learning rate for gradient descent 
    num_iterations -- number of iterations to run gradient descent
    print_cost -- if True, print the cost every 1000 iterations
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
    Returns:
    parameters -- parameters learnt by the model
    """
        
    grads = {} # to keep track of the gradients æ¢¯åº¦
    costs = [] # to keep track of the loss æŸå¤±å‡½æ•°
    m = X.shape[1] # number of examples  æ ·æœ¬æ•°
    layers_dims = [X.shape[0], 10, 5, 1] #ä¸‰å±‚ç¥ç»ç½‘ç»œ
    
    # Initialize parameters dictionary.
    if initialization == "zeros":
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == "random":
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == "he":
        parameters = initialize_parameters_he(layers_dims)

    # Loop (gradient descent)
    #æ¢¯åº¦ä¸‹é™ç®—æ³•
    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        a3, cache = forward_propagation(X, parameters) #å‰å‘ä¼ æ’­
        
        # Loss
        cost = compute_loss(a3, Y)#æŸå¤±å‡½æ•°

        # Backward propagation.
        grads = backward_propagation(X, Y, cache) #åå‘ä¼ æ’­
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)#æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
            costs.append(cost)
            
    # plot the loss
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```

### 1.1-é›¶åˆå§‹åŒ–

åœ¨ç¥ç»ç½‘ç»œä¸­æœ‰ä¸¤ç§ç±»å‹çš„å‚æ•°è¦åˆå§‹åŒ–ï¼š

- æƒé‡çŸ©é˜µ$(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$
- åå·®å‘é‡ $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$

**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ä»¥å°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ã€‚ ç¨åä½ ä¼šçœ‹åˆ°æ­¤æ–¹æ³•ä¼šæŠ¥é”™ï¼Œå› ä¸ºå®ƒæ— æ³•â€œæ‰“ç ´å¯¹ç§°æ€§â€ã€‚æ€»ä¹‹å…ˆå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚ç¡®ä¿ä½¿ç”¨æ­£ç¡®ç»´åº¦çš„np.zerosï¼ˆï¼ˆ..ï¼Œ..ï¼‰ï¼‰ã€‚

â€‹	åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œå¦‚æœå°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ï¼Œä¼šå¯¼è‡´ä¸€ä¸ªé—®é¢˜ç§°ä¸ºâ€œå¯¹ç§°æ€§ç ´åâ€ã€‚æ„å‘³ç€åœ¨ç½‘ç»œçš„æ¯ä¸€å±‚ï¼Œæ‰€æœ‰ç¥ç»å…ƒçš„æƒé‡æ›´æ–°å°†æ˜¯ç›¸åŒçš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œå¦‚æœæƒé‡ç›¸åŒï¼Œé‚£ä¹ˆåœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒæ¥æ”¶åˆ°çš„è¾“å…¥å’Œæ¢¯åº¦å°†æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­æ—¶å®ƒä»¬çš„æƒé‡æ›´æ–°ä¹Ÿä¼šç›¸åŒã€‚è¿™æ ·ï¼Œæ— è®ºç½‘ç»œæœ‰å¤šå°‘å±‚æˆ–å¤šå°‘ç¥ç»å…ƒï¼Œæ¯ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½ä¼šæ‰§è¡Œç›¸åŒçš„æ“ä½œï¼Œç›¸å½“äºç½‘ç»œæ²¡æœ‰å¤šä¸ªç¥ç»å…ƒçš„å­¦ä¹ èƒ½åŠ›ã€‚

```PYTHON
def initialize_parameters_zeros(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    
    for l in range(1, L): #åˆ°L-1
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.zeros((layers_dims[l],layers_dims[l-1])) #ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œå‰ä¸€å±‚]ï¼Œé€šå¸¸ä¾é é¢„å¤„ç†åçš„Xæ¥ç¡®å®š
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))#å‚æ•°bçš„ç»´åº¦ï¼Œ[å½“å‰å±‚ï¼Œ1]
        ### END CODE HERE ###
    return parameters

parameters = initialize_parameters_zeros([3,2,1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨é›¶åˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "zeros")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```python
Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599455
Cost after iteration 11000: 0.6931471805599453
Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453
On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5
```

![image-20240610160011776](images/image-20240610160011776.png)

æ€§èƒ½ç¡®å®å¾ˆå·®ï¼ŒæŸå¤±ä¹Ÿæ²¡æœ‰é™ä½ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é¢„æµ‹çš„è¯¦ç»†ä¿¡æ¯å’Œå†³ç­–è¾¹ç•Œï¼š

```python
print ("predictions_train = " + str(predictions_train))
print ("predictions_test = " + str(predictions_test))
```

output:

```python
predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
```

```python
plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)#ç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚
```

output:

![image-20240610160400535](images/image-20240610160400535.png)

è¯¥æ¨¡å‹é¢„æµ‹çš„æ¯ä¸ªç¤ºä¾‹éƒ½ä¸º0ã€‚

èƒŒæ™¯è¢«æ¶‚æˆé»„è‰²ï¼Œä»£è¡¨å†³ç­–è¾¹ç•ŒåŒºåŸŸ,ç”±äºæ²¡æœ‰å¯è§çš„çº¿æ¡æˆ–æ›²çº¿æ¥åˆ†éš”è¿™ä¸¤ç±»æ•°æ®ç‚¹ï¼Œè¿™è¡¨æ˜æ¨¡å‹æ²¡æœ‰å­¦ä¼šåŒºåˆ†å®ƒä»¬ã€‚

é€šå¸¸ï¼Œå°†æ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸ºé›¶ä¼šå¯¼è‡´ç½‘ç»œæ— æ³•æ‰“ç ´å¯¹ç§°æ€§ã€‚ è¿™æ„å‘³ç€æ¯ä¸€å±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒéƒ½å°†å­¦ä¹ ç›¸åŒçš„ä¸œè¥¿ï¼Œå¹¶ä¸”ä½ ä¸å¦¨è®­ç»ƒæ¯ä¸€å±‚$n^{[l]}=1$çš„ç¥ç»ç½‘ç»œï¼Œä¸”è¯¥ç½‘ç»œçš„æ€§èƒ½ä¸å¦‚çº¿æ€§åˆ†ç±»å™¨ï¼Œä¾‹å¦‚é€»è¾‘å›å½’ã€‚

**å› æ­¤ç»“è®ºå¦‚ä¸‹**ï¼š

- æƒé‡$W^{[l]}$åº”è¯¥éšæœºåˆå§‹åŒ–ä»¥æ‰“ç ´å¯¹ç§°æ€§ã€‚
- å°†åå·®$b^{[l]}$åˆå§‹åŒ–ä¸ºé›¶æ˜¯å¯ä»¥çš„ã€‚åªè¦éšæœºåˆå§‹åŒ–äº†$W^{[l]}$ï¼Œå¯¹ç§°æ€§ä»ç„¶ä¼šç ´åã€‚

### 1.2-éšæœºåˆå§‹åŒ–

â€‹	ä¸ºäº†æ‰“ç ´å¯¹ç§°æ€§ï¼Œè®©æˆ‘ä»¬éšæœºè®¾ç½®æƒé‡ã€‚ åœ¨éšæœºåˆå§‹åŒ–ä¹‹åï¼Œæ¯ä¸ªç¥ç»å…ƒå¯ä»¥ç»§ç»­å­¦ä¹ å…¶è¾“å…¥çš„ä¸åŒç‰¹å¾ã€‚ åœ¨æœ¬ç»ƒä¹ ä¸­ï¼Œä½ å°†çœ‹åˆ°å¦‚æœå°†æƒé‡éšæœºåˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„å€¼ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œå°†æƒé‡åˆå§‹åŒ–ä¸ºè¾ƒå¤§çš„éšæœºå€¼ï¼ˆæŒ‰*10ç¼©æ”¾ï¼‰ï¼Œå¹¶å°†åå·®è®¾ä¸º0ã€‚ å°† `np.random.randn(..,..) * 10`ç”¨äºæƒé‡ï¼Œå°†`np.zeros((.., ..))`ç”¨äºåå·®ã€‚

```python
# GRADED FUNCTION: initialize_parameters_random

def initialize_parameters_random(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
    parameters = {}
    L = len(layers_dims)            # integer representing the number of layers
    
    for l in range(1, L):
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*10
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###

    return parameters

parameters = initialize_parameters_random([3, 2, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

output:

```python
W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ä½¿ç”¨éšæœºåˆå§‹åŒ–è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒæ¨¡å‹ã€‚

```python
parameters = model(train_X, train_Y, initialization = "random")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

output:

```PYTHON
Cost after iteration 0: inf
Cost after iteration 1000: 0.6247924745506072
Cost after iteration 2000: 0.5980258056061102
Cost after iteration 3000: 0.5637539062842213
Cost after iteration 4000: 0.5501256393526495
Cost after iteration 5000: 0.5443826306793814
Cost after iteration 6000: 0.5373895855049121
Cost after iteration 7000: 0.47157999220550006
Cost after iteration 8000: 0.39770475516243037
Cost after iteration 9000: 0.3934560146692851
Cost after iteration 10000: 0.3920227137490125
Cost after iteration 11000: 0.38913700035966736
Cost after iteration 12000: 0.3861358766546214
Cost after iteration 13000: 0.38497629552893475
Cost after iteration 14000: 0.38276694641706693
On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86
```

![image-20240614115011370](images/image-20240614115011370.png)

å› ä¸ºæ•°å€¼èˆå…¥ï¼Œä½ å¯èƒ½åœ¨0è¿­ä»£ä¹‹åçœ‹åˆ°æŸå¤±ä¸º"inf"(infinite æ— é™)ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¹‹åç”¨æ›´å¤æ‚çš„æ•°å­—å®ç°è§£å†³æ­¤é—®é¢˜ã€‚

```python
print (predictions_train)
print (predictions_test)
```
outputï¼š
```python
[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]
```

```python
plt.title("Model with large random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

outputï¼š

![image-20240614115411087](images/image-20240614115411087.png)

- æŸå¤±ä¸€å¼€å§‹å¾ˆé«˜æ˜¯å› ä¸ºè¾ƒå¤§çš„éšæœºæƒé‡å€¼ï¼Œå¯¹äºæŸäº›æ•°æ®ï¼Œæœ€åä¸€å±‚æ¿€æ´»å‡½æ•°sigmoidè¾“å‡ºçš„ç»“æœéå¸¸æ¥è¿‘0æˆ–1ï¼Œå¹¶ä¸”å½“è¯¥ç¤ºä¾‹æ•°æ®é¢„æµ‹é”™è¯¯æ—¶ï¼Œå°†å¯¼è‡´éå¸¸é«˜çš„æŸå¤±ã€‚å½“$\log(a^{[3]}) = \log(0)$æ—¶ï¼ŒæŸå¤±è¾¾åˆ°æ— ç©·å¤§ã€‚
- åˆå§‹åŒ–ä¸å½“ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŒæ—¶ä¹Ÿä¼šå‡æ…¢ä¼˜åŒ–ç®—æ³•çš„é€Ÿåº¦ã€‚
- è®­ç»ƒè¾ƒé•¿æ—¶é—´çš„ç½‘ç»œï¼Œå°†ä¼šçœ‹åˆ°æ›´å¥½çš„ç»“æœï¼Œä½†æ˜¯ä½¿ç”¨å¤ªå¤§çš„éšæœºæ•°è¿›è¡Œåˆå§‹åŒ–ä¼šé™ä½ä¼˜åŒ–é€Ÿåº¦ã€‚

**æ€»ç»“**ï¼š

- å°†æƒé‡åˆå§‹åŒ–ä¸ºéå¸¸å¤§çš„éšæœºå€¼æ•ˆæœä¸ä½³ã€‚
- åˆå§‹åŒ–ä¸ºè¾ƒå°çš„éšæœºå€¼ä¼šæ›´å¥½ã€‚é‡è¦çš„é—®é¢˜æ˜¯ï¼šè¿™äº›éšæœºå€¼åº”ä¸ºå¤šå°ï¼Ÿè®©æˆ‘ä»¬åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­æ‰¾åˆ°ç­”æ¡ˆï¼

### 1.3-Heåˆå§‹åŒ–

â€‹	å°è¯•â€œHe åˆå§‹åŒ–â€ï¼Œè¯¥åç§°ä»¥Heç­‰äººçš„åå­—å‘½åï¼ˆç±»ä¼¼äºâ€œXavieråˆå§‹åŒ–â€ï¼Œä½†Xavieråˆå§‹åŒ–ä½¿ç”¨æ¯”ä¾‹å› å­ `sqrt(1./layers_dims[l-1])`æ¥è¡¨ç¤ºæƒé‡ğ‘Š[ğ‘™] ï¼Œè€ŒHeåˆå§‹åŒ–ä½¿ç”¨`sqrt(2./layers_dims[l-1])`ï¼‰ã€‚

â€‹	**ç»ƒä¹ **ï¼šå®ç°ä»¥ä¸‹å‡½æ•°ï¼Œä»¥Heåˆå§‹åŒ–æ¥åˆå§‹åŒ–å‚æ•°ã€‚

â€‹	**æç¤º**ï¼šæ­¤å‡½æ•°ç±»ä¼¼äºå…ˆå‰çš„`initialize_parameters_random(...)`ã€‚ å”¯ä¸€çš„ä¸åŒæ˜¯ï¼Œæ— éœ€å°†`np.random.randn(..,..)`ä¹˜ä»¥10ï¼Œè€Œæ˜¯å°†å…¶ä¹˜ä»¥2dimension of the previous layerï¼Œè¿™æ˜¯Heåˆå§‹åŒ–å»ºè®®ä½¿ç”¨çš„ReLUæ¿€æ´»å±‚ã€‚

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer. åŒ…å«æ¯å±‚å¤§å°çš„ python æ•°ç»„ï¼ˆåˆ—è¡¨ï¼‰ã€‚
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1 # integer representing the number of layers  æ­¤å¤„æ˜¯åŒ…å«æ¯å±‚å¤§å°çš„pythonæ•°ç»„ï¼Œè€Œå¹¶éç›´æ¥å®šä¹‰çš„å±‚æ•°ã€‚
     
    for l in range(1, L + 1): #ä»1åˆ°L
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layers_dims[l],1))
        ### END CODE HERE ###
        
    return parameters
parameters = initialize_parameters_he([2, 4, 1])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```python
W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]
```

è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œä½¿ç”¨Heåˆå§‹åŒ–å¹¶è¿­ä»£15,000æ¬¡ä»¥è®­ç»ƒä½ çš„æ¨¡å‹ã€‚

```py
parameters = model(train_X, train_Y, initialization = "he")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```python
Cost after iteration 0: 0.8830537463419761
Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
Cost after iteration 3000: 0.6526117768893805
Cost after iteration 4000: 0.6082958970572938
Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.4138645817071794
Cost after iteration 7000: 0.3117803464844441
Cost after iteration 8000: 0.23696215330322562
Cost after iteration 9000: 0.1859728720920684
Cost after iteration 10000: 0.15015556280371808
Cost after iteration 11000: 0.12325079292273551
Cost after iteration 12000: 0.09917746546525937
Cost after iteration 13000: 0.08457055954024283
Cost after iteration 14000: 0.07357895962677366
On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96
```

![image-20240617231530976](images/image-20240617231530976.png)

```python
plt.title("Model with He initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
# plot_decision_boundary çš„å‡½æ•°ï¼Œå®ƒç”¨äºç»˜åˆ¶å†³ç­–è¾¹ç•Œã€‚è¿™ä¸ªå‡½æ•°æ¥å—ä¸‰ä¸ªå‚æ•°ï¼š
	#ä¸€ä¸ª lambda å‡½æ•°ï¼šlambda x: predict_dec(parameters, x.T)ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒ¿åå‡½æ•°ï¼Œç”¨äºå¯¹è¾“å…¥çš„ x åº”ç”¨ predict_dec å‡½æ•°ã€‚è¿™é‡Œçš„ parameters æ˜¯é¢„å…ˆå®šä¹‰çš„æ¨¡å‹å‚æ•°ï¼Œè€Œ x.T æ˜¯å°†è¾“å…¥çš„ x è½¬ç½®ã€‚
    #train_Xï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®ç‰¹å¾çš„æ•°ç»„ã€‚
	#train_Yï¼šè¿™é€šå¸¸æ˜¯ä¸€ä¸ªåŒ…å«è®­ç»ƒæ•°æ®æ ‡ç­¾çš„æ•°ç»„ã€‚
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

![image-20240617234257865](images/image-20240617234257865.png)

# ç¥ç»ç½‘ç»œæ­£åˆ™åŒ–

â€‹	æ·±åº¦å­¦ä¹ æ¨¡å‹å…·æœ‰å¾ˆé«˜çš„çµæ´»æ€§å’Œèƒ½åŠ›ï¼Œå¦‚æœè®­ç»ƒæ•°æ®é›†ä¸å¤Ÿå¤§ï¼Œ**å°†ä¼šé€ æˆä¸€ä¸ªä¸¥é‡çš„é—®é¢˜--è¿‡æ‹Ÿåˆ**ã€‚å°½ç®¡å®ƒåœ¨è®­ç»ƒé›†ä¸Šæ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å­¦åˆ°çš„ç½‘ç»œ**ä¸èƒ½åº”ç”¨åˆ°æµ‹è¯•é›†ä¸­ï¼**

â€‹	é¦–å…ˆå¯¼å…¥è¦ä½¿ç”¨çš„åŒ…ã€‚

```python
# import packages
import numpy as np
import matplotlib.pyplot as plt
from lib.reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec
from lib.reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters
import sklearn
import sklearn.datasets
import scipy.io
from lib.testCases import *

plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'
```

**é—®é¢˜é™ˆè¿°**ï¼šä½ åˆšåˆšè¢«æ³•å›½è¶³çƒå…¬å¸è˜ä¸ºAIä¸“å®¶ã€‚ä»–ä»¬å¸Œæœ›ä½ æ¨èé¢„æµ‹æ³•å›½å®ˆé—¨å‘˜å°†çƒè¸¢å‡ºçš„ä½ç½®ï¼Œä»¥ä¾¿æ³•å›½é˜Ÿçš„çƒå‘˜å¯ä»¥ç”¨å¤´å°†çƒå‡»ä¸­ã€‚

![image-20240619213902974](images/image-20240619213902974.png)
	å®ˆé—¨å‘˜å°†çƒè¸¢åˆ°ç©ºä¸­ï¼Œæ¯æ”¯çƒé˜Ÿçš„çƒå‘˜éƒ½åœ¨å°½åŠ›ç”¨å¤´å‡»çƒ

â€‹	ä»¥ä¸‹æä¾›äº†æ³•å›½è¿‡å»10åœºæ¯”èµ›çš„äºŒç»´æ•°æ®é›†ã€‚

```python
train_X, train_Y, test_X, test_Y = load_2D_dataset()
```

output:

![image-20240619214954850](images/image-20240619214954850.png)

æ•°æ®ä¸­æ¯ä¸ªç‚¹å¯¹åº”äºè¶³çƒåœºä¸Šçš„ä½ç½®ï¼Œåœ¨è¯¥ä½ç½®ä¸Šï¼Œæ³•å›½å®ˆé—¨å‘˜ä»è¶³çƒåœºå·¦ä¾§å°„å‡ºçƒåï¼Œè¶³çƒè¿åŠ¨å‘˜ç”¨ä»–/å¥¹çš„å¤´éƒ¨å‡»ä¸­äº†çƒã€‚

- å¦‚æœåœ†ç‚¹ä¸ºè“è‰²ï¼Œåˆ™è¡¨ç¤ºæ³•å›½çƒå‘˜è®¾æ³•ç”¨å¤´éƒ¨å°†çƒå‡»ä¸­
- å¦‚æœåœ†ç‚¹ä¸ºçº¢è‰²ï¼Œåˆ™è¡¨ç¤ºå¦ä¸€æ”¯çƒé˜Ÿçš„çƒå‘˜ç”¨å¤´æ’çƒ

**ä½ çš„ç›®æ ‡**ï¼šè¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æµ‹å®ˆé—¨å‘˜åº”å°†çƒè¸¢åˆ°çƒåœºä¸Šçš„ä½ç½®ã€‚

**æ•°æ®é›†åˆ†æ**ï¼šè¯¥æ•°æ®é›†å«æœ‰å™ªå£°ï¼Œä½†ä¸€æ¡å°†å·¦ä¸ŠåŠéƒ¨åˆ†ï¼ˆè“è‰²ï¼‰ä¸å³ä¸‹åŠéƒ¨åˆ†ï¼ˆçº¢è‰²ï¼‰åˆ†å¼€çš„å¯¹è§’çº¿ä¼šå¾ˆæ¯”è¾ƒæœ‰æ•ˆã€‚

é¦–å…ˆå°è¯•éæ­£åˆ™åŒ–æ¨¡å‹ã€‚ç„¶åå­¦ä¹ å¦‚ä½•å¯¹å…¶è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå¹¶å†³å®šé€‰æ‹©å“ªç§æ¨¡å‹æ¥è§£å†³æ³•å›½è¶³çƒå…¬å¸çš„é—®é¢˜ã€‚

## 1-éæ­£åˆ™åŒ–æ¨¡å‹

ä½ å°†ä½¿ç”¨ä»¥ä¸‹ç¥ç»ç½‘ç»œï¼ˆå·²ä¸ºä½ å®ç°ï¼‰ï¼Œå¯ä»¥å¦‚ä¸‹ä½¿ç”¨æ­¤æ¨¡å‹ï¼š

- åœ¨*regularization mode*ä¸­ï¼Œé€šè¿‡`lambd`å°†è¾“å…¥è®¾ç½®ä¸ºéé›¶å€¼ã€‚æˆ‘ä»¬ä½¿ç”¨`lambd`ä»£æ›¿`lambda`ï¼Œå› ä¸º`lambda`æ˜¯Pythonä¸­çš„ä¿ç•™å…³é”®å­—ã€‚
- åœ¨*dropout mode*ä¸­ï¼Œå°†`keep_prob`è®¾ç½®ä¸ºå°äº1çš„å€¼

é¦–å…ˆï¼Œä½ å°†å°è¯•ä¸è¿›è¡Œä»»ä½•æ­£åˆ™åŒ–çš„æ¨¡å‹ã€‚ç„¶åï¼Œä½ å°†å®ç°ï¼š

- *L2 æ­£åˆ™åŒ–* å‡½æ•°ï¼š`compute_cost_with_regularization()`å’Œ`backward_propagation_with_regularization()`
- *Dropout* å‡½æ•°ï¼š`forward_propagation_with_dropout()`å’Œ`backward_propagation_with_dropout()`

```PYTHON
def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
    learning_rate -- learning rate of the optimization
    num_iterations -- number of iterations of the optimization loop
    print_cost -- If True, print the cost every 10000 iterations
    lambd -- regularization hyperparameter, scalar
    keep_prob - probability of keeping a neuron active during drop-out, scalar.
    
    Returns:
    parameters -- parameters learned by the model. They can then be used to predict.
    """
        
    grads = {}
    costs = []                            # to keep track of the cost
    m = X.shape[1]                        # number of examples æ ·æœ¬æ•°
    layers_dims = [X.shape[0], 20, 3, 1] #ä¸‰å±‚ç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€å±‚ä¸ºé¢„å¤„ç†åçš„æ ·æœ¬
    
    # Initialize parameters dictionary.
    parameters = initialize_parameters(layers_dims) #åˆå§‹åŒ–å‚æ•°wå’Œbï¼ˆå¤šä¸ªï¼‰

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
        elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob) #a3æ˜¯æœ€åä¸€å±‚çš„è¾“å‡º
        
        # Cost function
        if lambd == 0: #è¡¥å¿å­¦ä¹ ç‡
            cost = compute_cost(a3, Y)
        else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)  #æ˜¯å¯¹ç”¨wå¯¹æˆæœ¬å‡½æ•°è¿›è¡Œä¿®æ­£ï¼Œå› æ­¤ä¼ å…¥parametersè·å–wå‚æ•°
            
        # Backward propagation.
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache) #æ­£å¸¸åå‘ä¼ æ’­
        elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
        elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 10000 iterations
        if print_cost and i % 10000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
        if print_cost and i % 1000 == 0:
            costs.append(cost)
    
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (x1,000)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
parameters = model(train_X, train_Y)
print ("On the training set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
plt.title("Model without regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
plt.show()
```



åœ¨ä¸è¿›è¡Œä»»ä½•æ­£åˆ™åŒ–çš„æƒ…å†µä¸‹è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è§‚å¯Ÿè®­ç»ƒ/æµ‹è¯•é›†çš„å‡†ç¡®æ€§ã€‚

outputï¼š

```PYTHON
Cost after iteration 0: 0.6557412523481002
Cost after iteration 10000: 0.1632998752572417
Cost after iteration 20000: 0.13851642423284755
```

![image-20240619220547567](images/image-20240619220547567.png)



![image-20240619220641508](images/image-20240619220641508.png)

éæ­£åˆ™åŒ–æ¨¡å‹æ˜¾ç„¶è¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒé›†ï¼Œæ‹Ÿåˆäº†ä¸€äº›å™ªå£°ç‚¹ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å‡å°‘è¿‡æ‹Ÿåˆçš„ä¸¤ç§æ‰‹æ®µã€‚

## 2-L2æ­£åˆ™åŒ–	

é¿å…è¿‡æ‹Ÿåˆçš„æ ‡å‡†æ–¹æ³•ç§°ä¸º **L2æ­£åˆ™åŒ–**ï¼Œå®ƒå°†æŸå¤±å‡½æ•°ä»ï¼š
$$
J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}
$$
ä¿®æ”¹åˆ°ï¼š
$$
J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}
$$
åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œè¿™ä¸ªå…¬å¼ä¸­çš„ **k** å’Œ **j** ä»£è¡¨ä»¥ä¸‹å†…å®¹ï¼š

- **k** è¡¨ç¤ºç¥ç»ç½‘ç»œä¸­çš„ç¬¬ **k** ä¸ªç¥ç»å…ƒæˆ–èŠ‚ç‚¹ã€‚
- **j** è¡¨ç¤ºä¸ç¬¬ **k** ä¸ªç¥ç»å…ƒç›¸è¿çš„æƒé‡ã€‚

å‡è®¾æœ‰ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œéšè—å±‚æœ‰3ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸ªç¥ç»å…ƒä¸è¾“å…¥å±‚çš„4ä¸ªç‰¹å¾ç›¸è¿ã€‚æˆ‘ä»¬å¯ä»¥è¡¨ç¤ºè¿™ä¸ªéšè—å±‚çš„æƒé‡çŸ©é˜µä¸ºï¼š
$$
W^{[1]} = \begin{bmatrix}
0.5  ; -0.2 ; 0.8 ; 0.3 \\
-0.1 ; 0.6  ; -0.4; 0.7 \\
0.2  ; 0.4  ; 0.1 ; -0.5 \\
\end{bmatrix}
$$
ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæƒé‡çš„å¹³æ–¹å¹¶æ±‚å’Œï¼š
$$
\sum_{k}\sum_{j} W_{k,j}^{[1]2} = (0.5)^2 + (-0.2)^2 + 0.8^2 + 0.3^2 + (-0.1)^2 + 0.6^2 + (-0.4)^2 + 0.7^2 + 0.2^2 + 0.4^2 + 0.1^2 + (-0.5)^2
$$
è®¡ç®—ç»“æœä¸ºï¼š
$$
\sum_{k}\sum_{j} W_{k,j}^{[1]2} = 2.63
$$
â€‹	**ç»ƒä¹ **ï¼šå®ç°`compute_cost_with_regularizationï¼ˆï¼‰`ï¼Œä»¥è®¡ç®—å…¬å¼ï¼ˆ2ï¼‰çš„æŸå¤±ã€‚è¦è®¡ç®—$\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}$ ï¼Œè¯·ä½¿ç”¨ï¼š

```python
np.sum(np.square(Wl))
```

å¿…é¡»å¯¹$W^{[1]}$ï¼Œ$W^{[2]}$å’Œ$W^{[3]}$æ‰§è¡Œæ­¤æ“ä½œï¼Œç„¶åå°†ä¸‰ä¸ªé¡¹ç›¸åŠ å¹¶ä¹˜ä»¥$\frac{1}{m}\frac{\lambda}{2}$ã€‚

```PYTHON
def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost è®¡ç®—æŸå¤±å‡½æ•°
    
    ### START CODE HERE ### (approx. 1 line)
    L2_regularization_cost = (1./m*lambd/2)*(np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) #è®¡ç®—L2æ­£åˆ™åŒ–é¡¹
    ### END CODER HERE ###
    
    cost = cross_entropy_cost + L2_regularization_cost
    
    return cost

A3, Y_assess, parameters = compute_cost_with_regularization_test_case()

print("cost = " + str(compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))
```

output:

```PYTHON
cost = 1.7864859451590758
```

å½“ç„¶ï¼Œå› ä¸ºä½ æ›´æ”¹äº†æŸå¤±ï¼Œæ‰€ä»¥è¿˜å¿…é¡»æ›´æ”¹åå‘ä¼ æ’­ï¼ å¿…é¡»é’ˆå¯¹æ–°æŸå¤±å‡½æ•°è®¡ç®—æ‰€æœ‰æ¢¯åº¦ã€‚

**ç»ƒä¹ **ï¼šå®ç°æ­£åˆ™åŒ–åçš„åå‘ä¼ æ’­ã€‚æ›´æ”¹ä»…æ¶‰åŠdW1ï¼ŒdW2å’ŒdW3ã€‚å¯¹äºæ¯ä¸€ä¸ªï¼Œä½ å¿…é¡»æ·»åŠ æ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$ã€‚

éå¸¸å®¹æ˜“ã€æ ¹æ®å¯¼æ•°å››åˆ™è¿ç®—ã€æ±‚å¯¼åä¾æ—§ç›¸åŠ ï¼Œå³åœ¨dwåæ–¹æ·»åŠ $\frac{\lambda}{m} W$å³å¯ã€‚

```PYTHON

def backward_propagation_with_regularization(X, Y, cache, lambd):
    #å¯¼æ•°éƒ½æ˜¯ä»ä¼ å¯¼å›¾ä¸­ç›´æ¥æ¨ç®—å¾—åˆ°
    """
    Implements the backward propagation of our baseline model to which we added an L2 regularization.

    Arguments:
    X -- input dataset, of shape (input size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation()
    lambd -- regularization hyperparameter, scalar

    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """

    m = X.shape[1] #æ ·æœ¬æ•°
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache

    dZ3 = A3 - Y

    ### START CODE HERE ### (approx. 1 line)
    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd/m * W3
    ### END CODE HERE ###
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)

    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd/m * W2
    ### END CODE HERE ###
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)

    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW1 = 1./m * np.dot(dZ1, X.T) + lambd/m * W1
    ### END CODE HERE ###
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)

    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}

    return gradients

X_assess, Y_assess, cache = backward_propagation_with_regularization_test_case()

grads = backward_propagation_with_regularization(X_assess, Y_assess, cache, lambd = 0.7)
print ("dW1 = "+ str(grads["dW1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("dW3 = "+ str(grads["dW3"]))
```

outputï¼š

```PYTHON
dW1 = [[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 = [[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 = [[-1.77691347 -0.11832879 -0.09397446]]
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨L2æ­£åˆ™åŒ–(ğœ†=0.7)è¿è¡Œçš„æ¨¡å‹ã€‚`modelï¼ˆï¼‰`å‡½æ•°å°†è°ƒç”¨ï¼š

- `compute_cost_with_regularization`ä»£æ›¿`compute_cost`
- `backward_propagation_with_regularization`ä»£æ›¿`backward_propagation`

```PYTHON
parameters = model(train_X, train_Y, lambd = 0.7)
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```python
Cost after iteration 0: 0.6974484493131264
Cost after iteration 10000: 0.2684918873282239
Cost after iteration 20000: 0.2680916337127301
On the train set:
Accuracy: 0.9383886255924171
On the test set:
Accuracy: 0.93
```

![image-20240620222750656](images/image-20240620222750656.png)

å†³ç­–è¾¹ç•Œï¼š

```python
plt.title("Model with L2-regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

outputï¼š

![image-20240620222850603](images/image-20240620222850603.png)

- ğœ†  çš„å€¼æ˜¯ä½ å¯ä»¥è°ƒæ•´å¼€å‘é›†çš„è¶…å‚æ•°ã€‚
- L2æ­£åˆ™åŒ–ä½¿å†³ç­–è¾¹ç•Œæ›´å¹³æ»‘ã€‚å¦‚æœğœ† å¤ªå¤§ï¼Œåˆ™ä¹Ÿå¯èƒ½â€œè¿‡åº¦å¹³æ»‘â€ï¼Œä»è€Œä½¿æ¨¡å‹åå·®è¾ƒé«˜ã€‚

### 2.1-L2æ­£åˆ™åŒ–çš„åŸç†

L2æ­£åˆ™åŒ–åŸºäºä»¥ä¸‹å‡è®¾ï¼šæƒé‡è¾ƒå°çš„æ¨¡å‹æ¯”æƒé‡è¾ƒå¤§çš„æ¨¡å‹æ›´ç®€å•ã€‚å› æ­¤ï¼Œé€šè¿‡å¯¹æŸå¤±å‡½æ•°ä¸­æƒé‡çš„å¹³æ–¹å€¼è¿›è¡Œæƒ©ç½šï¼Œå¯ä»¥å°†æ‰€æœ‰æƒé‡é©±åŠ¨ä¸ºè¾ƒå°çš„å€¼ã€‚æ¯”é‡å¤ªå¤§ä¼šä½¿æŸå¤±è¿‡é«˜ï¼è¿™å°†å¯¼è‡´æ¨¡å‹æ›´å¹³æ»‘ï¼Œè¾“å‡ºéšç€è¾“å…¥çš„å˜åŒ–è€Œå˜åŒ–å¾—æ›´æ…¢ã€‚

 L2æ­£åˆ™åŒ–çš„å½±å“ï¼š

- æŸå¤±è®¡ç®—ï¼š
    \- æ­£åˆ™åŒ–æ¡ä»¶ä¼šæ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­
- åå‘ä¼ æ’­å‡½æ•°ï¼š
    \- æœ‰å…³æƒé‡çŸ©é˜µçš„æ¸å˜ä¸­è¿˜æœ‰å…¶ä»–æœ¯è¯­
- æƒé‡æœ€ç»ˆå˜å°ï¼ˆâ€œæƒé‡è¡°å‡â€ï¼‰ï¼š
    \- æƒé‡è¢«æ¨åˆ°è¾ƒå°çš„å€¼ã€‚

## 3-Dropout

**Dropout**æ˜¯å¹¿æ³›ç”¨äºæ·±åº¦å­¦ä¹ çš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚
**å®ƒä¼šåœ¨æ¯æ¬¡è¿­ä»£ä¸­éšæœºå…³é—­ä¸€äº›ç¥ç»å…ƒã€‚**

```
è¦äº†è§£Dropoutï¼Œå¯ä»¥æ€è€ƒä¸æœ‹å‹è¿›è¡Œä»¥ä¸‹å¯¹è¯ï¼š  
- æœ‹å‹ï¼šâ€œä¸ºä»€ä¹ˆä½ éœ€è¦æ‰€æœ‰ç¥ç»å…ƒæ¥è®­ç»ƒä½ çš„ç½‘ç»œä»¥åˆ†ç±»å›¾åƒï¼Ÿâ€ã€‚  
- ä½ ï¼šâ€œå› ä¸ºæ¯ä¸ªç¥ç»å…ƒéƒ½æœ‰æƒé‡ï¼Œå¹¶ä¸”å¯ä»¥å­¦ä¹ å›¾åƒçš„ç‰¹å®šç‰¹å¾/ç»†èŠ‚/å½¢çŠ¶ã€‚æˆ‘æ‹¥æœ‰çš„ç¥ç»å…ƒè¶Šå¤šï¼Œæ¨¡å‹å­¦ä¹ çš„ç‰¹å¾å°±è¶Šä¸°å¯Œï¼â€  
- æœ‹å‹ï¼šâ€œæˆ‘çŸ¥é“äº†ï¼Œä½†æ˜¯ä½ ç¡®å®šä½ çš„ç¥ç»å…ƒå­¦ä¹ çš„æ˜¯ä¸åŒçš„ç‰¹å¾è€Œä¸æ˜¯å…¨éƒ¨ç›¸åŒçš„ç‰¹å¾å—ï¼Ÿâ€  
- ä½ ï¼šâ€œè¿™æ˜¯ä¸ªå¥½é—®é¢˜â€¦â€¦åŒä¸€å±‚ä¸­çš„ç¥ç»å…ƒå®é™…ä¸Šå¹¶ä¸å…³è”ã€‚åº”è¯¥ç»å¯¹æœ‰å¯èƒ½è®©ä»–ä»¬å­¦ä¹ ç›¸åŒçš„å›¾åƒç‰¹å¾/å½¢çŠ¶/å½¢å¼/ç»†èŠ‚...è¿™æ˜¯å¤šä½™çš„ã€‚ä¸ºæ­¤åº”è¯¥æœ‰ä¸€ä¸ªè§£å†³æ–¹æ¡ˆã€‚â€
```

â€‹	åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä»¥æ¦‚ç‡$1 - keep\_prob$æˆ–ä»¥æ¦‚ç‡$keep\_prob$ï¼ˆæ­¤å¤„ä¸º50ï¼…ï¼‰å…³é—­æ­¤å±‚çš„æ¯ä¸ªç¥ç»å…ƒã€‚å…³é—­çš„ç¥ç»å…ƒå¯¹è¿­ä»£çš„æ­£å‘å’Œåå‘ä¼ æ’­å‡æ— åŠ©äºè®­ç»ƒã€‚

![Image Name](https://cdn.kesci.com/upload/image/q1au928m1v.gif?imageView2/0/w/960/h/960)

$1^{st}$å±‚ï¼šæˆ‘ä»¬å¹³å‡å…³é—­äº†40ï¼…çš„ç¥ç»å…ƒã€‚$3^{rd}$å±‚ï¼šæˆ‘ä»¬å¹³å‡å…³é—­äº†20ï¼…çš„ç¥ç»å…ƒã€‚

â€‹	å½“ä½ å…³é—­æŸäº›ç¥ç»å…ƒæ—¶ï¼Œå®é™…ä¸Šæ˜¯åœ¨ä¿®æ”¹æ¨¡å‹ã€‚DropoutèƒŒåçš„æƒ³æ³•æ˜¯ï¼Œåœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä½ å°†è®­ç»ƒä»…ä½¿ç”¨ç¥ç»å…ƒå­é›†çš„ä¸åŒæ¨¡å‹ã€‚é€šè¿‡Dropoutï¼Œä½ çš„ç¥ç»å…ƒå¯¹å¦ä¸€ç§ç‰¹å®šç¥ç»å…ƒçš„æ¿€æ´»å˜å¾—ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œå› ä¸ºå¦ä¸€ç¥ç»å…ƒå¯èƒ½éšæ—¶å…³é—­ã€‚

![Image Name](https://cdn.kesci.com/upload/image/q1aua8h2gs.gif?imageView2/0/w/960/h/960)

### 3.1-Dropoutæ­£å‘ä¼ æ’­

â€‹	**ç»ƒä¹ **ï¼šå®ç°å¸¦æœ‰Dropoutçš„æ­£å‘ä¼ æ’­ã€‚ä½ æ­£åœ¨ä½¿ç”¨3å±‚çš„ç¥ç»ç½‘ç»œï¼Œå¹¶å°†ä¸ºç¬¬ä¸€å’Œç¬¬äºŒéšè—å±‚æ·»åŠ Dropoutã€‚æˆ‘ä»¬ä¸ä¼šå°†Dropoutåº”ç”¨äºè¾“å…¥å±‚æˆ–è¾“å‡ºå±‚ã€‚

**è¯´æ˜**ï¼š
å…³é—­ç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚ä¸­çš„æŸäº›ç¥ç»å…ƒã€‚ä¸ºæ­¤ï¼Œå°†æ‰§è¡Œ4ä¸ªæ­¥éª¤ï¼š

1. æˆ‘ä»¬è®¨è®ºäº†ä½¿ç”¨`np.random.randï¼ˆï¼‰`åˆ›å»ºä¸$a^{[1]}$å½¢çŠ¶ç›¸åŒçš„å˜é‡$d^{[1]}$çš„æ–¹æ³•ï¼Œä»¥éšæœºè·å–0åˆ°1ä¹‹é—´çš„æ•°ã€‚åœ¨è¿™é‡Œï¼Œä½ å°†ä½¿ç”¨å‘é‡åŒ–çš„å®ç°ï¼Œåˆ›å»ºä¸€ä¸ªä¸$A^{[1]}$çš„çŸ©é˜µç»´åº¦ç›¸åŒçš„éšæœºçŸ©é˜µ$D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}]$ã€‚
2. é€šè¿‡å¯¹ğ·[1]ä¸­çš„å€¼è¿›è¡Œé˜ˆå€¼è®¾ç½®ï¼Œå°†$D^{[1]}$çš„æ¯ä¸ªæ¡ç›®è®¾ç½®ä¸º0ï¼ˆæ¦‚ç‡ä¸º`1-keep_prob`ï¼‰æˆ–1ï¼ˆæ¦‚ç‡ä¸º`keep_prob`ï¼‰ã€‚æç¤ºï¼šå°†çŸ©é˜µXçš„æ‰€æœ‰æ¡ç›®è®¾ç½®ä¸º0ï¼ˆå¦‚æœæ¦‚ç‡å°äº0.5ï¼‰æˆ–1ï¼ˆå¦‚æœæ¦‚ç‡å¤§äº0.5ï¼‰ï¼Œåˆ™å¯ä»¥æ‰§è¡Œï¼š`X = (X < 0.5)`ã€‚æ³¨æ„0å’Œ1åˆ†åˆ«å¯¹åº”Falseå’ŒTrueã€‚
3. å°†$A^{[1]}$è®¾ç½®ä¸º$A^{[1]} * D^{[1]}$ï¼ˆå…³é—­ä¸€äº›ç¥ç»å…ƒï¼‰ã€‚ä½ å¯ä»¥å°†$D^{[1]}$ è§†ä¸ºæ©ç ï¼Œè¿™æ ·å½“å®ƒä¸å¦ä¸€ä¸ªçŸ©é˜µç›¸ä¹˜æ—¶ï¼Œå…³é—­æŸäº›å€¼ã€‚
4. å°†$A^{[1]}$é™¤ä»¥`keep_prob`ã€‚é€šè¿‡è¿™æ ·åšï¼Œä½ å¯ä»¥ç¡®ä¿æŸå¤±ç»“æœä»å…·æœ‰ä¸dropoutç›¸åŒçš„æœŸæœ›å€¼ï¼Œç›¸å½“äºåŸæ¥çš„æœ‰`keep_prob`çš„æ¦‚ç‡è¢«ç•™ä¸‹æ¥ï¼Œå³æœŸæœ›ä¹˜keep_probï¼Œæ­¤å¤„å†é™¤äº†å°±ä¿æŒåŸæœŸæœ›ã€‚ï¼ˆæ­¤æŠ€æœ¯ä¹Ÿç§°ä¸ºåå‘dropoutï¼‰

```PYTHON

def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
    """
    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.
    
    Arguments:
    X -- input dataset, of shape (output size, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    A3 -- last activation value, output of the forward propagation, of shape (1,1)
    cache -- tuple, information stored for computing the backward propagation
    """
    
    np.random.seed(1)
    
    # retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]
    
    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. 
    D1 = np.random.rand(A1.shape[0],A1.shape[1])               # Step 1: initialize matrix D1 = np.random.rand(..., ...)
    D1 = D1 < keep_prob                                      # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
    A1 = A1 * D1                                         # Step 3: shut down some neurons of A1
    A1 = A1 / keep_prob                                        # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    ### START CODE HERE ### (approx. 4 lines)
    D2 = np.random.rand(A2.shape[0],A2.shape[1])               # Step 1: initialize matrix D2 = np.random.rand(..., ...)
    D2 = D2 < keep_prob                                         # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)
    A2 = A2 * D2                                         # Step 3: shut down some neurons of A2
    A2 = A2 / keep_prob                                      # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)

    return A3, cache

X_assess, parameters = forward_propagation_with_dropout_test_case()

A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)
print ("A3 = " + str(A3))
```

outputï¼š

```PYTHON
X_assess, parameters = forward_propagation_with_dropout_test_case()

A3, cache = forward_propagation_with_dropout(X_assess, parameters, keep_prob = 0.7)
print ("A3 = " + str(A3))
```

### 3.2-Dropoutåå‘ä¼ æ’­

**ç»ƒä¹ **ï¼šå®ç°å¸¦æœ‰dropoutçš„åå‘ä¼ æ’­ã€‚å’Œä¹‹å‰ä¸€æ ·ï¼Œè®­ç»ƒä¸€ä¸ª3å±‚çš„ç½‘ç»œã€‚ä½¿ç”¨å­˜å‚¨åœ¨ç¼“å­˜ä¸­çš„æ©ç $D^{[1]}$å’Œ$D^{[2]}$ï¼Œæ·»åŠ dropoutåˆ°ç¬¬ä¸€å’Œç¬¬äºŒä¸ªéšè—å±‚ã€‚

**è¯´æ˜**ï¼š
å¸¦æœ‰dropoutçš„åå‘ä¼ æ’­å®ç°ä¸Šéå¸¸å®¹æ˜“ã€‚ä½ å°†å¿…é¡»æ‰§è¡Œ2ä¸ªæ­¥éª¤ï¼š
1.åœ¨ä¸Šæ–¹é€šè¿‡åœ¨$A^{[1]}$ä¸Šåº”ç”¨æ©ç $D^{[1]}$æ¥å…³é—­æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­çš„æŸäº›ç¥ç»å…ƒã€‚åœ¨åå‘ä¼ æ’­ä¸­ï¼Œå¿…é¡»å°†ç›¸åŒçš„æ©ç $D^{[1]}$é‡æ–°åº”ç”¨äº$dA^{[1]}$æ¥å…³é—­ç›¸åŒçš„ç¥ç»å…ƒã€‚
2.åœ¨æ­£å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œä½ å·²å°†$A^{[1]}$é™¤ä»¥`keep_prob`ã€‚ å› æ­¤ï¼Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œå¿…é¡»å†æ¬¡å°†`dA1`é™¤ä»¥`keep_prob`ï¼ˆè®¡ç®—çš„è§£é‡Šæ˜¯ï¼Œå¦‚æœğ´[1]è¢«`keep_prob`ç¼©æ”¾ï¼Œåˆ™å…¶æ´¾ç”Ÿçš„ğ‘‘ğ´[1]ä¹Ÿç”±ç›¸åŒçš„`keep_prob`ç¼©æ”¾ï¼‰ã€‚

```PYTHON
def backward_propagation_with_dropout(X, Y, cache, keep_prob):
    """
    Implements the backward propagation of our baseline model to which we added dropout.
    
    Arguments:

    X -- input dataset, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation_with_dropout()
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """
    
    m = X.shape[1] #æ ·æœ¬æ•°
    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    dA2 = np.dot(W3.T, dZ3)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob           # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)

    dA1 = np.dot(W2.T, dZ2)
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
    dA1 = dA1 / keep_prob             # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)

    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients

X_assess, Y_assess, cache = backward_propagation_with_dropout_test_case()

gradients = backward_propagation_with_dropout(X_assess, Y_assess, cache, keep_prob = 0.8)

print ("dA1 = " + str(gradients["dA1"]))
print ("dA2 = " + str(gradients["dA2"]))
```

outputï¼š

```PYTHON
dA1 = [[ 0.36544439  0.         -0.00188233  0.         -0.17408748]
 [ 0.65515713  0.         -0.00337459  0.         -0.        ]]
dA2 = [[ 0.58180856  0.         -0.00299679  0.         -0.27715731]
 [ 0.          0.53159854 -0.          0.53159854 -0.34089673]
 [ 0.          0.         -0.00292733  0.         -0.        ]]
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨dropoutï¼ˆ`keep_prob = 0.86`ï¼‰è¿è¡Œæ¨¡å‹ã€‚ è¿™æ„å‘³ç€åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä½ éƒ½ä»¥24ï¼…çš„æ¦‚ç‡å…³é—­ç¬¬1å±‚å’Œç¬¬2å±‚çš„æ¯ä¸ªç¥ç»å…ƒã€‚ å‡½æ•°`model()`å°†è°ƒç”¨ï¼š

- `forward_propagation_with_dropout`è€Œä¸æ˜¯`forward_propagation`ã€‚
- `backward_propagation_with_dropout`ï¼Œè€Œä¸æ˜¯`backward_propagation`ã€‚

```PYTHON
parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)

print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
```

outputï¼š

```PYTHON
Cost after iteration 0: 0.6543912405149825
Cost after iteration 10000: 0.0610169865749056
Cost after iteration 20000: 0.060582435798513114
On the train set:
Accuracy: 0.9289099526066351
On the test set:
Accuracy: 0.95
```

![image-20240620231309071](images/image-20240620231309071.png)

å†³ç­–è¾¹ç•Œ

```python
plt.title("Model with dropout")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

![image-20240620231344425](images/image-20240620231344425.png)

**æ³¨æ„**ï¼š

- ä½¿ç”¨dropoutæ—¶çš„**å¸¸è§é”™è¯¯**æ˜¯åœ¨è®­ç»ƒå’Œæµ‹è¯•ä¸­éƒ½ä½¿ç”¨ã€‚ä½ åªèƒ½åœ¨è®­ç»ƒä¸­ä½¿ç”¨dropoutï¼ˆéšæœºåˆ é™¤èŠ‚ç‚¹ï¼‰ã€‚
- æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œä¾‹å¦‚[tensorflow](https://www.tensorflow.org/api_docs/python/tf/nn/dropout), [PaddlePaddle](http://doc.paddlepaddle.org/release_doc/0.9.0/doc/ui/api/trainer_config_helpers/attrs.html), [keras](https://keras.io/layers/core/#dropout)æˆ–è€… [caffe](http://caffe.berkeleyvision.org/tutorial/layers/dropout.html) é™„å¸¦dropoutå±‚çš„å®ç°ã€‚

**å…³dropoutåº”è¯¥è®°ä½çš„äº‹æƒ…ï¼š**

- dropoutæ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ã€‚
- ä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨dropoutï¼Œåœ¨æµ‹è¯•æœŸé—´ä¸è¦ä½¿ç”¨ã€‚
- åœ¨æ­£å‘å’Œåå‘ä¼ æ’­æœŸé—´å‡åº”ç”¨dropoutã€‚
- åœ¨è®­ç»ƒæœŸé—´ï¼Œå°†æ¯ä¸ªdropoutå±‚é™¤ä»¥keep_probï¼Œä»¥ä¿æŒæ¿€æ´»çš„æœŸæœ›å€¼ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœkeep_probä¸º0.5ï¼Œåˆ™å¹³å‡è€Œè¨€ï¼Œæˆ‘ä»¬å°†å…³é—­ä¸€åŠçš„èŠ‚ç‚¹ï¼Œå› æ­¤è¾“å‡ºå°†æŒ‰0.5ç¼©æ”¾ï¼Œå› ä¸ºåªæœ‰å‰©ä½™çš„ä¸€åŠå¯¹è§£å†³æ–¹æ¡ˆæœ‰æ‰€è´¡çŒ®ã€‚é™¤ä»¥0.5ç­‰äºä¹˜ä»¥2ï¼Œå› æ­¤è¾“å‡ºç°åœ¨å…·æœ‰ç›¸åŒçš„æœŸæœ›å€¼ã€‚ä½ å¯ä»¥æ£€æŸ¥æ­¤æ–¹æ³•æ˜¯å¦æœ‰æ•ˆï¼Œå³ä½¿keep_probçš„å€¼ä¸æ˜¯0.5ã€‚

**ä»æ­¤ç¬”è®°æœ¬ä¸­è®°ä½çš„å†…å®¹**ï¼š

- æ­£åˆ™åŒ–å°†å¸®åŠ©å‡å°‘è¿‡æ‹Ÿåˆã€‚
- æ­£åˆ™åŒ–å°†ä½¿æƒé‡é™ä½åˆ°è¾ƒä½çš„å€¼ã€‚
- L2æ­£åˆ™åŒ–å’ŒDropoutæ˜¯ä¸¤ç§éå¸¸æœ‰æ•ˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚

# ç¥ç»ç½‘ç»œæ¢¯åº¦æ£€éªŒ

â€‹	å‡è®¾ä½ æ˜¯è‡´åŠ›äºåœ¨å…¨çƒèŒƒå›´å†…æä¾›ç§»åŠ¨æ”¯ä»˜çš„å›¢é˜Ÿçš„ä¸€å‘˜ï¼Œè¢«ä¸Šçº§è¦æ±‚å»ºç«‹æ·±åº¦å­¦ä¹ æ¨¡å‹æ¥æ£€æµ‹æ¬ºè¯ˆè¡Œä¸º--æ¯å½“æœ‰äººè¿›è¡Œæ”¯ä»˜æ—¶ï¼Œä½ éƒ½åº”è¯¥ç¡®è®¤è¯¥æ”¯ä»˜æ˜¯å¦å¯èƒ½æ˜¯æ¬ºè¯ˆæ€§çš„ï¼Œä¾‹å¦‚ç”¨æˆ·çš„å¸æˆ·å·²è¢«é»‘å®¢å…¥ä¾µã€‚

â€‹	ä½†æ˜¯æ¨¡å‹çš„åå‘ä¼ æ’­å¾ˆéš¾å®ç°ï¼Œæœ‰æ—¶è¿˜ä¼šæœ‰é”™è¯¯ã€‚å› ä¸ºè¿™æ˜¯å…³é”®çš„åº”ç”¨ä»»åŠ¡ï¼Œæ‰€ä»¥ä½ å…¬å¸çš„CEOè¦åå¤ç¡®å®šåå‘ä¼ æ’­çš„å®ç°æ˜¯æ­£ç¡®çš„ã€‚CEOè¦æ±‚ä½ è¯æ˜ä½ çš„åå‘ä¼ æ’­å®é™…ä¸Šæ˜¯æœ‰æ•ˆçš„ï¼ä¸ºäº†ä¿è¯è¿™ä¸€ç‚¹ï¼Œä½ å°†åº”ç”¨åˆ°â€œæ¢¯åº¦æ£€éªŒâ€ã€‚

```python
import numpy as np
from lib.testCases import *
from lib.gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector
```

## 1-æ¢¯åº¦æ£€éªŒåŸç†

åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦$\frac{\partial J}{\partial \theta}$ï¼Œå…¶ä¸­ğœƒè¡¨ç¤ºæ¨¡å‹çš„å‚æ•°ã€‚ä½¿ç”¨æ­£å‘ä¼ æ’­å’ŒæŸå¤±å‡½æ•°æ¥è®¡ç®—$ğ½$ã€‚

ç”±äºæ­£å‘ä¼ æ’­ç›¸å¯¹å®¹æ˜“å®ç°ï¼Œç›¸ä¿¡ä½ æœ‰ä¿¡å¿ƒèƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œç¡®å®š100ï¼…è®¡ç®—æ­£ç¡®çš„æŸå¤±$ğ½$ã€‚ä¸ºæ­¤ï¼Œä½ å¯ä»¥ä½¿ç”¨$ğ½$æ¥éªŒè¯ä»£ç $\frac{\partial J}{\partial \theta}$ã€‚

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹å¯¼æ•°ï¼ˆæˆ–è€…è¯´æ¢¯åº¦ï¼‰çš„å®šä¹‰ï¼š
$$
\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}
$$
æˆ‘ä»¬çŸ¥é“ä»¥ä¸‹å†…å®¹ï¼š

- $\frac{\partial J}{\partial \theta}$æ˜¯ä½ è¦ç¡®ä¿è®¡ç®—æ­£ç¡®çš„å¯¹è±¡ã€‚
- ä½ å¯ä»¥è®¡ç®—$J(\theta + \varepsilon)$å’Œ$J(\theta - \varepsilon)$ï¼ˆåœ¨ğœƒæ˜¯å®æ•°çš„æƒ…å†µä¸‹ï¼‰ï¼Œå› ä¸ºè¦ä¿è¯ğ½çš„å®ç°æ˜¯æ­£ç¡®çš„ã€‚

è®©æˆ‘ä»¬ä½¿ç”¨æ–¹ç¨‹å¼ï¼ˆ1ï¼‰å’Œ ğœ€çš„ä¸€ä¸ªå°å€¼æ¥è¯´æœCEOä½ è®¡ç®—âˆ‚ğ½âˆ‚ğœƒçš„ä»£ç æ˜¯æ­£ç¡®çš„ï¼

## 2-ä¸€ç»´æ¢¯åº¦æ£€æŸ¥

æ€è€ƒä¸€ç»´çº¿æ€§å‡½æ•°$J(\theta) = \theta x$ï¼Œè¯¥æ¨¡å‹ä»…åŒ…å«ä¸€ä¸ªå®æ•°å€¼å‚æ•°$\theta$ï¼Œå¹¶ä»¥$ğ‘¥$ä½œä¸ºè¾“å…¥ã€‚

ä½ å°†å®ç°ä»£ç ä»¥è®¡ç®—$ ğ½(.)$åŠå…¶æ´¾ç”Ÿ$\frac{\partial J}{\partial \theta}$ï¼Œç„¶åï¼Œä½ å°†ä½¿ç”¨æ¢¯åº¦æ£€éªŒæ¥ç¡®ä¿$ğ½$çš„å¯¼æ•°è®¡ç®—æ­£ç¡®ã€‚

ä¸‹å›¾æ˜¾ç¤ºäº†å…³é”®çš„è®¡ç®—æ­¥éª¤ï¼šé¦–å…ˆä»$ğ‘¥$å¼€å§‹ï¼Œå†è¯„ä¼°å‡½æ•°$ğ½(ğ‘¥)$ï¼ˆæ­£å‘ä¼ æ’­ï¼‰ï¼Œç„¶åè®¡ç®—å¯¼æ•°$\frac{\partial J}{\partial \theta}$ï¼ˆåå‘ä¼ æ’­ï¼‰ã€‚

![image-20240620233749298](images/image-20240620233749298.png)

**ç»ƒä¹ **ï¼šä¸ºæ­¤ç®€å•å‡½æ•°å®ç°â€œæ­£å‘ä¼ æ’­â€å’Œâ€œå‘åä¼ æ’­â€ã€‚ å³åœ¨ä¸¤ä¸ªå•ç‹¬çš„å‡½æ•°ä¸­ï¼Œè®¡ç®—$ğ½(.)$ ï¼ˆæ­£å‘ä¼ æ’­ï¼‰åŠå…¶ç›¸å¯¹äº$\theta$(åå‘ä¼ æ’­ï¼‰çš„å¯¼æ•°ã€‚

```PYTHON
# GRADED FUNCTION: forward_propagation

def forward_propagation(x, theta):
    """
    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)
    
    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    
    Returns:
    J -- the value of function J, computed using the formula J(theta) = theta * x
    """
    
    ### START CODE HERE ### (approx. 1 line)
    J = theta * x
    ### END CODE HERE ###
    
    return J
```

**ç»ƒä¹ **ï¼šç°åœ¨ï¼Œæ‰§è¡Œå›¾1çš„åå‘ä¼ æ’­æ­¥éª¤ï¼ˆå¯¼æ•°è®¡ç®—ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè®¡ç®—ğ½(ğœƒ)=ğœƒğ‘¥ ç›¸å¯¹äº ğœƒçš„å¯¼æ•°ã€‚ä¸ºé¿å…è¿›è¡Œæ¼”ç®—ï¼Œä½ åº”è¯¥å¾—åˆ°$d\theta = \frac { \partial J }{ \partial \theta} = x$ã€‚

```PYTHON
# GRADED FUNCTION: backward_propagation

def backward_propagation(x, theta):
    """
    Computes the derivative of J with respect to theta (see Figure 1).
    
    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    
    Returns:
    dtheta -- the gradient of the cost with respect to theta
    """
    
    ### START CODE HERE ### (approx. 1 line)
    dtheta = x
    ### END CODE HERE ###

    
    return dtheta
```

**ç»ƒä¹ **ï¼šä¸ºäº†å±•ç¤º`backward_propagationï¼ˆï¼‰`å‡½æ•°æ­£ç¡®è®¡ç®—äº†æ¢¯åº¦$\frac { \partial J }{ \partial \theta}$ï¼Œè®©æˆ‘ä»¬å®æ–½æ¢¯åº¦æ£€éªŒã€‚

**è¯´æ˜**ï¼š

- é¦–å…ˆä½¿ç”¨ä¸Šå¼ï¼ˆ1ï¼‰å’Œ$\varepsilon$çš„æå°å€¼è®¡ç®—â€œgradapproxâ€ã€‚ä»¥ä¸‹æ˜¯è¦éµå¾ªçš„æ­¥éª¤

$$
\theta^{+} = \theta + \varepsilon
$$

$$
\theta^{+} = \theta - \varepsilon
$$

$$
J^{+} = J(\theta^{+})
$$

$$
J^{-} = J(\theta^{-})
$$

$$
gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon}
$$

- ç„¶åä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œå¹¶å°†ç»“æœå­˜å‚¨åœ¨å˜é‡â€œgradâ€ä¸­
- æœ€åï¼Œä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—â€œgradapproxâ€å’Œâ€œgradâ€ä¹‹é—´çš„ç›¸å¯¹å·®ï¼š

$$
difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2} \tag{2}
$$

- `np.linalg.norm()` æ˜¯ NumPy åº“ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªå‘é‡çš„èŒƒæ•°ã€‚å®ƒå¯ä»¥è®¡ç®—ä¸åŒç»´åº¦çš„å‘é‡çš„ä¸åŒèŒƒæ•°ï¼Œæ¯”å¦‚å‘é‡çš„ L1 èŒƒæ•°ã€L2 èŒƒæ•°ç­‰ã€‚
  1. **åŸºæœ¬ç”¨æ³•**ï¼š
     - `np.linalg.norm(x, ord=None, axis=None, keepdims=False)`
     - `x`ï¼šè¡¨ç¤ºè¦è®¡ç®—èŒƒæ•°çš„å‘é‡æˆ–çŸ©é˜µã€‚
     - `ord`ï¼šè¡¨ç¤ºèŒƒæ•°çš„ç±»å‹ã€‚
     - `axis`ï¼šè¡¨ç¤ºè®¡ç®—èŒƒæ•°çš„è½´ã€‚
     - å‘é‡çš„èŒƒæ•°ï¼š
       - L1 èŒƒæ•°ï¼šè¡¨ç¤ºæ±‚åˆ—å’Œçš„æœ€å¤§å€¼ã€‚
       - L2 èŒƒæ•°ï¼šè¡¨ç¤ºæ±‚ç‰¹å¾å€¼ï¼Œç„¶åæ±‚æœ€å¤§ç‰¹å¾å€¼çš„ç®—æœ¯å¹³æ–¹æ ¹ã€‚
       - Lâˆ èŒƒæ•°ï¼šè¡¨ç¤ºæ±‚è¡Œå’Œçš„æœ€å¤§å€¼ã€‚
       - `ord=None`ï¼šè¡¨ç¤ºæ±‚æ•´ä½“çš„çŸ©é˜µå…ƒç´ å¹³æ–¹å’Œï¼Œå†å¼€æ ¹å·ã€‚
- ä½ éœ€è¦3ä¸ªæ­¥éª¤æ¥è®¡ç®—æ­¤å…¬å¼ï¼š
  - 1. ä½¿ç”¨np.linalg.normï¼ˆ...ï¼‰è®¡ç®—åˆ†å­
  - 2. è®¡ç®—åˆ†æ¯ï¼Œè°ƒç”¨np.linalg.normï¼ˆ...ï¼‰ä¸¤æ¬¡
  - 3. ç›¸é™¤
- å¦‚æœå·®å¼‚å¾ˆå°ï¼ˆä¾‹å¦‚å°äº$10^{-7}$ï¼‰ï¼Œåˆ™å¯ä»¥ç¡®ä¿¡æ­£ç¡®è®¡ç®—äº†æ¢¯åº¦ã€‚å¦åˆ™ï¼Œæ¢¯åº¦è®¡ç®—å¯èƒ½ä¼šå‡ºé”™ã€‚

```python
# GRADED FUNCTION: gradient_check

def gradient_check(x, theta, epsilon = 1e-7):
    """
    Implement the backward propagation presented in Figure 1.
    
    Arguments:
    x -- a real-valued input
    theta -- our parameter, a real number as well
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    
    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.
    ### START CODE HERE ### (approx. 5 lines)
    thetaplus = theta + epsilon                               # Step 1
    thetaminus = theta - epsilon                              # Step 2
    J_plus = forward_propagation(x, thetaplus)                                  # Step 3
    J_minus = forward_propagation(x, thetaminus)                                 # Step 4
    gradapprox = (J_plus - J_minus) / (2 * epsilon)                              # Step 5
    ### END CODE HERE ###

    # Check if gradapprox is close enough to the output of backward_propagation()
    ### START CODE HERE ### (approx. 1 line)
    grad = backward_propagation(x, theta)
    ### END CODE HERE ###

    ### START CODE HERE ### (approx. 1 line)
    numerator = np.linalg.norm(grad - gradapprox)                               # Step 1'
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                            # Step 2'
    difference = numerator / denominator                              # Step 3'
    ### END CODE HERE ###
    
    if difference < 1e-7:
        print ("The gradient is correct!")
    else:
        print ("The gradient is wrong!")
    
    return difference
x, theta = 2, 4
difference = gradient_check(x, theta)
print("difference = " + str(difference))
```

outputï¼š

```PYTHON
The gradient is correct!
difference = 2.919335883291695e-10
```

## 3-Nç»´æ¢¯åº¦æ£€éªŒ

åœ¨æ›´ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼ŒæŸå¤±å‡½æ•°$ğ½$å…·æœ‰å¤šä¸ªå•ä¸ªè¾“å…¥ã€‚å½“ä½ è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œ$ğœƒ$å®é™…ä¸Šç”±å¤šä¸ªçŸ©é˜µ$W^{[l]}$ç»„æˆï¼Œå¹¶åŠ ä¸Šåå·®$b^{[l]}$ï¼é‡è¦çš„æ˜¯è¦çŸ¥é“å¦‚ä½•å¯¹é«˜ç»´è¾“å…¥è¿›è¡Œæ¢¯åº¦æ£€éªŒã€‚

ä¸‹å›¾æè¿°äº†æ¬ºè¯ˆæ£€æµ‹æ¨¡å‹çš„æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼š

![image-20240620235147746](images/image-20240620235147746.png)

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„å®ç°ã€‚

```PYTHON
def forward_propagation_n(X, Y, parameters):
    """
    Implements the forward propagation (and computes the cost) presented in Figure 3.
    
    Arguments:
    X -- training set for m examples
    Y -- labels for m examples 
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    
    Returns:
    cost -- the cost function (logistic cost for one example)
    """
    
    # retrieve parameters
    m = X.shape[1]
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    # Cost
    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)
    cost = 1./m * np.sum(logprobs)
    
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    
    return cost, cache
```

```PYTHON
def backward_propagation_n(X, Y, cache):
    """
    Implement the backward propagation presented in figure 2.
    
    Arguments:
    X -- input datapoint, of shape (input size, 1)
    Y -- true "label"
    cache -- cache output from forward_propagation_n()
    
    Returns:
    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T) * 2
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,
                 "dA2": dA2, "dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dA1": dA1, "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
```

### 3.1-Nç»´æ¢¯åº¦æ£€éªŒåŸç†

ä½ æƒ³å°†â€œgradapproxâ€ä¸é€šè¿‡åå‘ä¼ æ’­è®¡ç®—çš„æ¢¯åº¦è¿›è¡Œæ¯”è¾ƒã€‚å…¬å¼ä»ç„¶æ˜¯ï¼š
$$
\frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}
$$
ä½†æ˜¯ï¼Œ$ğœƒ$ä¸å†æ˜¯æ ‡é‡ã€‚ è€Œæ˜¯ä¸€ä¸ªå«åšâ€œå‚æ•°â€çš„å­—å…¸ã€‚ æˆ‘ä»¬ä¸ºä½ å®ç°äº†ä¸€ä¸ªå‡½æ•°"`dictionary_to_vector()`"ã€‚å®ƒå°†â€œå‚æ•°â€å­—å…¸è½¬æ¢ä¸ºç§°ä¸ºâ€œå€¼â€çš„å‘é‡ï¼Œè¯¥å‘é‡æ˜¯é€šè¿‡å°†æ‰€æœ‰å‚æ•°($W^{[1]}$,$b^{[1]}$, $W^{[2]}$, $b^{[2]}$,$W^{[3]}$, $b^{[3]}$)é‡å¡‘ä¸ºå‘é‡å¹¶å°†å®ƒä»¬ä¸²è”è€Œè·å¾—çš„ã€‚ï¼ˆæ­¤å¤„ç”Ÿæˆçš„è¡Œä¸ºæ•°é‡ï¼Œåˆ—ä¸º1ï¼‰

åå‡½æ•°æ˜¯â€œ`vector_to_dictionary`â€ï¼Œå®ƒè¾“å‡ºå›â€œparametersâ€å­—å…¸ã€‚

![image-20240620235836335](images/image-20240620235836335.png)

å°†åœ¨ gradient_check_n()ä¸­ç”¨åˆ°è¿™äº›å‡½æ•°

æˆ‘ä»¬è¿˜ä½¿ç”¨gradients_to_vector()å°†â€œgradientsâ€å­—å…¸è½¬æ¢ä¸ºå‘é‡â€œgradâ€ã€‚

- `np.copy()` çš„ä¸»è¦åŠŸèƒ½æ˜¯åˆ›å»ºä¸€ä¸ªæ•°ç»„æˆ–çŸ©é˜µçš„å‰¯æœ¬ã€‚
- è¿™ä¸ªå‰¯æœ¬ä¸åŸå§‹æ•°ç»„æˆ–çŸ©é˜µåœ¨å†…å­˜ä¸­æ˜¯ç‹¬ç«‹çš„ï¼Œå¯¹å‰¯æœ¬çš„ä¿®æ”¹ä¸ä¼šå½±å“åˆ°åŸå§‹æ•°æ®ã€‚

**ç»ƒä¹ **ï¼šå®ç°gradient_check_n()ã€‚

**è¯´æ˜**ï¼šè¿™æ˜¯ä¼ªä»£ç ï¼Œå¯å¸®åŠ©ä½ å®ç°æ¢¯åº¦æ£€éªŒã€‚

For each i in num_parameters:

- è®¡ç®—

  `J_plus [i] `:

  1. å°†$\theta^{+}$è®¾ä¸º `np.copy(parameters_values)`  
  
  2. å°†$\theta^{+}_i$è®¾ä¸º$\theta^{+}_i + \varepsilon$
  
- è®¡ç®—`J_minus [i]`ï¼š

    1.å°†$\theta^{-}$è®¾ä¸º `np.copy(parameters_values)`  

    2.å°†$\theta^{-}_i$è®¾ä¸º$\theta^{-}_i - \varepsilon$


- $gradapprox[i] = \frac{J^{+}_i - J^{-}_i}{2 \varepsilon}$

å› æ­¤ï¼Œä½ å°†è·å¾—å‘é‡`gradapprox`ï¼Œå…¶ä¸­`gradapprox[i]`æ˜¯ç›¸å¯¹äº`parameter_values[i]`çš„æ¢¯åº¦çš„è¿‘ä¼¼å€¼ã€‚ç°åœ¨ï¼Œä½ å¯ä»¥å°†æ­¤`gradapprox`å‘é‡ä¸åå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦å‘é‡è¿›è¡Œæ¯”è¾ƒã€‚å°±åƒä¸€ç»´æƒ…å†µï¼ˆæ­¥éª¤1'ï¼Œ2'ï¼Œ3'ï¼‰ä¸€æ ·è®¡ç®—ï¼š
$$
difference = \frac {\| grad - gradapprox \|_2}{\| grad \|_2 + \| gradapprox \|_2 } \tag{3}
$$

```PYTHON

def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n
    
    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
    x -- input datapoint, of shape (input size, 1)
    y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    
    # Set-up variables
    parameters_values, _ = dictionary_to_vector(parameters)
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0] # number of parameters ï¼ˆ47ï¼Œ1ï¼‰ è¡Œä¸ºæ•°é‡
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))
    
    # Compute gradapprox
    for i in range(num_parameters):
        
        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have to outputs two parameters but we only care about the first one
        ### START CODE HERE ### (approx. 3 lines)
        thetaplus = np.copy(parameters_values)                                      # Step 1
        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2
        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))    #æˆæœ¬å‡½æ•°     # Step 3
        ### END CODE HERE ###

        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        ### START CODE HERE ### (approx. 3 lines)
        thetaminus = np.copy(parameters_values)                                     # Step 1
        thetaminus[i][0] = thetaminus[i][0] - epsilon                            # Step 2        
        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  # Step 3
        ### END CODE HERE ###

        # Compute gradapprox[i]
        ### START CODE HERE ### (approx. 1 line)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2.* epsilon)
        ### END CODE HERE ###

    # Compare gradapprox to backward propagation gradients by computing difference.
    ### START CODE HERE ### (approx. 1 line)
    numerator = np.linalg.norm(grad - gradapprox)                                           # Step 1'
    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                                         # Step 2'
    difference = numerator / denominator                                          # Step 3'
    ### END CODE HERE ###

    if difference > 1e-7:
        print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
    else:
        print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")
    
    return difference

X, Y, parameters = gradient_check_n_test_case()

cost, cache = forward_propagation_n(X, Y, parameters)
gradients = backward_propagation_n(X, Y, cache)
difference = gradient_check_n(parameters, gradients, X, Y)
```

outputï¼š

```PYTHON
There is a mistake in the backward propagation! difference = 0.2850931567761624
```

â€‹	çœ‹èµ·æ¥`backward_propagation_n`ä»£ç ä¼¼ä¹æœ‰é”™è¯¯ï¼å¾ˆå¥½ï¼Œä½ å·²ç»å®ç°äº†æ¢¯åº¦æ£€éªŒã€‚è¿”å›åˆ°`backward_propagation`å¹¶å°è¯•æŸ¥æ‰¾/æ›´æ­£é”™è¯¯*ï¼ˆæç¤ºï¼šæ£€æŸ¥dW2å’Œdb1ï¼‰*ã€‚å¦‚æœä½ å·²è§£å†³é—®é¢˜ï¼Œè¯·é‡æ–°è¿è¡Œæ¢¯åº¦æ£€éªŒã€‚

ä½ å¯ä»¥è¿›è¡Œæ¢¯åº¦æ£€éªŒæ¥è¯æ˜ä½ çš„å¯¼æ•°è®¡ç®—çš„æ­£ç¡®å—ï¼Ÿå³ä½¿ä½œä¸šçš„è¿™ä¸€éƒ¨åˆ†æ²¡æœ‰è¯„åˆ†ï¼Œæˆ‘ä»¬ä¹Ÿå¼ºçƒˆå»ºè®®ä½ å°è¯•æŸ¥æ‰¾é”™è¯¯å¹¶é‡æ–°è¿è¡Œæ¢¯åº¦æ£€éªŒï¼Œç›´åˆ°ç¡®ä¿¡å®ç°äº†æ­£ç¡®çš„åå‘ä¼ æ’­ã€‚

**æ³¨æ„**

- æ¢¯åº¦æ£€éªŒå¾ˆæ…¢ï¼ç”¨$\frac{\partial J}{\partial \theta} \approx  \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}$é€¼è¿‘æ¢¯åº¦åœ¨è®¡ç®—ä¸Šæ˜¯å¾ˆè€—è´¹èµ„æºçš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨è®­ç»ƒæœŸé—´çš„æ¯æ¬¡è¿­ä»£ä¸­éƒ½è¿›è¡Œæ¢¯åº¦æ£€éªŒã€‚åªéœ€æ£€æŸ¥å‡ æ¬¡æ¢¯åº¦æ˜¯å¦æ­£ç¡®ã€‚
- è‡³å°‘å¦‚æˆ‘ä»¬ä»‹ç»çš„é‚£æ ·ï¼Œæ¢¯åº¦æ£€éªŒä¸é€‚ç”¨äºdropoutã€‚é€šå¸¸ï¼Œä½ å°†è¿è¡Œä¸å¸¦dropoutçš„æ¢¯åº¦æ£€éªŒç®—æ³•ä»¥ç¡®ä¿ä½ çš„backpropæ˜¯æ­£ç¡®çš„ï¼Œç„¶åæ·»åŠ dropoutã€‚
- æ¢¯åº¦æ£€éªŒå¯éªŒè¯åå‘ä¼ æ’­çš„æ¢¯åº¦ä¸æ¢¯åº¦çš„æ•°å€¼è¿‘ä¼¼å€¼ä¹‹é—´çš„æ¥è¿‘åº¦ï¼ˆä½¿ç”¨æ­£å‘ä¼ æ’­è¿›è¡Œè®¡ç®—ï¼‰ã€‚
- æ¢¯åº¦æ£€éªŒå¾ˆæ…¢ï¼Œå› æ­¤æˆ‘ä»¬ä¸ä¼šåœ¨æ¯æ¬¡è®­ç»ƒä¸­éƒ½è¿è¡Œå®ƒã€‚é€šå¸¸ï¼Œä½ ä»…éœ€ç¡®ä¿å…¶ä»£ç æ­£ç¡®å³å¯è¿è¡Œå®ƒï¼Œç„¶åå°†å…¶å…³é—­å¹¶å°†backpropç”¨äºå®é™…çš„å­¦ä¹ è¿‡ç¨‹ã€‚
