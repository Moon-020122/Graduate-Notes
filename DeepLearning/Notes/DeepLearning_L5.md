# 序列模型

## 1.1-符号

​	在序列模型中，**t** 代表的是样本的某个元素的位置，而 **T_x** 代表的是样本总元素的长度。
$$
X^{(i)<t>} 
$$

$$
y^{(i)<t>}
$$

​	代表的是第 (i) 个样本的第 (t) 个元素。

​	对于自然语言处理（NLP），我们通常需要先创建一个词汇表（字典）。每个词汇可以用一个 one-hot 向量表示，这意味着在这个向量中，只有一个位置为 1，其余位置为 0。这样，每个词都可以表示为一个与字典维数相对应的向量。

![image-20240722194948104](images/image-20240722194948104.png)

## 1.2-循环神经网络模型

在普通神经网络中训练看某一个单词是否为人名时，确实会遇到两个主要问题：

1. **输入和输出长度不一致**：不同的句子长度会导致输入和输出的长度不同。虽然可以通过填充0来使它们长度一致，但这并不是一个理想的解决方案，因为填充的0会引入噪声，影响模型的训练效果。
2. **特征共享问题**：普通神经网络不会共享从不同文本位置学到的特征。例如，如果一个词在位置1被标记为人名的一部分，那么在其他位置出现时，模型可能无法识别它仍然是人名的一部分。这是因为普通神经网络没有记忆机制，无法利用上下文信息来判断词汇的意义。

​	为了解决这些问题，循环神经网络（RNN）被引入。RNN 可以处理变长的输入和输出序列，并且通过其内部的循环结构，可以共享和记住从不同位置学到的特征。例如，RNN 可以记住一个词在不同位置出现的频率，从而更准确地判断它是否为人名。

![image-20240722195409341](images/image-20240722195409341.png)

​	在循环神经网络（RNN）中，每一步的预测不仅依赖于当前输入，还依赖于前一步的计算结果。这种机制使得 RNN 能够捕捉序列中的时间依赖关系。

1. **激活值传递**：在第一个单词的预测之后，RNN 会将第一个单词的激活值 ($a^{<1>}$) 传递给第二个单词的预测过程。因此，第二个单词的预测不仅依赖于当前输入 ($x^{<2>}$)，还依赖于前一步的激活值 ($a^{<1>}$)。这种传递机制使得 RNN 能够记住之前的上下文信息。
2. **初始激活值 ($a^{<0>}$)**：在序列的开始，我们通常会初始化一个激活值 ($a^{<0>}$)，这个值通常是一个全零的向量。
3. **共享权重**：RNN 的权重矩阵在每一步都是共享的，这意味着它可以在不同的时间步中 学习到相同的特征。这解决了普通神经网络中无法共享特征的问题。

**参数共享**：在每一步中，RNN 使用相同的参数集来处理输入和计算激活值。具体来说：

- **$W_{ax}$**：控制从输入 (X) 到隐藏层的连接。这组参数在每一个时间步中都会被使用。
- **$W_{aa}$**：控制隐藏层之间的水平连接。这组参数也在每一个时间步中都会被使用。
- **$W_{ya}$**：控制从隐藏层到输出的连接。

**RNN 的局限性**：RNN 只能利用序列中早期的信息来做预测。例如，在预测 $(y^{<3>})$ 时，RNN 只使用了$ (x^{<1>})、(x^{<2>}) 和 (x^{<3>})$ 的信息，而没有利用到 ($x^{<4>}) 和 (x^{<5>}$) 等后续的信息。这在处理长序列时会导致信息丢失，尤其是当后续的信息对于当前预测非常重要时。

![image-20240722201940040](images/image-20240722201940040.png)

### 前向传播

​	在每一步，RNN 的计算可以表示为： [$ a^{<t>} = g(W_{aa} \cdot a^{<t-1>} + W_{ax} \cdot x^{<t>} + b_a) $] [ $\hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) $] 其中，$(W_{aa})、(W_{ax}) 和 (W_{ya})$ 是权重矩阵，$(b_a) 和 (b_y) $是偏置项。$W$的下标，左边代表计算的字母右边代码要与之相乘的字母。

![image-20240722202322764](images/image-20240722202322764.png)

1. **合并权重矩阵**：我们可以将不同的权重矩阵$ (W_{ax}) 和 (W_{aa})$ 合并成一个大的权重矩阵 $(W)$。这样做的好处是可以将所有的权重操作集中在一个矩阵乘法中，从而简化计算过程。例如：$ [ W = \begin{bmatrix} W_{ax} & W_{aa} \end{bmatrix} ]$ 这样，输入 $(x^{<t>})$ 和前一个时间步的激活值 $(a^{<t-1>}) $可以上下合并成一个大的输入向量：$ [ \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} ]$。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ab50f64f70978beb304e633e7d2edf76.jpeg)

​	2.**简化计算公式**：通过合并后的矩阵，我们可以简化前向传播的计算公式。
$$
 [ a^{<t>} = g(W \cdot \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} + b_a) ]
$$
​		同理，$\hat y$也可以简化。

### 通过时间反向传播(BPTT)

1. **前向传播**：
   - 输入序列 $( {x^{<1>}, x^{<2>}, \ldots, x^{<T>}} ) $依次通过 RNN 的每个时间步，计算出每个时间步的激活值 $( a^{<t>} ) $和输出值 $( \hat{y}^{<t>} )$。
   - 公式表示为：$ [ a^{<t>} = g(W_{ax} \cdot x^{<t>} + W_{aa} \cdot a^{<t-1>} + b_a) ] [ \hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) ]$
2. **计算损失**：
   - 对于整个序列，计算损失函数 ( L )，通常使用交叉熵损失：$ [ L = -\sum_{t=1}^{T} \left( y^{<t>} \log(\hat{y}^{<t>}) + (1 - y^{<t>}) \log(1 - \hat{y}^{<t>}) \right) ]$
3. **反向传播**：
   - 在反向传播过程中，BPTT 会将损失函数对每个参数的梯度计算展开到每个时间步。
   - 具体来说，计算每个时间步的误差项$ ( \delta^{<t>} )$，并将其传播回前面的时间步： $[ \delta^{<t>} = \frac{\partial L}{\partial \hat{y}^{<t>}} \cdot \frac{\partial \hat{y}^{<t>}}{\partial a^{<t>}} \cdot \frac{\partial a^{<t>}}{\partial z^{<t>}} ]$
   - 其中，( $z^{<t>} $) 是激活前的线性组合，( $\delta^{<t>}$) 是误差项。
4. **更新权重**：
   - 使用梯度下降法更新权重矩阵$ ( W_{ax} )、( W_{aa} ) 和 ( W_{ya} )： [ W_{ax} \leftarrow W_{ax} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot x^{<t>} ] [ W_{aa} \leftarrow W_{aa} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t-1>} ] [ W_{ya} \leftarrow W_{ya} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t>} ]$
   - 其中，$( \alpha )$ 是学习率。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/486af4bf5f798ed4cd20e6d690c67cba.jpeg)

### 不同类型的RNNS

1. **一对一（One to One）**：
   - **结构**：单个输入对应单个输出。
   - **应用**：传统的神经网络，如图像分类。
2. **一对多（One to Many）**：
   - **结构**：单个输入对应多个输出。
   - **应用**：图像描述生成。给定一张图片，生成一段描述文字。
3. **多对一（Many to One）**：
   - **结构**：多个输入对应单个输出。
   - **应用**：情感分析。给定一段文字，判断其情感（如正面或负面）。
4. **多对多（Many to Many，同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度相同。
   - **应用**：视频分类。给定一段视频，生成对应的标签序列。
5. **多对多（Many to Many，不同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度可以不同。
   - **应用**：机器翻译。给定一句话，翻译成另一种语言。

![image-20240722203808672](images/image-20240722203808672.png)

## 1.3-语言模型和序列生成

语言模型的主要功能是估计一个给定句子的概率。

1. **语言模型的定义**：
   - 语言模型（Language Model, LM）是一个概率分布模型，它给定一个单词序列$ ( y^{(1)}, y^{(2)}, \ldots, y^{(T)} ) $的概率 $( P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) )。$
   - 这个概率表示了该句子在语言中的合理性或自然性。
2. **计算句子概率**：
   - 语言模型通过分解联合概率来计算句子的概率：$[ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$
   - 这种分解方式利用了条件概率，使得计算更为可行。

- 假设我们有一个句子 “The apple and pear salad”。语言模型会计算这个句子的概率 $( P(\text{The apple and pear salad}) )$，并与其他可能的句子进行比较，如 “The apple and pair salad”。通过比较这些概率，模型可以选择最自然的句子。

![image-20240722205706456](images/image-20240722205706456.png)

### 构建语言模型

1. **收集语料库**：
   - 首先，需要一个包含大量文本数据的语料库（corpus）。这些文本数据可以是新闻文章、书籍、对话记录等。
2. **标记化（Tokenization）**：
   - 将语料库中的文本数据分割成单词或标记（tokens）。例如，句子 “Cats average 15 hours of sleep a day.” 会被分割成 “Cats”, “average”, “15”, “hours”, “of”, “sleep”, “a”, “day”, 和一个表示句子结束的 <EOS> 标记。
3. **构建词汇表**：
   - 根据标记化后的单词，构建一个词汇表（vocabulary）。词汇表包含所有在语料库中出现的单词及其对应的索引。
4. **一位有效矢量（One-Hot Encoding）**：
   - 将每个单词映射到一个一位有效矢量（one-hot vector）。在这个向量中，只有一个位置为1，其余位置为0。例如，如果词汇表中有10,000个单词，那么每个单词都会被表示为一个长度为10,000的向量，其中只有一个位置为1。
5. **处理未知单词**：
   - 对于不在词汇表中的单词，用一个全局唯一的标记 $<UNK>$（unknown）代替，并对 $<UNK> $的概率进行建模。这可以帮助模型处理在训练数据中未见过的单词。
6. **训练语言模型**：
   - 使用循环神经网络（RNN）或其他序列模型来训练语言模型。模型的输入是一个单词序列，输出是下一个单词的概率分布。
   - 例如，对于句子 “Cats average 15 hours of sleep a day.”，模型会学习到在 “Cats” 之后最有可能出现的单词是 “average”，在 “average” 之后最有可能出现的单词是 “15”，以此类推。
7. **计算句子概率**：
   - 语言模型通过计算每个单词在给定前面单词的条件下出现的概率，来估计整个句子的概率。
   - 公式表示为：$ [ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$

![image-20240722210046148](images/image-20240722210046148.png)

### RNN训练过程

1. **初始化**：
   - 输入 ( $x_1$ ) 和初始激活值 ($ a_0$ ) 被初始化为零向量。这是因为在序列的开始，我们没有之前的上下文信息。
2. **前向传播**：
   - 在第一个时间步，RNN 使用 softmax 函数预测第一个单词的概率分布$(\hat{y}^{<1>})$。假设词汇表中有 10,000 个单词，那么$( \hat{y}^{<1>} ) $是一个 10,000 维的向量，每个元素表示对应单词的概率。
   - 例如，句子 “Cats average 15 hours of sleep a day.” 中，第一个单词 “Cats” 的概率由$( \hat{y}^{<1>} ) $表示。
3. **输入正确的单词**：
   - 在第二个时间步，RNN 的输入是第一个单词的正确输出$({y}^{<1>} ) $，即 “Cats”。这解释了为什么 ($({y}^{<1>} ) $ = $({x}^{<2>} ) $ )。通过这种方式，RNN 可以利用前一个时间步的正确单词来预测下一个单词。
4. **继续前向传播**：
   - 这个过程继续进行，例如在第三个时间步，RNN 需要预测第三个单词 “15” 的概率分布$(\hat{y}^{<3>})$。为了做到这一点，RNN 需要前两个单词 “Cats” 和 “average” 作为输入。
   - 公式表示为： [ P($({y}^{<3>})$ | $({y}^{<1>})$$({y}^{<2>})$) = $\text{softmax}(W_{ya} \cdot a^{<3>} + b_y) ]$
   - 其中，( $a_3$ ) 是第三个时间步的激活值，( $W_{ya} $) 是权重矩阵，( $b_y$ ) 是偏置项。

![image-20240722211006048](images/image-20240722211006048.png)

### 采样新序列

​	采样新序列是理解序列模型（如RNN）如何生成新数据的重要过程

- 在采样过程，我们使用训练好的模型生成新的序列。这个过程如下：
  1. **初始化**：首先，我们需要一个起始输入（可以是一个特定的词或一个特殊的起始符号）。
  2. **预测下一个词**：模型根据当前输入和隐藏状态，使用softmax函数计算每个可能的下一个词的概率分布。
  3. **随机采样**：根据softmax输出的概率分布，我们使用`np.random.choice`函数随机选择下一个词。这意味着概率较高的词更有可能被选中，但概率较低的词也有一定的机会被选中。
  4. **更新输入**：将选中的词作为下一个时间步的输入，重复步骤2和3，直到生成完整的序列或达到预定的长度。

​	通过这种方式，模型可以生成与训练数据相似的新序列，但也可能会有一些创新和变化。

![image-20240723200911116](images/image-20240723200911116.png)

### 字符级RNN模型

**优点（Pros）：**

1. **处理未知词汇**：基于字符的模型不会遇到未知词汇的问题。即使是训练集中没有出现过的词汇，模型也可以通过字符组合生成。例如，像“Mau”这样的词汇，即使不在训练集中，模型也能处理。
2. **更细粒度的控制**：字符级模型可以捕捉到更细微的语言特征，如拼写错误、缩写和新词的生成。

**缺点（Cons）：**

1. **序列长度增加**：字符级模型的输入序列通常比词汇级模型长得多。一个句子可能包含数十个字符，而不是几个单词。这使得模型需要处理更长的依赖关系，增加了计算复杂度。
2. **训练成本高**：由于序列长度增加，训练字符级模型需要更多的计算资源和时间。模型需要更多的时间来学习字符之间的关系，而不是单词之间的关系。
3. **长距离依赖关系**：字符级模型在捕捉长距离依赖关系时可能不如词汇级模型有效。句子前后部分的依赖关系在字符级模型中可能更难捕捉。

![image-20240723201807806](images/image-20240723201807806.png)

## 1.4-RNNs梯度消失

### 梯度消失问题

在训练RNN时，我们使用反向传播算法来更新模型的权重。然而，当我们计算梯度并将其反向传播通过网络时，梯度值可能会变得非常小，尤其是在深层网络中。这种现象称为梯度消失。具体来说：

1. **梯度计算**：在每个时间步，梯度是通过链式法则计算的，这意味着梯度是多个小值的乘积。
2. **梯度减小**：由于这些小值的乘积，梯度会随着时间步的增加而指数级减小。这导致早期时间步的梯度几乎为零，无法有效更新权重。
3. **长期依赖关系**：由于梯度消失，RNN很难捕捉到远距离的依赖关系。例如，句子的前半部分的信息很难影响到后半部分的输出。

### 解决方法

为了缓解梯度消失问题，研究人员提出了几种改进的RNN结构：

1. **长短期记忆网络（LSTM）**：LSTM通过引入门控机制（如输入门、遗忘门和输出门）来控制信息的流动，从而有效地缓解了梯度消失问题。LSTM可以更好地捕捉长期依赖关系。
2. **门控循环单元（GRU）**：GRU是LSTM的简化版本，具有类似的门控机制，但结构更简单，计算效率更高。
3. **深度RNN**：通过堆叠多个RNN层，可以增强模型的表达能力，但也需要更复杂的训练技巧来处理梯度消失问题。

![image-20240723202540424](images/image-20240723202540424.png)

#### 门控循环单元（GRU）

​	门控循环单元修改了神经网络的隐藏层从而更好地捕捉长距离的关系同时有助于减轻梯度消失的问题。GRU通过引入两个门控机制：**相关门**和**更新门**，来控制信息的流动。这些门控机制帮助GRU在训练过程中更好地捕捉和保留重要的信息，同时减轻梯度消失的问题。

​	$∗$表示元素级乘法。

**相关门（relation Gate）**：

- 相关门控制前一时间步的隐藏状态对当前时间步的影响。它决定了多少过去的信息需要被遗忘。

$$
Γ_r = \sigma(W_r \cdot [C^{<t-1>}, x^{<t>}]+b_r)
$$

​	$C_{t-1}$是前一步的隐藏状态。

**更新门（Update Gate）**

- 更新门决定了当前时间步的隐藏状态有多少需要被更新。它控制了新信息和旧信息的混合比例。

$$
Γ_u = \sigma(W_u \cdot [C^{<t-1>}, x^{<t>}]+b_u)
$$

**候选隐藏状态（Candidate Hidden State）**：

- 候选隐藏状态是当前时间步的新信息，它结合了当前输入和前一时间步的隐藏状态（经过相关门的调节）。

$$
\tilde{C}^{<t>} = \tanh(W_C \cdot [Γ_r \ast C^{<t-1>}, x^{<t>}]+b_c)
$$

**隐藏状态更新（Hidden State Update）**：

- 最终的隐藏状态是前一时间步的隐藏状态和候选隐藏状态的加权和，由更新门控制。

$$
C^{<t>} = (1 - Γ_u) \ast C^{<t-1>} + Γ_u \ast \tilde{C}^{<t>}
$$

- **捕捉长距离依赖关系**：通过门控机制，GRU能够更好地捕捉和保留长距离依赖关系。
- **减轻梯度消失问题**：门控机制帮助GRU在反向传播过程中保持梯度的稳定，减轻梯度消失的问题。
- **计算效率高**：相比LSTM，GRU结构更简单，计算效率更高。

![image-20240723204318713](images/image-20240723204318713.png)

1. **更新门（Update Gate）**：更新门的值介于0到1之间，用于控制当前状态和新信息的混合程度。更新门的值通过sigmoid函数计算，sigmoid函数的输出范围是0到1，因此更新门的值也在这个范围内。
2. **候选状态（Candidate State）**：候选状态是GRU在当前时间步计算的潜在新状态。这个状态会根据更新门的值决定是否更新到最终状态。
3. **门控机制**：更新门的作用是决定是否更新记忆单元C的值。如果更新门的值接近1，表示需要更新记忆单元；如果接近0，表示保持当前记忆单元的值不变。
4. **记忆单元C的更新**：记忆单元C的值会根据更新门的值进行更新。具体来说，如果更新门的值为1，记忆单元C会被更新为候选状态；如果更新门的值为0，记忆单元C保持不变。
5. **语法示例**：你提到的语法示例是为了说明GRU如何在自然语言处理中记住主语的数（单数或复数）并在适当的时候更新记忆单元C。例如，当主语是单数时，记忆单元C的值可能为1；当主语是复数时，记忆单元C的值可能为0。GRU会记住这个值直到需要更新谓语动词的数时。

![img](https://i-blog.csdnimg.cn/blog_migrate/266cc3c55c7d848108d2b0ffe611794e.jpeg)

#### 长短期记忆(LSTM)

​	LSTM（长短期记忆网络）和GRU（门控循环单元）都是RNN（循环神经网络）的变体，用于处理序列数据，但LSTM比GRU更复杂，具有更强的泛化能力。

GRU有两个主要的门控机制：

1. **更新门（Update Gate, $Γ_u$）**：控制当前状态和新信息的混合程度。
2. **相关门（relation Gate, $Γ_r$）**：决定如何结合新输入和之前的记忆。

LSTM引入了更多的门控机制，使其能够更好地捕捉长时间依赖关系。LSTM有三个主要的门控机制：

1. **遗忘门（Forget Gate, $Γ_f$）**：决定是否丢弃之前的记忆。
2. **更新门（update Gate, $Γ_u$）**：决定是否更新当前的记忆。
3. **输出门（Output Gate, $Γ_o$）**：决定当前状态的输出。

LSTM的状态更新公式更复杂，包含两个状态：细胞状态$C_t$和隐藏状态$a_t$。细胞状态通过遗忘门和更新门进行更新，而隐藏状态通过输出门进行更新。

![image-20240723210031293](images/image-20240723210031293.png)

​	窥孔连接引入了前一时刻的细胞状态$C^{<t−1>}$，使得门控机制能够直接访问细胞状态的信息。	

​	例如，遗忘门的计算公式可以表示为：
$$
Γ_f = \sigma(W_f \cdot [C^{<t-1>}, x^{<t>}]+U_f\cdot C^{<t-1>}+b_f)
$$
​	其中，$U_f$是与细胞状态相关的权重矩阵。

​	如果细胞状态是一个100维的向量那么$ C^{<t-1>}$的第5个元素只会影响门控向量的第5个元素。

​	这里使用$a^{<t-1>)}$和$x^{<t>}$一起来计算遗忘门、更新门和输出门的值。注意一下下方的三张图，把它们按照时间次序连接起来，这里输入$x^{<1>}、x^{<2>}、x^{<3>}$，有个很有意思的事情（红色直线），这条线显示了只要你正确地设置了遗忘门和更新门，LSTM是相当容易把$c^{<t>}$的值一直往下传递到右边。

![img](https://i-blog.csdnimg.cn/blog_migrate/722e0f3509ccb7a8e881b75874f2105c.jpeg)