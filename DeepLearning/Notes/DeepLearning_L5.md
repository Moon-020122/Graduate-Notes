# 序列模型

## 1.1-符号

​	在序列模型中，**t** 代表的是样本的某个元素的位置，而 **T_x** 代表的是样本总元素的长度。
$$
X^{(i)<t>} 
$$

$$
y^{(i)<t>}
$$

​	代表的是第 (i) 个样本的第 (t) 个元素。

​	对于自然语言处理（NLP），我们通常需要先创建一个词汇表（字典）。每个词汇可以用一个 one-hot 向量表示，这意味着在这个向量中，只有一个位置为 1，其余位置为 0。这样，每个词都可以表示为一个与字典维数相对应的向量。

![image-20240722194948104](images/image-20240722194948104.png)

## 1.2-循环神经网络模型

在普通神经网络中训练看某一个单词是否为人名时，确实会遇到两个主要问题：

1. **输入和输出长度不一致**：不同的句子长度会导致输入和输出的长度不同。虽然可以通过填充0来使它们长度一致，但这并不是一个理想的解决方案，因为填充的0会引入噪声，影响模型的训练效果。
2. **特征共享问题**：普通神经网络不会共享从不同文本位置学到的特征。例如，如果一个词在位置1被标记为人名的一部分，那么在其他位置出现时，模型可能无法识别它仍然是人名的一部分。这是因为普通神经网络没有记忆机制，无法利用上下文信息来判断词汇的意义。

​	为了解决这些问题，循环神经网络（RNN）被引入。RNN 可以处理变长的输入和输出序列，并且通过其内部的循环结构，可以共享和记住从不同位置学到的特征。例如，RNN 可以记住一个词在不同位置出现的频率，从而更准确地判断它是否为人名。

![image-20240722195409341](images/image-20240722195409341.png)

​	在循环神经网络（RNN）中，每一步的预测不仅依赖于当前输入，还依赖于前一步的计算结果。这种机制使得 RNN 能够捕捉序列中的时间依赖关系。

1. **激活值传递**：在第一个单词的预测之后，RNN 会将第一个单词的激活值 ($a^{<1>}$) 传递给第二个单词的预测过程。因此，第二个单词的预测不仅依赖于当前输入 ($x^{<2>}$)，还依赖于前一步的激活值 ($a^{<1>}$)。这种传递机制使得 RNN 能够记住之前的上下文信息。
2. **初始激活值 ($a^{<0>}$)**：在序列的开始，我们通常会初始化一个激活值 ($a^{<0>}$)，这个值通常是一个全零的向量。
3. **共享权重**：RNN 的权重矩阵在每一步都是共享的，这意味着它可以在不同的时间步中 学习到相同的特征。这解决了普通神经网络中无法共享特征的问题。

**参数共享**：在每一步中，RNN 使用相同的参数集来处理输入和计算激活值。具体来说：

- **$W_{ax}$**：控制从输入 (X) 到隐藏层的连接。这组参数在每一个时间步中都会被使用。
- **$W_{aa}$**：控制隐藏层之间的水平连接。这组参数也在每一个时间步中都会被使用。
- **$W_{ya}$**：控制从隐藏层到输出的连接。

**RNN 的局限性**：RNN 只能利用序列中早期的信息来做预测。例如，在预测 $(y^{<3>})$ 时，RNN 只使用了$ (x^{<1>})、(x^{<2>}) 和 (x^{<3>})$ 的信息，而没有利用到 ($x^{<4>}) 和 (x^{<5>}$) 等后续的信息。这在处理长序列时会导致信息丢失，尤其是当后续的信息对于当前预测非常重要时。

![image-20240722201940040](images/image-20240722201940040.png)

### 前向传播

​	在每一步，RNN 的计算可以表示为： [$ a^{<t>} = g(W_{aa} \cdot a^{<t-1>} + W_{ax} \cdot x^{<t>} + b_a) $] [ $\hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) $] 其中，$(W_{aa})、(W_{ax}) 和 (W_{ya})$ 是权重矩阵，$(b_a) 和 (b_y) $是偏置项。$W$的下标，左边代表计算的字母右边代码要与之相乘的字母。

![image-20240722202322764](images/image-20240722202322764.png)

1. **合并权重矩阵**：我们可以将不同的权重矩阵$ (W_{ax}) 和 (W_{aa})$ 合并成一个大的权重矩阵 $(W)$。这样做的好处是可以将所有的权重操作集中在一个矩阵乘法中，从而简化计算过程。例如：$ [ W = \begin{bmatrix} W_{ax} & W_{aa} \end{bmatrix} ]$ 这样，输入 $(x^{<t>})$ 和前一个时间步的激活值 $(a^{<t-1>}) $可以上下合并成一个大的输入向量：$ [ \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} ]$。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ab50f64f70978beb304e633e7d2edf76.jpeg)

​	2.**简化计算公式**：通过合并后的矩阵，我们可以简化前向传播的计算公式。
$$
 [ a^{<t>} = g(W \cdot \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} + b_a) ]
$$
​		同理，$\hat y$也可以简化。

### 通过时间反向传播(BPTT)

1. **前向传播**：
   - 输入序列 $( {x^{<1>}, x^{<2>}, \ldots, x^{<T>}} ) $依次通过 RNN 的每个时间步，计算出每个时间步的激活值 $( a^{<t>} ) $和输出值 $( \hat{y}^{<t>} )$。
   - 公式表示为：$ [ a^{<t>} = g(W_{ax} \cdot x^{<t>} + W_{aa} \cdot a^{<t-1>} + b_a) ] [ \hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) ]$
2. **计算损失**：
   - 对于整个序列，计算损失函数 ( L )，通常使用交叉熵损失：$ [ L = -\sum_{t=1}^{T} \left( y^{<t>} \log(\hat{y}^{<t>}) + (1 - y^{<t>}) \log(1 - \hat{y}^{<t>}) \right) ]$
3. **反向传播**：
   - 在反向传播过程中，BPTT 会将损失函数对每个参数的梯度计算展开到每个时间步。
   - 具体来说，计算每个时间步的误差项$ ( \delta^{<t>} )$，并将其传播回前面的时间步： $[ \delta^{<t>} = \frac{\partial L}{\partial \hat{y}^{<t>}} \cdot \frac{\partial \hat{y}^{<t>}}{\partial a^{<t>}} \cdot \frac{\partial a^{<t>}}{\partial z^{<t>}} ]$
   - 其中，( $z^{<t>} $) 是激活前的线性组合，( $\delta^{<t>}$) 是误差项。
4. **更新权重**：
   - 使用梯度下降法更新权重矩阵$ ( W_{ax} )、( W_{aa} ) 和 ( W_{ya} )： [ W_{ax} \leftarrow W_{ax} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot x^{<t>} ] [ W_{aa} \leftarrow W_{aa} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t-1>} ] [ W_{ya} \leftarrow W_{ya} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t>} ]$
   - 其中，$( \alpha )$ 是学习率。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/486af4bf5f798ed4cd20e6d690c67cba.jpeg)

### 不同类型的RNNS

1. **一对一（One to One）**：
   - **结构**：单个输入对应单个输出。
   - **应用**：传统的神经网络，如图像分类。
2. **一对多（One to Many）**：
   - **结构**：单个输入对应多个输出。
   - **应用**：图像描述生成。给定一张图片，生成一段描述文字。
3. **多对一（Many to One）**：
   - **结构**：多个输入对应单个输出。
   - **应用**：情感分析。给定一段文字，判断其情感（如正面或负面）。
4. **多对多（Many to Many，同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度相同。
   - **应用**：视频分类。给定一段视频，生成对应的标签序列。
5. **多对多（Many to Many，不同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度可以不同。
   - **应用**：机器翻译。给定一句话，翻译成另一种语言。

![image-20240722203808672](images/image-20240722203808672.png)

## 1.3-语言模型和序列生成

语言模型的主要功能是估计一个给定句子的概率。

1. **语言模型的定义**：
   - 语言模型（Language Model, LM）是一个概率分布模型，它给定一个单词序列$ ( y^{(1)}, y^{(2)}, \ldots, y^{(T)} ) $的概率 $( P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) )。$
   - 这个概率表示了该句子在语言中的合理性或自然性。
2. **计算句子概率**：
   - 语言模型通过分解联合概率来计算句子的概率：$[ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$
   - 这种分解方式利用了条件概率，使得计算更为可行。

- 假设我们有一个句子 “The apple and pear salad”。语言模型会计算这个句子的概率 $( P(\text{The apple and pear salad}) )$，并与其他可能的句子进行比较，如 “The apple and pair salad”。通过比较这些概率，模型可以选择最自然的句子。

![image-20240722205706456](images/image-20240722205706456.png)

### 构建语言模型

1. **收集语料库**：
   - 首先，需要一个包含大量文本数据的语料库（corpus）。这些文本数据可以是新闻文章、书籍、对话记录等。
2. **标记化（Tokenization）**：
   - 将语料库中的文本数据分割成单词或标记（tokens）。例如，句子 “Cats average 15 hours of sleep a day.” 会被分割成 “Cats”, “average”, “15”, “hours”, “of”, “sleep”, “a”, “day”, 和一个表示句子结束的 <EOS> 标记。
3. **构建词汇表**：
   - 根据标记化后的单词，构建一个词汇表（vocabulary）。词汇表包含所有在语料库中出现的单词及其对应的索引。
4. **一位有效矢量（One-Hot Encoding）**：
   - 将每个单词映射到一个一位有效矢量（one-hot vector）。在这个向量中，只有一个位置为1，其余位置为0。例如，如果词汇表中有10,000个单词，那么每个单词都会被表示为一个长度为10,000的向量，其中只有一个位置为1。
5. **处理未知单词**：
   - 对于不在词汇表中的单词，用一个全局唯一的标记 $<UNK>$（unknown）代替，并对 $<UNK> $的概率进行建模。这可以帮助模型处理在训练数据中未见过的单词。
6. **训练语言模型**：
   - 使用循环神经网络（RNN）或其他序列模型来训练语言模型。模型的输入是一个单词序列，输出是下一个单词的概率分布。
   - 例如，对于句子 “Cats average 15 hours of sleep a day.”，模型会学习到在 “Cats” 之后最有可能出现的单词是 “average”，在 “average” 之后最有可能出现的单词是 “15”，以此类推。
7. **计算句子概率**：
   - 语言模型通过计算每个单词在给定前面单词的条件下出现的概率，来估计整个句子的概率。
   - 公式表示为：$ [ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$

![image-20240722210046148](images/image-20240722210046148.png)

### RNN训练过程

1. **初始化**：
   - 输入 ( $x_1$ ) 和初始激活值 ($ a_0$ ) 被初始化为零向量。这是因为在序列的开始，我们没有之前的上下文信息。
2. **前向传播**：
   - 在第一个时间步，RNN 使用 softmax 函数预测第一个单词的概率分布$(\hat{y}^{<1>})$。假设词汇表中有 10,000 个单词，那么$( \hat{y}^{<1>} ) $是一个 10,000 维的向量，每个元素表示对应单词的概率。
   - 例如，句子 “Cats average 15 hours of sleep a day.” 中，第一个单词 “Cats” 的概率由$( \hat{y}^{<1>} ) $表示。
3. **输入正确的单词**：
   - 在第二个时间步，RNN 的输入是第一个单词的正确输出$({y}^{<1>} ) $，即 “Cats”。这解释了为什么 ($({y}^{<1>} ) $ = $({x}^{<2>} ) $ )。通过这种方式，RNN 可以利用前一个时间步的正确单词来预测下一个单词。
4. **继续前向传播**：
   - 这个过程继续进行，例如在第三个时间步，RNN 需要预测第三个单词 “15” 的概率分布$(\hat{y}^{<3>})$。为了做到这一点，RNN 需要前两个单词 “Cats” 和 “average” 作为输入。
   - 公式表示为： [ P($({y}^{<3>})$ | $({y}^{<1>})$$({y}^{<2>})$) = $\text{softmax}(W_{ya} \cdot a^{<3>} + b_y) ]$
   - 其中，( $a_3$ ) 是第三个时间步的激活值，( $W_{ya} $) 是权重矩阵，( $b_y$ ) 是偏置项。

![image-20240722211006048](images/image-20240722211006048.png)