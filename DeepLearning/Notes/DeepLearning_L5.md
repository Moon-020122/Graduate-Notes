# 序列模型

## 1.1-符号

​	在序列模型中，**t** 代表的是样本的某个元素的位置，而 **T_x** 代表的是样本总元素的长度。
$$
X^{(i)<t>} 
$$

$$
y^{(i)<t>}
$$

​	代表的是第 (i) 个样本的第 (t) 个元素。

​	对于自然语言处理（NLP），我们通常需要先创建一个词汇表（字典）。每个词汇可以用一个 one-hot 向量表示，这意味着在这个向量中，只有一个位置为 1，其余位置为 0。每个词都可以表示为一个与字典维数相对应的向量。

![image-20240722194948104](images/image-20240722194948104.png)

## 1.2-循环神经网络模型

在普通神经网络中训练看某一个单词是否为人名时，会遇到两个主要问题：

1. **输入和输出长度不一致**：不同的句子长度会导致输入和输出的长度不同。虽然可以通过填充0来使它们长度一致，但这并不是一个理想的解决方案，因为填充的0会引入噪声，影响模型的训练效果。
2. **特征共享问题**：普通神经网络不会共享从不同文本位置学到的特征。例如，如果一个词在位置1被标记为人名的一部分，那么在其他位置出现时，模型可能无法识别它仍然是人名的一部分。这是因为普通神经网络没有记忆机制，无法利用上下文信息来判断词汇的意义。

​	为了解决这些问题，循环神经网络（RNN）被引入。RNN 可以处理变长的输入和输出序列，并且通过其内部的循环结构，可以共享和记住从不同位置学到的特征。例如，RNN 可以记住一个词在不同位置出现的频率，从而更准确地判断它是否为人名。

![image-20240722195409341](images/image-20240722195409341.png)

​	在循环神经网络（RNN）中，每一步的预测不仅依赖于当前输入，还依赖于前一步的计算结果。这种机制使得 RNN 能够捕捉序列中的时间依赖关系。

1. **激活值传递**：在第一个单词的预测之后，RNN 会将第一个单词的激活值 ($a^{<1>}$) 传递给第二个单词的预测过程。因此，第二个单词的预测不仅依赖于当前输入 ($x^{<2>}$)，还依赖于前一步的激活值 ($a^{<1>}$)。这种传递机制使得 RNN 能够记住之前的上下文信息。
2. **初始激活值 ($a^{<0>}$)**：在序列的开始，我们通常会初始化一个激活值 ($a^{<0>}$)，这个值通常是一个全零的向量。
3. **共享权重**：RNN 的权重矩阵在每一步都是共享的，这意味着它可以在不同的时间步中 学习到相同的特征。这解决了普通神经网络中无法共享特征的问题。

**参数共享**：在每一步中，RNN 使用相同的参数集来处理输入和计算激活值。具体来说：

- **$W_{ax}$**：控制从输入 (X) 到隐藏层的连接。这组参数在每一个时间步中都会被使用。
- **$W_{aa}$**：控制隐藏层之间的水平连接。这组参数也在每一个时间步中都会被使用。
- **$W_{ya}$**：控制从隐藏层到输出的连接。

**RNN 的局限性**：RNN 只能利用序列中早期的信息来做预测。例如，在预测 $(y^{<3>})$ 时，RNN 只使用了$ (x^{<1>})、(x^{<2>}) 和 (x^{<3>})$ 的信息，而没有利用到 ($x^{<4>}) 和 (x^{<5>}$) 等后续的信息。这在处理长序列时会导致信息丢失，尤其是当后续的信息对于当前预测非常重要时。

![image-20240722201940040](images/image-20240722201940040.png)

### 前向传播

​	在每一步，RNN 的计算可以表示为： [$ a^{<t>} = g(W_{aa} \cdot a^{<t-1>} + W_{ax} \cdot x^{<t>} + b_a) $] [ $\hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) $] 其中，$(W_{aa})、(W_{ax}) 和 (W_{ya})$ 是权重矩阵，$(b_a) 和 (b_y) $是偏置项。$W$的下标，左边代表计算的字母右边代码要与之相乘的字母。

![image-20240722202322764](images/image-20240722202322764.png)

1. **合并权重矩阵**：我们可以将不同的权重矩阵$ (W_{ax}) 和 (W_{aa})$ 合并成一个大的权重矩阵 $(W)$。这样做的好处是可以将所有的权重操作集中在一个矩阵乘法中，从而简化计算过程。例如：$ [ W = \begin{bmatrix} W_{ax} & W_{aa} \end{bmatrix} ]$ 这样，输入 $(x^{<t>})$ 和前一个时间步的激活值 $(a^{<t-1>}) $可以上下合并成一个大的输入向量：$ [ \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} ]$。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/ab50f64f70978beb304e633e7d2edf76.jpeg)

​	2.**简化计算公式**：通过合并后的矩阵，我们可以简化前向传播的计算公式。
$$
 [ a^{<t>} = g(W \cdot \begin{bmatrix} x^{<t>} \ a^{<t-1>} \end{bmatrix} + b_a) ]
$$
​		同理，$\hat y$也可以简化。

### 通过时间反向传播(BPTT)

1. **前向传播**：
   - 输入序列 $( {x^{<1>}, x^{<2>}, \ldots, x^{<T>}} ) $依次通过 RNN 的每个时间步，计算出每个时间步的激活值 $( a^{<t>} ) $和输出值 $( \hat{y}^{<t>} )$。
   - 公式表示为：$ [ a^{<t>} = g(W_{ax} \cdot x^{<t>} + W_{aa} \cdot a^{<t-1>} + b_a) ] [ \hat{y}^{<t>} = g(W_{ya} \cdot a^{<t>} + b_y) ]$
2. **计算损失**：
   - 对于整个序列，计算损失函数 ( L )，通常使用交叉熵损失：$ [ L = -\sum_{t=1}^{T} \left( y^{<t>} \log(\hat{y}^{<t>}) + (1 - y^{<t>}) \log(1 - \hat{y}^{<t>}) \right) ]$
3. **反向传播**：
   - 在反向传播过程中，BPTT 会将损失函数对每个参数的梯度计算展开到每个时间步。
   - 具体来说，计算每个时间步的误差项$ ( \delta^{<t>} )$，并将其传播回前面的时间步： $[ \delta^{<t>} = \frac{\partial L}{\partial \hat{y}^{<t>}} \cdot \frac{\partial \hat{y}^{<t>}}{\partial a^{<t>}} \cdot \frac{\partial a^{<t>}}{\partial z^{<t>}} ]$
   - 其中，( $z^{<t>} $) 是激活前的线性组合，( $\delta^{<t>}$) 是误差项。
4. **更新权重**：
   - 使用梯度下降法更新权重矩阵$ ( W_{ax} )、( W_{aa} ) 和 ( W_{ya} )： [ W_{ax} \leftarrow W_{ax} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot x^{<t>} ] [ W_{aa} \leftarrow W_{aa} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t-1>} ] [ W_{ya} \leftarrow W_{ya} - \alpha \sum_{t=1}^{T} \delta^{<t>} \cdot a^{<t>} ]$
   - 其中，$( \alpha )$ 是学习率。

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/486af4bf5f798ed4cd20e6d690c67cba.jpeg)

### 不同类型的RNNS

1. **一对一（One to One）**：
   - **结构**：单个输入对应单个输出。
   - **应用**：传统的神经网络，如图像分类。
2. **一对多（One to Many）**：
   - **结构**：单个输入对应多个输出。
   - **应用**：图像描述生成。给定一张图片，生成一段描述文字。
3. **多对一（Many to One）**：
   - **结构**：多个输入对应单个输出。
   - **应用**：情感分析。给定一段文字，判断其情感（如正面或负面）。
4. **多对多（Many to Many，同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度相同。
   - **应用**：视频分类。给定一段视频，生成对应的标签序列。
5. **多对多（Many to Many，不同步长度）**：
   - **结构**：多个输入对应多个输出，输入和输出序列长度可以不同。
   - **应用**：机器翻译。给定一句话，翻译成另一种语言。

![image-20240722203808672](images/image-20240722203808672.png)

## 1.3-语言模型和序列生成

语言模型的主要功能是估计一个给定句子的概率。

1. **语言模型的定义**：
   - 语言模型（Language Model, LM）是一个概率分布模型，它给定一个单词序列$ ( y^{(1)}, y^{(2)}, \ldots, y^{(T)} ) $的概率 $( P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) )。$
   - 这个概率表示了该句子在语言中的合理性或自然性。
2. **计算句子概率**：
   - 语言模型通过分解联合概率来计算句子的概率：$[ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$
   - 这种分解方式利用了条件概率，使得计算更为可行。

- 假设我们有一个句子 “The apple and pear salad”。语言模型会计算这个句子的概率 $( P(\text{The apple and pear salad}) )$，并与其他可能的句子进行比较，如 “The apple and pair salad”。通过比较概率，模型可以选择最自然的句子。

![image-20240722205706456](images/image-20240722205706456.png)

### 构建语言模型

1. **收集语料库**：
   - 首先，需要一个包含大量文本数据的语料库（corpus）。这些文本数据可以是新闻文章、书籍、对话记录等。
2. **标记化（Tokenization）**：
   - 将语料库中的文本数据分割成单词或标记（tokens）。例如，句子 “Cats average 15 hours of sleep a day.” 会被分割成 “Cats”, “average”, “15”, “hours”, “of”, “sleep”, “a”, “day”, 和一个表示句子结束的 <EOS> 标记。
3. **构建词汇表**：
   - 根据标记化后的单词，构建一个词汇表（vocabulary）。词汇表包含所有在语料库中出现的单词及其对应的索引。
4. **一位有效矢量（One-Hot Encoding）**：
   - 将每个单词映射到一个一位有效矢量（one-hot vector）。在这个向量中，只有一个位置为1，其余位置为0。例如，如果词汇表中有10,000个单词，那么每个单词都会被表示为一个长度为10,000的向量，其中只有一个位置为1。
5. **处理未知单词**：
   - 对于不在词汇表中的单词，用一个全局唯一的标记 $<UNK>$（unknown）代替，并对 $<UNK> $的概率进行建模。这可以帮助模型处理在训练数据中未见过的单词。
6. **训练语言模型**：
   - 使用循环神经网络（RNN）或其他序列模型来训练语言模型。模型的输入是一个单词序列，输出是下一个单词的概率分布。
   - 例如，对于句子 “Cats average 15 hours of sleep a day.”，模型会学习到在 “Cats” 之后最有可能出现的单词是 “average”，在 “average” 之后最有可能出现的单词是 “15”，以此类推。
7. **计算句子概率**：
   - 语言模型通过计算每个单词在给定前面单词的条件下出现的概率，来估计整个句子的概率。
   - 公式表示为：$ [ P(y^{(1)}, y^{(2)}, \ldots, y^{(T)}) = P(y^{(1)}) \cdot P(y{(2)}|y{(1)}) \cdot P(y{(3)}|y{(1)}, y^{(2)}) \cdots P(y{(T)}|y{(1)}, y^{(2)}, \ldots, y^{(T-1)}) ]$

![image-20240722210046148](images/image-20240722210046148.png)

### RNN训练过程

1. **初始化**：
   - 输入 ( $x_1$ ) 和初始激活值 ($ a_0$ ) 被初始化为零向量。这是因为在序列的开始，我们没有之前的上下文信息。
2. **前向传播**：
   - 在第一个时间步，RNN 使用 softmax 函数预测第一个单词的概率分布$(\hat{y}^{<1>})$。假设词汇表中有 10,000 个单词，那么$( \hat{y}^{<1>} ) $是一个 10,000 维的向量，每个元素表示对应单词的概率。
   - 例如，句子 “Cats average 15 hours of sleep a day.” 中，第一个单词 “Cats” 的概率由$( \hat{y}^{<1>} ) $表示。
3. **输入正确的单词**：
   - 在第二个时间步，RNN 的输入是第一个单词的正确输出$({y}^{<1>} ) $，即 “Cats”。这解释了为什么 ($({y}^{<1>} ) $ = $({x}^{<2>} ) $ )。通过这种方式，RNN 可以利用前一个时间步的正确单词来预测下一个单词。
4. **继续前向传播**：
   - 这个过程继续进行，例如在第三个时间步，RNN 需要预测第三个单词 “15” 的概率分布$(\hat{y}^{<3>})$。为了做到这一点，RNN 需要前两个单词 “Cats” 和 “average” 作为输入。
   - 公式表示为： [ P($({y}^{<3>})$ | $({y}^{<1>})$$({y}^{<2>})$) = $\text{softmax}(W_{ya} \cdot a^{<3>} + b_y) ]$。
   - 其中，( $a_3$ ) 是第三个时间步的激活值，( $W_{ya} $) 是权重矩阵，( $b_y$ ) 是偏置项。

![image-20240722211006048](images/image-20240722211006048.png)

### 采样新序列

​	采样新序列是理解序列模型（如RNN）如何生成新数据的重要过程

- 在采样过程，我们使用训练好的模型生成新的序列。这个过程如下：
  1. **初始化**：首先，我们需要一个起始输入（可以是一个特定的词或一个特殊的起始符号）。
  2. **预测下一个词**：模型根据当前输入和隐藏状态，使用softmax函数计算每个可能的下一个词的概率分布。
  3. **随机采样**：根据softmax输出的概率分布，我们使用`np.random.choice`函数随机选择下一个词。这意味着概率较高的词更有可能被选中，但概率较低的词也有一定的机会被选中。
  4. **更新输入**：将选中的词作为下一个时间步的输入，重复步骤2和3，直到生成完整的序列或达到预定的长度。

​	通过这种方式，模型可以生成与训练数据相似的新序列，但也可能会有一些创新和变化。

![image-20240723200911116](images/image-20240723200911116.png)

### 字符级RNN模型

**优点（Pros）：**

1. **处理未知词汇**：基于字符的模型不会遇到未知词汇的问题。即使是训练集中没有出现过的词汇，模型也可以通过字符组合生成。例如，像“Mau”这样的词汇，即使不在训练集中，模型也能处理。
2. **更细粒度的控制**：字符级模型可以捕捉到更细微的语言特征，如拼写错误、缩写和新词的生成。

**缺点（Cons）：**

1. **序列长度增加**：字符级模型的输入序列通常比词汇级模型长得多。一个句子可能包含数十个字符，而不是几个单词。这使得模型需要处理更长的依赖关系，增加了计算复杂度。
2. **训练成本高**：由于序列长度增加，训练字符级模型需要更多的计算资源和时间。模型需要更多的时间来学习字符之间的关系，而不是单词之间的关系。
3. **长距离依赖关系**：字符级模型在捕捉长距离依赖关系时可能不如词汇级模型有效。句子前后部分的依赖关系在字符级模型中可能更难捕捉。

![image-20240723201807806](images/image-20240723201807806.png)

## 1.4-RNNs梯度消失

### 梯度消失问题

在训练RNN时，我们使用反向传播算法来更新模型的权重。然而，当我们计算梯度并将其反向传播通过网络时，梯度值可能会变得非常小，尤其是在深层网络中。这种现象称为梯度消失。具体来说：

1. **梯度计算**：在每个时间步，梯度是通过链式法则计算的，这意味着梯度是多个小值的乘积。
2. **梯度减小**：由于这些小值的乘积，梯度会随着时间步的增加而指数级减小。这导致早期时间步的梯度几乎为零，无法有效更新权重。
3. **长期依赖关系**：由于梯度消失，RNN很难捕捉到远距离的依赖关系。例如，句子的前半部分的信息很难影响到后半部分的输出。

### 解决方法

为了缓解梯度消失问题，研究人员提出了几种改进的RNN结构：

1. **长短期记忆网络（LSTM）**：LSTM通过引入门控机制（如输入门、遗忘门和输出门）来控制信息的流动，从而有效地缓解了梯度消失问题。LSTM可以更好地捕捉长期依赖关系。
2. **门控循环单元（GRU）**：GRU是LSTM的简化版本，具有类似的门控机制，但结构更简单，计算效率更高。
3. **深度RNN**：通过堆叠多个RNN层，可以增强模型的表达能力，但也需要更复杂的训练技巧来处理梯度消失问题。

![image-20240723202540424](images/image-20240723202540424.png)

#### 门控循环单元（GRU）

​	门控循环单元修改了神经网络的隐藏层从而更好地捕捉长距离的关系同时有助于减轻梯度消失的问题。GRU通过引入两个门控机制：**相关门**和**更新门**，来控制信息的流动。这些门控机制帮助GRU在训练过程中更好地捕捉和保留重要的信息，同时减轻梯度消失的问题。

​	$∗$表示元素级乘法。

**相关门（relation Gate）**：

- 相关门控制前一时间步的隐藏状态对当前时间步的影响。它决定了多少过去的信息需要被遗忘。

$$
Γ_r = \sigma(W_r \cdot [C^{<t-1>}, x^{<t>}]+b_r)
$$

​	$C_{t-1}$是前一步的隐藏状态。

**更新门（Update Gate）**

- 更新门决定了当前时间步的隐藏状态有多少需要被更新。它控制了新信息和旧信息的混合比例。

$$
Γ_u = \sigma(W_u \cdot [C^{<t-1>}, x^{<t>}]+b_u)
$$

**候选隐藏状态（Candidate Hidden State）**：

- 候选隐藏状态是当前时间步的新信息，它结合了当前输入和前一时间步的隐藏状态（经过相关门的调节）。

$$
\tilde{C}^{<t>} = \tanh(W_C \cdot [Γ_r \ast C^{<t-1>}, x^{<t>}]+b_c)
$$

**隐藏状态更新（Hidden State Update）**：

- 最终的隐藏状态是前一时间步的隐藏状态和候选隐藏状态的加权和，由更新门控制。

$$
C^{<t>} = (1 - Γ_u) \ast C^{<t-1>} + Γ_u \ast \tilde{C}^{<t>}
$$

- **捕捉长距离依赖关系**：通过门控机制，GRU能够更好地捕捉和保留长距离依赖关系。
- **减轻梯度消失问题**：门控机制帮助GRU在反向传播过程中保持梯度的稳定，减轻梯度消失的问题。
- **计算效率高**：相比LSTM，GRU结构更简单，计算效率更高。

![image-20240723204318713](images/image-20240723204318713.png)

1. **更新门（Update Gate）**：更新门的值介于0到1之间，用于控制当前状态和新信息的混合程度。更新门的值通过sigmoid函数计算，sigmoid函数的输出范围是0到1，因此更新门的值也在这个范围内。
2. **候选状态（Candidate State）**：候选状态是GRU在当前时间步计算的潜在新状态。这个状态会根据更新门的值决定是否更新到最终状态。
3. **门控机制**：更新门的作用是决定是否更新记忆单元C的值。如果更新门的值接近1，表示需要更新记忆单元；如果接近0，表示保持当前记忆单元的值不变。
4. **记忆单元C的更新**：记忆单元C的值会根据更新门的值进行更新。具体来说，如果更新门的值为1，记忆单元C会被更新为候选状态；如果更新门的值为0，记忆单元C保持不变。
5. **语法示例**：你提到的语法示例是为了说明GRU如何在自然语言处理中记住主语的数（单数或复数）并在适当的时候更新记忆单元C。例如，当主语是单数时，记忆单元C的值可能为1；当主语是复数时，记忆单元C的值可能为0。GRU会记住这个值直到需要更新谓语动词的数时。

![img](https://i-blog.csdnimg.cn/blog_migrate/266cc3c55c7d848108d2b0ffe611794e.jpeg)

#### 长短期记忆(LSTM)

​	LSTM（长短期记忆网络）和GRU（门控循环单元）都是RNN（循环神经网络）的变体，用于处理序列数据，但LSTM比GRU更复杂，具有更强的泛化能力。

GRU有两个主要的门控机制：

1. **更新门（Update Gate, $Γ_u$）**：控制当前状态和新信息的混合程度。
2. **相关门（relation Gate, $Γ_r$）**：决定如何结合新输入和之前的记忆。

LSTM引入了更多的门控机制，使其能够更好地捕捉长时间依赖关系。LSTM有三个主要的门控机制：

1. **遗忘门（Forget Gate, $Γ_f$）**：决定是否丢弃之前的记忆。
2. **更新门（update Gate, $Γ_u$）**：决定是否更新当前的记忆。
3. **输出门（Output Gate, $Γ_o$）**：决定当前状态的输出。

LSTM的状态更新公式更复杂，包含两个状态：细胞状态$C_t$和隐藏状态$a_t$。细胞状态通过遗忘门和更新门进行更新，而隐藏状态通过输出门进行更新。

![image-20240723210031293](images/image-20240723210031293.png)

​	窥孔连接引入了前一时刻的细胞状态$C^{<t−1>}$，使得门控机制能够直接访问细胞状态的信息。	

​	例如，遗忘门的计算公式可以表示为：
$$
Γ_f = \sigma(W_f \cdot [C^{<t-1>}, x^{<t>}]+U_f\cdot C^{<t-1>}+b_f)
$$
​	其中，$U_f$是与细胞状态相关的权重矩阵。

​	如果细胞状态是一个100维的向量那么$ C^{<t-1>}$的第5个元素只会影响门控向量的第5个元素。

​	这里使用$a^{<t-1>)}$和$x^{<t>}$一起来计算遗忘门、更新门和输出门的值。注意一下下方的三张图，把它们按照时间次序连接起来，这里输入$x^{<1>}、x^{<2>}、x^{<3>}$，有个很有意思的事情（红色直线），这条线显示了只要你正确地设置了遗忘门和更新门，LSTM是相当容易把$c^{<t>}$的值一直往下传递到右边。

![img](https://i-blog.csdnimg.cn/blog_migrate/722e0f3509ccb7a8e881b75874f2105c.jpeg)

## 1.5-双向RNN(BRNN)

1. **基本概念**

- **前向序列**：从左到右处理输入序列（例如，$x^{<1>}, x^{<2>}, x^{<3>}, …$）。
- **后向序列**：从右到左处理输入序列（例如，$…, x^{<3>}, x^{<2>}, x^{<1>}$）。

2. **信息流动**

在BRNN中，信息在两个方向上流动：

- **前向传播**：标准RNN的前向传播，从输入序列的起点到终点。
- **后向传播**：反向处理输入序列，从终点到起点。

3. **结合前向和后向信息**

在每个时间点（例如时间点t=3），BRNN结合了前向和后向的信息：

- **前向信息**：在时间点t=3，前向序列提供了x1和x2的信息。
- **后向信息**：在时间点t=3，后向序列提供了x4的信息。

4. **预测的优势**

通过结合前向和后向的信息，BRNN能够在时间点t=3时利用过去、现在和未来的信息进行预测。这种双向处理方式使得BRNN在处理序列数据（如自然语言处理、时间序列预测等）时具有更高的准确性和鲁棒性。

![image-20240724192810244](images/image-20240724192810244.png)

![image-20240724192819085](images/image-20240724192819085.png)

深度RNN（Deep RNN）通过堆叠多个RNN层来学习复杂的函数。

### 1. **基本概念**

- **多层RNN**：通过堆叠多个RNN层（例如，$l=1, l=2, l=3$）来形成更深的网络。
- **激活函数**：在每一层和每个时间步上，使用激活函数（例如，g）来计算激活值。

### 2. **计算过程**

在深度RNN中，每一层的激活值 ( $a^{[l]<t>}$ ) 是通过前一层的激活值和当前层的输入计算得到的。例如：

- **输入**：在时间步t=3，输入为 ( $x^{<3>} $)。
- **激活值**：在第二层（l=2），时间步t=3的激活值 ( $a^{[2]<3>}$ ) 由以下公式计算：$ [ a^{[2]<3>} = g(W_a \cdot a^{[2]<2>} + W_x \cdot x^{<3>} + b) ] $其中，( $W_a$ ) 是激活权重矩阵，( $W_x$ ) 是输入权重矩阵，b是偏置项。

### 3. **训练复杂性**

深度RNN在训练时计算复杂且耗时，原因包括：

- **长时间依赖**：需要处理长时间范围内的依赖关系。
- **梯度消失和爆炸**：随着层数增加，梯度可能会消失或爆炸，导致训练困难。

![image-20240724193324378](images/image-20240724193324378.png)

# 词嵌入

**One-Hot编码（$O_{num}$）的局限性**：

1. **高维稀疏性**：每个单词都被表示为一个高维向量，其中只有一个位置是1，其余都是0。例如，如果词汇表有10,000个单词，那么每个单词的one-hot向量就是一个10,000维的向量。这种表示方式非常稀疏且高维。
2. **无法捕捉语义关系**：one-hot编码无法捕捉单词之间的语义关系。例如，“apple”和“orange”在one-hot表示中是完全不同的向量，它们之间的内积为0，这些向量之间的距离都一样，无法反映它们在语义上的相似性。
3. **维度不变**：one-hot向量的维度是固定的，等于词汇表的大小。这意味着随着词汇表的增长，向量的维度也会增加，导致计算复杂度增加。

**词嵌入（$E_{num}$）的优势**：

词嵌入通过将单词映射到一个低维的连续向量空间来解决上述问题。以下是词嵌入的几个关键优势：

1. **低维密集表示**：词嵌入将每个单词表示为一个低维的密集向量（例如，300维），这大大减少了计算复杂度。
2. **捕捉语义关系**：词嵌入能够捕捉单词之间的语义关系。例如，在词嵌入空间中，“apple”和“orange”的向量距离会比“apple”和“king”的距离更近。这是因为词嵌入通过训练数据学习到了单词的上下文信息。
3. **向量运算**：词嵌入允许我们进行向量运算来捕捉单词之间的关系。例如，向量运算“King - Man + Woman”会得到一个接近“Queen”的向量。这种运算反映了单词之间的语义类比关系。

例如句子“I want a glass of orange ____”很可能会被填充为“juice”，因为“orange”和“juice”在训练数据中经常一起出现。而句子“I want a glass of apple ____”也应该填充为“juice”，但one-hot编码无法捕捉到“apple”和“orange”之间的相似性，导致泛化能力差。

通过使用词嵌入，模型可以学习到“apple”和“orange”在某些上下文中是相似的，从而更好地进行预测。

![image-20240724194053752](images/image-20240724194053752.png)

## 2.1-特征化表示（featurized representation）

​	特征化表示（featurized representation）通过为每个单词分配多个特征值来捕捉其语义关系。

1. **多维特征向量**：每个单词被表示为一个多维向量，每个维度代表一个特征。例如，性别（Gender）、高贵（Royal）、年龄（Age）、食物（Food）等。每个特征的值可以是一个实数，表示该单词在该特征上的程度。
2. **特征值的分配**：根据经验或训练数据，为每个单词分配特征值。例如，性别特征中，男性的值为-1，女性的值为+1。对于“man”，性别值为-1；对于“woman”，性别值为+1。类似地，“king”的性别值可能为-0.95，而“queen”的性别值为+0.97。
3. **捕捉语义关系**：通过这种方式，单词之间的语义关系可以通过向量之间的距离或角度来度量。例如，“apple”和“orange”在食物特征上的值都为1，因此它们在食物特征上是相似的。
4. **捕捉复杂关系**：能够捕捉单词之间的复杂语义关系。例如，“king”和“queen”在性别和高贵特征上都有相似的值，因此它们在这些特征上是相似的。

![image-20240724194746423](images/image-20240724194746423.png)

### 可视化

​	嵌入（embeddings）确实是通过将单词映射到一个高维空间中的点来表示它们的语义关系。由于我们无法直观地展示300维空间，所以通常使用降维技术（如t-SNE）将这些高维数据映射到低维空间（如2D或3D）进行可视化。

​	t-SNE（t-Distributed Stochastic Neighbor Embedding）是一种常用的降维算法，它能够将高维数据映射到低维空间，同时尽量保持数据点之间的相对距离和结构。这使得我们可以在低维空间中观察到单词之间的相似性和聚类情况。

​	假设我们有一个300维的嵌入空间，其中每个单词都被表示为一个300维的向量。通过t-SNE算法，我们可以将这些向量降维到2D空间，并绘制出一个2D图像。在这个图像中，语义相似的单词会聚集在一起。例如，“apple”和“orange”可能会靠得很近，而“king”和“queen”也会靠得很近。

![image-20240724195101330](images/image-20240724195101330.png)

## 2.2-使用词嵌入

### 词嵌入的学习

1. **大规模无标签文本**：词嵌入的学习通常依赖于非常大的无标签文本数据集。这些数据集可以包含数亿甚至数十亿个单词。通过在这些大规模数据集上训练，词嵌入模型能够捕捉到单词之间的语义关系。例如，通过大量的文本，模型可以学习到“orange”和“durian”都是水果，“farmer”和“cultivator”都是从事农业工作的人。
2. **预训练词嵌入**：你可以使用预训练的词嵌入模型，这些模型已经在大规模数据集上训练好了，可以直接下载使用。这些预训练模型已经包含了丰富的语义信息，可以应用到各种NLP任务中。

### 迁移学习

1. **迁移到新任务**：在有了预训练的词嵌入之后，你可以将这些嵌入应用到一个新的任务中，即使这个任务的训练集很小。例如，在命名实体识别任务中，即使你的训练集中没有“durian”或“cultivator”这两个词，预训练的词嵌入仍然可以帮助模型理解这些词的语义。
2. **小规模标记数据**：即使你的训练集只有10万单词甚至更少，预训练的词嵌入仍然可以提供有价值的信息。通过将预训练的词嵌入应用到你的任务中，模型可以利用从大规模无标签数据中学到的知识，从而在小规模标记数据上表现得更好。
3. **微调**：你还可以选择在新任务的数据上继续微调预训练的词嵌入，以进一步提高模型的性能。这种方法可以使词嵌入更加适应特定任务的需求。

例如句子“Robert Lin is a durian cultivator”中的“durian”和“cultivator”可能在你的训练集中没有出现过。但是，通过使用预训练的词嵌入，模型可以理解“durian”是水果，就像“orange”一样，而“cultivator”是从事农业工作的人，就像“farmer”一样。因此，模型可以从训练集中学到的“an orange farmer”泛化到“a durian cultivator”。

![image-20240724195608857](images/image-20240724195608857.png)

### 词嵌入和人脸识别关系

#### 相似性

1. **向量表示**：无论是词嵌入还是人脸编码，核心思想都是将对象（单词或人脸）映射到一个高维向量空间中。对于词嵌入，每个单词被表示为一个固定维度的向量（例如300维）。对于人脸编码，每张人脸图片被表示为一个固定维度的向量（例如128维）。
2. **相似性度量**：在这两个领域中，相似性都是通过计算向量之间的距离来度量的。对于词嵌入，语义相似的单词在向量空间中距离较近。对于人脸编码，相同或相似的人脸在向量空间中距离较近。
3. **神经网络**：两者都使用神经网络来学习这些向量表示。词嵌入通常通过训练神经网络（如Word2Vec或GloVe）来学习单词的向量表示。人脸编码则通过训练Siamese网络或其他深度学习模型来学习人脸的向量表示。

#### 差异

1. **固定词汇表 vs. 动态输入**：在词嵌入中，我们通常有一个固定的词汇表（例如10,000个单词），并为每个单词学习一个固定的向量表示。这意味着每个单词的嵌入是固定的，不会随输入变化。而在人脸识别中，神经网络可以处理任意输入的人脸图片，并为每张图片生成一个动态的编码结果。
2. **未知单词处理**：在自然语言处理（NLP）中，如果遇到词汇表中没有的单词，通常会将其标记为未知单词（<UNK>），并使用一个固定的向量表示。而在人脸识别中，神经网络可以处理从未见过的人脸图片，并生成相应的编码结果。

## 2.3 词嵌入的特性

​	假设我们使用的是四维的嵌入向量

**计算差异向量**：首先，我们计算两个单词之间的差异向量。例如，计算“man”和“woman”之间的差异向量：
$$
e_{man} - e_{woman}
$$


这个差异向量捕捉了“man”和“woman”之间的主要差异，即性别。

**应用差异向量**：接下来，我们将这个差异向量应用到另一个单词对上。例如，计算“king”和“queen”之间的差异向量：
$$
e_{king} - e_{queen}
$$


这个差异向量也捕捉了“king”和“queen”之间的主要差异，即性别。

**类比推理**：通过比较这两个差异向量，我们可以发现它们是相似的：
$$
e_{man} - e_{woman} \approx e_{king} - e_{queen}
$$


这意味着“man”对“woman”的关系类似于“king”对“queen”的关系。





因此当算法被问及“man对woman相当于king对什么”时，算法所做的就是计算以下向量运算：
$$
e_{man} - e_{woman} + e_{king}
$$
然后找出一个向量，使得这个向量与其最接近，也就是说，算法会找到一个单词，使得：
$$
e_{man} - e_{woman} \approx e_{king} - e_{?}
$$
在这个例子中，最接近的单词是“queen”。

在类比推理中，我们希望找到一个单词 ( w )，使得以下等式近似成立：
$$
e_{\text{man}} - e_{\text{woman}} \approx e_{\text{king}} - e_w
$$


为了找到这个单词 ( w )，我们可以计算以下向量：
$$
e_{\text{king}} - e_{\text{man}} + e_{\text{woman}}
$$


然后，我们寻找一个单词 ( w )，使得其向量 ( e_w ) 与上述向量的余弦相似度最大：
$$
\text{sim}(e_w, e_{\text{king}} - e_{\text{man}} + e_{\text{woman}})
$$


#### 相似函数

​	余弦相似度（cosine similarity）是衡量向量相似度的一个常用且有效的方法，通过计算两个向量之间的夹角的余弦值来衡量它们的相似度。
$$
\text{sim}(u, v) = \frac{u^T \cdot v}{||u|| \, ||v||}
$$
其中：

- ( $u^T \cdot v$ ) 表示向量 ( $u$ ) 和 ( $v$ ) 的内积。
- $( ||u|| ) 和 ( ||v|| )$ 分别表示向量$( u ) 和 ( v )$ 的范数（即向量的长度）。

1. **范围**：余弦相似度的值介于 -1 和 1 之间。
   - 当两个向量完全相同（夹角为0度）时，余弦相似度为 1。
   - 当两个向量完全不相关（夹角为90度）时，余弦相似度为 0。
   - 当两个向量完全相反（夹角为180度）时，余弦相似度为 -1。
2. **方向性**：余弦相似度关注的是向量的方向而不是长度，因此它特别适合用于衡量文本或单词向量的相似度，因为这些向量的长度可能会有所不同，但方向更能反映它们的语义关系。

除了余弦相似度外，还有其他相似度度量方法，如欧氏距离（Euclidean distance）和平方距离（Squared distance）。

![image-20240724201131070](images/image-20240724201131070.png)

## 2.4 嵌入矩阵

​	嵌入矩阵 ( E ) 是一个大小为$( 300 \times 10,000 )$ 的矩阵（假设词汇表有10,000个单词），其中每一列代表一个单词的嵌入向量。这个矩阵的每一列都是一个300维的向量，表示词汇表中对应单词的嵌入。

​	假设我们有一个单词“orange”，它在词汇表中的编号是6257。我们用一个one-hot向量 ( $O_{6257}$ ) 来表示这个单词，这个向量除了第6257个位置上是1，其余位置都是0。这个one-hot向量是一个10,000维的列向量。

当我们用嵌入矩阵 ( E ) 去乘以这个one-hot向量 ( $O_{6257} $) 时，我们得到一个300维的向量：
$$
E \cdot O_{6257} = e_{6257}
$$
$( e_{6257} ) $是“orange”的嵌入向量。

​	在实际计算中，直接用矩阵乘法来计算嵌入向量效率很低，因为one-hot向量是一个高维稀疏向量，大部分元素都是0。因此，在实践中，我们通常使用专门的函数来直接查找嵌入矩阵 ( E ) 的某一列，而不是进行完整的矩阵乘法。

![image-20240724201500996](images/image-20240724201500996.png)

## 2.5-学习词嵌入

​	我们从稍微复杂一些的算法开始，因为我觉得这样更容易对算法的运作方式有一个更直观的了解，之后我们会对这些算法进行简化，使你能够明白即使一些简单的算法也能得到非常好的结果，让我们开始吧。

**词嵌入的生成**

1. **建立one-hot向量**：从第一个词“I”开始，建立一个one-hot向量 ( $O_{4343}$ )，这个向量在第4343个位置是1，其余位置都是0。这个one-hot向量是一个10,000维的向量。
2. **生成嵌入向量**：用嵌入矩阵 ( E ) 乘以one-hot向量 ( $O_{4343}$ )，得到嵌入向量 ($e_{4343}$ )。这个嵌入向量是一个300维的向量，表示单词“$I$”的嵌入。
3. **对其他单词进行相同操作**：对句子中的其他单词（如“want”、“a”、“glass”、“of”、“orange”）进行相同的操作。每个单词都有一个对应的one-hot向量，通过与嵌入矩阵 ( E ) 相乘，得到相应的嵌入向量。

**嵌入向量的堆叠**

将所有单词的嵌入向量堆叠在一起，形成一个大的输入向量。例如，如果每个嵌入向量是300维，而句子中有6个单词，那么最终的输入向量是一个1800维的向量（6个300维向量堆叠在一起）。

**输入神经网络**

1. **输入到神经网络**：将这个1800维的向量输入到神经网络中。神经网络会处理这个向量，提取出有用的特征。
2. **通过softmax层**：经过神经网络处理后，输出会通过一个softmax层。softmax层有自己的参数，用于在10,000个可能的输出中预测下一个单词。

![image-20240724201852367](images/image-20240724201852367.png)

### 固定历史窗口

1. **定义历史窗口**：在自然语言处理任务中，使用固定的历史窗口是为了简化模型的输入维度。假设我们总是使用前4个单词来预测下一个单词，这个4就是算法的超参数。无论句子有多长，我们只看前4个单词。
2. **输入维度固定**：使用固定的历史窗口意味着输入到神经网络的特征向量维度总是固定的。例如，如果每个单词的嵌入向量是300维，而我们使用4个单词的历史窗口，那么输入到神经网络的特征向量就是1200维（4个300维向量堆叠在一起）。
3. **处理任意长度的句子**：这种方法允许我们处理任意长度的句子，因为输入的维度总是固定的。无论句子有多长，我们只需要考虑前4个单词。

事实上通过这个算法能很好地学习词嵌入，原因是，如果你还记得我们的orange jucie，apple juice的例子，在这个算法的激励下，apple和orange会学到很相似的嵌入，这样做能够让算法更好地拟合训练集，因为它有时看到的是orange juice，有时看到的是apple juice。如果你只用一个300维的特征向量来表示所有这些词，算法会发现要想最好地拟合训练集，就要使apple（苹果）、orange（橘子）、grape（葡萄）和pear（梨）等等，还有像durian（榴莲）这种很稀有的水果都拥有相似的特征向量。
![image-20240724202621529](images/image-20240724202621529.png)

### 上下文类型

1. **固定历史窗口**：这种方法使用目标词之前的固定数量的单词作为上下文。例如，在句子“I want a glass of orange juice to go along with my cereal.”中，如果我们使用前4个单词作为上下文，那么上下文就是“I want a glass”。这种方法常用于语言模型，因为它能够捕捉到目标词之前的语境信息。
2. **左右上下文**：这种方法使用目标词左右两边的单词作为上下文。例如，在同一句子中，如果我们使用目标词左右各4个单词作为上下文，那么上下文就是“a glass of orange”和“to go along with”。这种方法能够捕捉到目标词前后更广泛的语境信息，有助于更准确地预测目标词。
3. **单个上下文词**：这种方法使用目标词的前一个或后一个单词作为上下文。例如，只使用“orange”来预测“juice”。这种方法简单但有效，特别是在处理短语或固定搭配时。
4. **邻近单词**：这种方法使用目标词附近的一个单词作为上下文。例如，使用“glass”来预测“orange”。这种方法在Skip-Gram模型中非常常见，能够有效地捕捉单词之间的局部关系。

如果你真想建立一个语言模型，用目标词的前几个单词作为上下文是常见做法。但如果你的目标是学习词嵌入，那么你就可以用这些其他类型的上下文，它们也能得到很好的词嵌入。

## 2.6 Word2Vec

​	一种简单而且计算时更加高效的方式来学习这种类型的嵌入，Skip-Gram模型的目标是通过给定一个上下文词来预测其周围的目标词。这种方法通过构造一个监督学习问题来学习词嵌入。

### 构造监督学习问题

1. **选择上下文词**：在给定的句子中，例如“I want a glass of orange juice to go along with my cereal.”，我们可以选择“orange”作为上下文词。
2. **选择目标词**：在上下文词“orange”前后一定范围内（例如前后5个词或前后10个词）随机选择目标词。例如，可能选择“juice”作为目标词，也可能选择“glass”或“my”作为目标词。
3. **构造配对**：通过这种方式，我们构造了多个上下文词和目标词的配对。例如：
   - 上下文词“orange”，目标词“juice”
   - 上下文词“orange”，目标词“glass”
   - 上下文词“orange”，目标词“my”

**构造这个监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型。**

![image-20240724203318907](images/image-20240724203318907.png)

### 词嵌入模型	

1.嵌入矩阵$E$将one-hot向量$O_c$通过公式$e_c = E O_c$转换为嵌入向量$e_c$，嵌入矩阵$E$是通过训练学习到的，每一行对应一个词的嵌入向量

2.嵌入向量$e_c$输入到softmax单元，输出概率分布$\hat{y}$，softmax函数将输入向量转换为概率分布，表示每个词作为目标词的概率。具体公式为：
$$
p(t|c) = \frac{e^{\theta_t^T e_c}}{\sum_{j=1}^{V} e^{\theta_j^T e_c}}
$$
​	其中$θ_t$是与目标词相关的参数向量，$\theta_t^T e_c$表示两个单词的相似度。内积越大，表示两个单词的相似程度越高。内积能够捕捉向量之间的方向和长度关系。

​		**平滑处理**：指数函数可以将线性组合的结果转换为非线性结果，从而避免了直接计算概率时可能出现的负值或零值问题。

​		**数值稳定性**：在计算过程中，指数函数可以帮助保持数值稳定性，特别是在处理大范围的数值时。它可以防止在计算过程中出现数值下溢或上溢的问题。

​		**归一化**：通过使用指数函数，可以确保所有概率的总和为1，这对于概率分布来说是必要的。分母中的求和项${\sum_{j=1}^{V} e^{\theta_j^T e_c}}$确保了这一点。

​		**凸优化**：使用指数函数可以将优化问题转换为凸优化问题，使得求解过程更加高效和稳定。

3.**损失函数**：使用softmax损失函数，计算目标词和预测词之间的差异：
$$
L(\theta) = -\sum_{i=1}^{V} y_i \log(\hat{y}_i)
$$


​	对于一个10,000词的词汇表，如果每个嵌入向量是300维的，那么嵌入矩阵$E$的大小就是10,000 x 300。

![image-20240724205736121](images/image-20240724205736121.png)

**计算速度问题**：

- 在softmax模型中，每次计算概率时，需要对词汇表中的所有词进行求和操作。对于一个10,000词的词汇表，这个操作已经相当慢了。如果词汇表扩大到100,000或1,000,000词，计算速度会显著降低。
- 这种计算瓶颈主要来自于softmax的分母部分，即对所有词的指数和进行归一化。

**解决方案**：

- 为了解决计算速度问题，研究人员提出了几种优化方法，如**负采样（Negative Sampling）**和**层次Softmax（Hierarchical Softmax）**。

#### 分级softmax分类器

1. **分级结构**：
   - 分级softmax分类器使用一个树形结构，将词汇表分成多个层次。每个内部节点是一个二分类器，决定目标词是在当前节点的左子树还是右子树中。
   - 这种结构的叶子节点代表具体的词汇表中的词。

2. **计算效率**：
   - 传统softmax分类器需要对整个词汇表进行求和计算，计算成本是线性的，即与词汇表大小成正比。
   - 分级softmax分类器通过树形结构，将计算成本降低到对数级别，即与词汇表大小的对数成正比。这大大提高了计算效率，尤其是在处理大词汇表时。

3. **非对称树结构**：
   - 在实践中，分级softmax分类器不会使用完美平衡的分类树。相反，树的结构会根据词的频率进行优化。
   - 常用词（如“the”、“of”）会放在树的顶部，因为这些词出现频率高，检索时需要更少的计算步骤。
   - 不常用词（如“durian”）会放在树的更深处，因为这些词出现频率低，检索时需要更多的计算步骤，但这种情况较少发生。

​	它告诉你目标词是在词汇表的前5000个中还是在词汇表的后5000个词中，假如这个二分类器告诉你这个词在前5000个词中，然后第二个分类器会告诉你这个词在词汇表的前2500个词中，或者在词汇表的第二组2500个词中，诸如此类，直到最终你找到一个词准确所在的分类器，那么就是这棵树的一个叶子节点。

![image-20240724205746747](images/image-20240724205746747.png)

**上下文词的采样**：

- 在Skip-Gram模型中，目标词$t$会在上下文词$c$的正负10个词距内进行采样。这意味着模型会尝试预测目标词在给定上下文词附近的词。
- 选择上下文词$c$的方法之一是对语料库进行均匀且随机的采样。然而，这种方法会导致一些高频词（如“the”、“of”、“a”、“and”、“to”）频繁出现，而其他低频词（如“orange”、“apple”、“durian”）则很少出现。

**高频词的问题**：

- 如果训练集中大部分都是高频词，模型会花大量时间更新这些高频词的嵌入向量$e_c$。这会导致模型对高频词的嵌入向量进行过度优化，而对低频词的嵌入向量更新不足。
- 这种情况会影响模型的整体性能，因为低频词的嵌入向量可能无法很好地捕捉其语义信息。

**平衡常见词和不常见词**：

- 为了平衡常见词和不常见词的训练，可以采用不同的启发式方法。例如，可以对高频词进行下采样（subsampling），即减少高频词在训练中的出现次数。
- 具体方法是根据词频对每个词进行采样概率计算，频率越高的词采样概率越低，从而减少高频词的出现次数，增加低频词的出现机会。

#### 负采样(Negative Sampling)

负采样是一种用于训练词嵌入模型（如Word2Vec）的技术。它的主要目的是通过生成正样本和负样本来优化模型的学习过程。

1. **正样本生成**：
   - 选择一个上下文词（context word），例如“orange”。
   - 在一定词距内（例如正负10个词距）选择一个目标词（target word），例如“juice”。
   - 将这个配对标记为正样本（1），因为“orange”和“juice”在上下文中是相关的。
2. **负样本生成**：
   - 使用相同的上下文词“orange”。
   - 从词汇表中随机选择一个词，例如“king”，并将其标记为负样本（0），因为“orange”和“king”在上下文中通常是不相关的。即使这些词偶尔出现在上下文词的词距内，也将其标记为负样本。
   - 重复这个过程，随机选择其他词汇表中的词，例如“book”、“the”、“of”等，并将这些配对标记为负样本（0）。

通过这种方式，模型可以学习到哪些词在上下文中是相关的（正样本），哪些是不相关的（负样本）。

![image-20240725204600357](images/image-20240725204600357.png)

1. **构造监督学习问题**：
   - 学习算法的输入是词对（context-word），例如“orange-juice”。
   - 目标是预测这些词对的标签（target），即预测输出y。
   - 问题是给定一对词，判断它们是否会一起出现，或者是否是通过随机采样获得的。
2. **选择K的值**：
   - 对于小数据集，K的值在5到20之间较为合适。
   - 对于大数据集，K的值应较小，通常在2到5之间。
   - 数据集越小，K的值应越大；数据集越大，K的值应越小。

##### 负采样模型

- 每个可能的目标词都有一个参数向量$θ_t$每个可能的上下文词也有一个嵌入向量$e_c$我们使用公式:

$$
\sigma(\theta_t^T e_c)
$$

​		来估计$y=1$的概率。对于每一个正样本，我们生成K个负样本，这意味着我们有一个正负样本比例为$\frac{1}{K}$例如，如果K=4，那么每一个正样本会有4个负样本。

- 其预测过程就是假设输入词是orange，即词6257，你要做的就是输入one-hot向量，再传递给$E$ ，通过两者相乘获得嵌入向量$e_{6527}$，我们就得到了10,000个可能的逻辑回归分类问题。

1. **负采样的作用**：
   - 负采样将10,000维的softmax问题转化为10,000个二分类问题，每个问题都很容易计算。
   - 每次迭代中，只训练K+1个分类器（K个负样本和1个正样本），而不是全部10,000个分类器。

2. **计算成本的降低**：
   - 由于每次迭代只更新K+1个逻辑单元，计算成本显著降低。
   - 负采样通过生成负样本（如“king”、“book”、“the”、“of”）来训练模型，使得每次迭代的计算量更小。

![image-20240725204538688](images/image-20240725204538688.png)

##### 负样本的采样

1. **经验频率采样**：
   - 根据词在语料库中的出现频率进行采样。
   - 这种方法会导致高频词（如“the”、“of”、“and”）被过度采样。

​	2.**均匀采样**：

- 每个词被采样的概率相同，即 $\frac 1{|V|}$，其中 $∣V∣$ 是词汇表的总词数。
- 这种方法对英文单词的分布没有代表性，因为高频词和低频词的采样概率相同。

​	3. **Mikolov等人的方法**：

- 介于上述两种极端方法之间的采样方法。
- 具体公式为：

$$
P(w_i) = \frac{f(w_i)^{3/4}}{\sum_{j=1}^{|V|} f(w_j)^{3/4}}
$$

其中$f(w_i)$是词$w_i$在语料库中的频率。

- 这种方法通过对词频进行 $\frac 34$ 次方的处理，使得高频词的采样概率降低，而低频词的采样概率增加，从而达到一个平衡。这种方法的优点在于，它既避免了高频词被过度采样的问题，又比均匀采样更能反映实际的词频分布。

![image-20240725205409748](images/image-20240725205409748.png)

## 2.7Glove词向量

​	GloVe（Global Vectors for Word Representation）是一种基于统计的词向量生成方法。

​	假设$X_{ij}$表示单词$i$在单词$j$的上下文中出现的次数，在Glove算法中$i和j$分别表示目标词和上下文，因此可以理解位$X_{tc}$，其就是一个计数器，用来记录单词$t和c$在彼此接近时出现的概率。

![image-20240725212441629](images/image-20240725212441629.png)

GloVe模型是通过最小化目标函数来优化词向量的。具体来说，GloVe模型的目标是最小化以下公式：
$$
\sum_{i=1}^{V} \sum_{j=1}^{V} f(X_{ij})(\theta_i^T e_j - \log(X_{ij}))^2
$$
​	其中，$f(X_{ij})$是一个加权函数，用于处理当$X_{ij}=0$的情况，并且规定$ 0\log{0} = 0 $以避免对数函数在零点处的定义问题。

​	它不仅帮助处理稀有词对的情况，还能平衡高频词和低频词的权重。具体来说：

1. **高频词（如this, is, of, a）**：这些词在语料库中出现频率很高，但它们的语义信息相对较少。如果不给它们适当的权重，它们会在模型中占据过多的影响力，导致模型无法有效捕捉其他词的语义关系。
2. **低频词（如durion）**：这些词虽然出现频率低，但它们可能包含丰富的语义信息。如果权重太小，它们的语义信息会被忽略，影响模型的表现。

常用的加权函数之一是：
$$
f(X_{ij}) = \left\{
\begin{array}{ll}
(X_{ij}/X_{\max})^\alpha ; \text{if } X_{ij} < X_{\max} \\
1 ; \text{otherwise}
\end{array}
\right.
$$
​	其中，$X_{\max}$是一个阈值，$α$ 通常取值在 0.75 左右。这个函数的设计原则是：

对于高频词对，当 $X_{ij}$ 接近或超过 $X_{max}$ 时，权重趋近于 1，避免过分放大它们的影响。

对于低频词对，当 $X_{ij}$ 较小时，权重会根据 $(X_{ij}/X_{\max})^\alpha$ 进行缩放，确保它们的语义信息不会被完全忽略。

![image-20230111200206087](https://i-blog.csdnimg.cn/blog_migrate/56b5bba43dc7596508869b7518acb0d4.png)

### 嵌入向量的解释性

1. **高维空间的复杂性**：词嵌入通常在高维空间中表示（例如300维或更高）。在这样高维的空间中，每个维度的具体含义变得更加抽象和难以解释。即使我们可以通过数学方法确定某些维度的重要性，但这些维度的具体语义解释仍然是一个挑战。
2. **特征组合**：嵌入向量的每个维度可能是多个语义特征的组合。例如，性别、皇室、年龄等特征可能会混合在一起，形成一个复杂的特征空间。这意味着一个特征轴可能同时包含这些特征的部分信息，而不是单一的语义。
3. **坐标系的选择**：在训练过程中，学习算法可能会选择不同的坐标系来表示特征。即使我们设定了某些特征轴（如性别和皇室），实际训练得到的嵌入向量可能不会与这些轴对齐。算法可能会选择其他方向作为主要特征轴，甚至可能是非正交的坐标系。
4. **可逆矩阵的影响**：如果存在一个可逆矩阵 ( A )，那么嵌入向量可以通过这个矩阵进行线性变换，得到新的表示。这意味着原始的特征轴可以被替换为新的轴，而这些新的轴可能不再具有直观的解释性。
5. **上下文依赖性**：词嵌入是通过大量语料库训练得到的，词与词之间的关系是基于它们在不同上下文中的共现频率。因此，某个特征轴的含义可能会因上下文的不同而变化。

## 2.8 情绪分类

​	情感分类是自然语言处理（NLP）中的一个重要任务，旨在分析文本并判断其情感倾向。

![image-20240725214220625](images/image-20240725214220625.png)

1. **模型架构**：
   - **输入**：句子 “dessert is excellent”。
   - **词典**：使用一个包含10,000个词的词汇表。
   - **词嵌入**：从一个大规模的文本集（如一亿或一百亿个词）中学习到的词嵌入矩阵 ( E )。每个词在词典中都有一个对应的嵌入向量，例如 “the” 对应 ( $e_{8928} $)。
   - **特征向量**：将句子中的每个词转换为其对应的300维嵌入向量，然后对这些向量进行求和或求平均，得到一个300维的特征向量。
   - **分类器**：将这个特征向量输入到softmax分类器中，输出五个可能的情感评分（从一星到五星）的概率。

2. **平均值运算的优点**：
   - **灵活性**：适用于不同长度的评论，无论评论有多少个词，都可以通过求和或求平均得到一个固定维度的特征向量。
   - **简便性**：计算简单，易于实现。

3. **模型的局限性**：
   - **忽略词序**：平均值运算忽略了词语的顺序信息，这在某些情况下会导致误分类。例如，句子 “Completely lacking in good taste, good service, and good ambiance.” 中虽然 “good” 出现了多次，但整体情感是负面的。如果仅仅对词嵌入进行求和或求平均，模型可能会误认为这是一个正面评价。

![image-20240725220310785](images/image-20240725220310785.png)

因此我们可以用一个RNN来做情感分类。

![image-20240725220252967](images/image-20240725220252967.png)

1. **RNN模型**：
   - **输入**：句子 “Completely lacking in good taste, good service, and good ambiance.”。
   - **词嵌入**：每个词转换为其对应的嵌入向量，例如 “Completely” 对应 ( e_{1852} )，“lacking” 对应 ( e_{4966} ) 等。
   - **RNN处理**：这些嵌入向量依次输入到循环神经网络（RNN）中，RNN通过其隐藏状态（如 ( a^{<0>} ), ( a^{<1>} ) 等）逐步处理整个句子，捕捉词序信息。
   - **输出**：RNN的最终隐藏状态输入到softmax分类器中，输出情感评分的概率分布。
2. **优势**：
   - **捕捉词序**：RNN能够记住前后文信息，识别出 “not good” 和 “good” 的不同含义，以及 “lacking in good taste” 的负面情感。
   - **泛化能力**：由于词嵌入是在大规模数据集上训练的，即使标记数据集中没有出现的词（如 “absent”），模型也能通过词嵌入捕捉其语义，从而做出正确的情感判断。

## 2.9 词嵌入除偏

​	去除性别歧视、种族歧视等等。

- **训练数据的偏见**：词嵌入模型是通过大量文本数据进行训练的。如果这些文本数据中包含了性别、种族、年龄等方面的偏见，模型就会学习并反映出这些偏见。例如，如果训练数据中 “man” 经常与 “computer programmer” 相关，而 “woman” 经常与 “homemaker” 相关，模型就会在类比任务中输出类似的偏见结果。
- **社会偏见的反映**：训练数据往往是从现实世界中收集的，因此它们不可避免地反映了社会中的偏见和不平等。这些偏见在模型中被放大和固化，导致模型在实际应用中产生不公平的结果。

![image-20240725220450318](images/image-20240725220450318.png)

**一、确定偏见方向**

1. **识别偏见方向**：
   - 通过计算词向量之间的差异来识别偏见方向。例如，计算 ( $e_{he} - e_{she} ) 和 ( e_{male} - e_{female}$ ) 的差异。
   - 将这些差异向量取平均，得到一个代表性别偏见的方向向量。
2. **偏见和无偏见子空间**：
   - **偏见子空间**：这个方向向量可以看作是一个1维的子空间，代表性别偏见的方向。
   - **无偏见子空间**：与偏见子空间正交的空间，即剩下的299维空间，可以看作是无偏见的子空间。

**二、中和**

1. **定义明确的词**：

   - **性别相关词**：一些词本身就包含性别信息，例如 “grandmother”（祖母）、“grandfather”（祖父）、“girl”（女孩）、“boy”（男孩）、“she”（她）、“he”（他）。这些词的定义中固有性别含义，因此不需要进行中和处理。

2. **定义不明确的词**：

   - **性别中立词**：一些词在定义上不应包含性别偏见，例如 “doctor”（医生）、“babysitter”（保姆）。这些词在理想情况下应该是性别中立的，但在训练数据中可能会反映出性别偏见。

3. **中和步骤**：

   - **投影和调整**

     ：将定义不明确的词（如 “doctor” 和 “babysitter”）的词向量投影到偏见方向上，然后调整这些词向量，使它们在偏见方向上的分量减少或消除。这可以通过以下步骤实现：

     - **投影**：计算词向量在偏见方向上的投影。
  - **调整**：从词向量中减去这个投影，得到一个在偏见方向上分量减少的词向量。

![image-20240725221526177](images/image-20240725221526177.png)

**三、均衡**

1. **均衡步骤的目的**：
   - **减少偏见**：对于一些词对（如 “grandmother” 和 “grandfather” 或 “girl” 和 “boy”），我们希望它们的词嵌入只在性别上有所区别，而在其他方面保持相似。这是为了避免不良的性别偏见，例如 “babysitter” 更接近 “grandmother” 而不是 “grandfather”。
2. **为什么需要均衡**：
   - **避免不良状态**：如果 “babysitter” 和 “grandmother” 之间的距离小于 “babysitter” 和 “grandfather” 之间的距离，这可能会加重性别偏见，暗示 “babysitting” 更适合女性。
   - **确保公平性**：通过均衡步骤，可以确保词嵌入在性别方面是中立的，不会因为性别而产生不公平的关联。

3. **实现方法**：

   - 线性代数步骤

     ：使用线性代数技术（如奇异值分解，SVD）来识别和调整词向量。具体步骤包括：

     - **均衡调整**：将词对（如 “grandmother” 和 “grandfather”）的词向量调整到与中间轴线等距的位置，确保它们与性别中立词（如 “babysitter”）的距离相同。

![image-20240725221535412](images/image-20240725221535412.png)

# Seq2Seq

​	Seq2Seq模型（Sequence to Sequence）是一种用于处理序列数据的深度学习模型，它的核心思想是将输入序列编码成一个固定长度的向量，然后再将这个向量解码成输出序列。

1. **编码器（Encoder）**：
   - 编码器通常由一个RNN（Recurrent Neural Network）组成，它逐步读取输入序列的每个元素（如单词或字符），并更新其隐藏状态。
   - 最终，编码器将整个输入序列压缩成一个固定长度的向量，这个向量包含了输入序列的全部信息。
2. **解码器（Decoder）**：
   - 解码器也是一个RNN，它使用编码器生成的向量作为初始隐藏状态，并生成输出序列。
   - 解码器逐步生成输出序列的每个元素，直到生成结束符（如句子的结束标点）。

![image-20240729191039320](images/image-20240729191039320.png)

改造**AlexNet**网络以实现图像描述

1. **去掉Softmax分类器**

- **原始**AlexNet**网络**：通常在最后一层使用Softmax分类器来进行图像分类。
- **改造后的网络**：去掉Softmax分类器，保留前面的卷积层和全连接层，这样可以提取图像的特征向量。

2. **使用RNN进行解码**

- **特征向量**：将提取的图像特征向量输入到RNN（如LSTM或GRU）中。
- **解码过程**：RNN逐步生成描述图像的文字序列。

## 3.1选择最有可能的句子

- **语言模型**：

  - 语言模型的任务是估计一个句子的可能性，即给定前面的单词，预测下一个单词的概率。例如，
    $$
    P(y^{(i)} | y^{(i-1)}, ..., y^{(1)})
    $$

  - 在生成句子时，语言模型从零向量开始（即全为0的向量），然后逐步生成每个单词。

- **机器翻译模型**：

  - 机器翻译模型由两个部分组成：编码器（encoder）和解码器（decoder）。
  - 编码器将输入句子（例如法语句子）编码成一系列向量，这些向量表示输入句子的语义信息。
  - 解码器使用这些编码向量来生成输出句子（例如英语句子），而不是从零向量开始。
  - 机器翻译模型实际上是一个条件语言模型，因为它的输出（例如英语句子）是基于输入（例如法语句子）的条件概率。
  - 具体来说，条件语言模型估计的是输出句子相对于输入句子的概率。例如

$$
P(\text{Jane is visiting Africa in September} | \text{Jane visite l'Afrique en septembre})
$$

![image-20240729192142519](images/image-20240729192142519.png)

1. **模型输出的概率分布**：
   - 输入一个法语句子（例如 “Jane visite l’Afrique en septembre.”），模型会给出各种可能的英语翻译及其对应的概率。
   - 这些概率表示每个翻译的可能性。例如，“Jane is visiting Africa in September.” 可能是一个高概率的翻译，而 “Her African friend welcomed Jane in September.” 可能是一个低概率的翻译。
2. **随机取样的问题**：
   - 如果从这些概率分布中随机取样，可能会得到一个好的翻译，但也可能得到一个不太理想的翻译。
   - 随机取样可能会导致翻译结果不稳定，有时会得到非常不准确的翻译。
3. **最大化条件概率**：
   - 为了确保翻译的准确性和一致性，我们需要找到一个英语句子 ( y )，使得条件概率 ( P(y|x) ) 最大化。
   - 这意味着你要选择一个翻译，使其在所有可能的翻译中具有最高的概率。
4. **集束搜索（Beam Search）**：
   - 集束搜索是一种常用的近似算法，用于在机器翻译中找到最可能的翻译。
   - 它通过在每一步保留若干个最有希望的候选翻译，逐步构建出最优的翻译句子。
   - 这种方法比随机取样更有效，因为它系统地探索了翻译空间，确保找到最可能的翻译。

![image-20240729192940666](images/image-20240729192940666.png)

### 为什么不是贪心搜索

1. **贪心搜索的工作原理**：
   - 贪心搜索是一种简单的算法，它在每一步都选择当前最有可能的词。
   - 具体来说，它首先选择最有可能的第一个词，然后基于这个词选择最有可能的第二个词，依此类推，直到生成整个句子。
2. **贪心搜索的局限性**：
   - 虽然贪心搜索在每一步都选择最有可能的词，但它并不考虑整个句子的全局最优性。
   - 可能会导致在某些步骤中选择了局部最优的词，但最终生成的句子并不是全局最优的。
3. **例子说明**：
   - 例如，对于法语句子 “Jane visite l’Afrique en septembre.”，贪心搜索可能会选择 “Jane is going to be visiting Africa in September.”，因为 “going” 在英语中更常见。
   - 然而，这个句子虽然正确，但不如 “Jane is visiting Africa in September.” 简洁和自然。
   - 贪心搜索在选择 “going” 之后，可能会导致后续词的选择也受到影响，最终生成一个不太理想的句子。
4. **全局最优性的重要性**：
   - 在机器翻译中，我们希望找到一个使得整个句子概率最大的翻译，而不仅仅是每个单词的局部最优。
   - 这意味着我们需要考虑整个句子的结构和上下文，而不仅仅是逐词选择。

### 近似算法

1. **组合数量巨大**：
   - 假设你的字典中有10,000个单词，并且你的翻译句子长度为10个单词，那么可能的组合数量是 $10000^{10}$，这是一个非常巨大的数字。
   - 这种情况下，逐一计算每种组合的可能性是不现实的，因为计算量过于庞大。
2. **近似搜索算法的必要性**：
   - 由于可能的句子组合数量过于巨大，我们需要使用近似的搜索算法来找到最有可能的翻译句子。
   - 近似搜索算法的目标是尽力找到使条件概率 $P(y∣x)$最大化的句子，尽管它不能保证找到的句子一定是全局最优的，但它已经足够接近最优解。

## 3.2集束(定向)搜索

​	与贪婪算法不同，集束搜索考虑多个可能的翻译选项，而不仅仅是最可能的一个。束搜索算法有一个参数$B$，称为集束宽度。B=3则意味着算法会同时考虑三个最可能的单词。通过编码和解码网络，集束搜索算法能够评估每个单词的概率值，使用softmax层来输出每个单词的概率，从而选择最可能的几个单词进行进一步的翻译步骤。

![image-20240729201430820](images/image-20240729201430820.png)

1. **第一步：选择第一个单词**：
   - 在集束搜索的第一步中，算法会评估所有可能的第一个单词的概率值。
   - 例如，对于法语句子 “Jane visite l’Afrique en Septembre.”，算法可能会选择 “in”、“jane” 和 “september” 作为第一个单词的三个最可能的选项。

2. **集束搜索的第二步**：
   - 在集束搜索的第一步中，我们已经选出了三个最可能的第一个单词，例如 “in”、“jane” 和 “september”。
   - 在第二步中，集束搜索会针对每个第一个单词，评估所有可能的第二个单词的概率值。

3. **评估第二个单词的概率**：
   - 为了评估第二个单词的概率，我们使用神经网络的编码器和解码器部分。
   - 编码器（绿色部分）处理输入的法语句子，生成编码向量。
   - 解码器（紫色部分）则根据第一个单词（例如 “in”）来预测第二个单词的概率。
   
   - 解码器的第一个输出 ( $\hat{y}^{<1>} $) 被设为第一个单词 “in”。
   - 这个输出 ($ \hat{y}^{<1>} $) 会被反馈回解码器，作为输入来预测第二个单词的概率。
   - 通过这种方式，解码器可以在给定法语句子和第一个单词 “in” 的情况下，评估第二个单词的概率。
   
5. **选择最可能的第二个单词**：
   
   - 集束搜索会根据解码器输出的概率值，选择最可能的第二个单词。
   - 例如，在第一个单词 “in” 的情况下，可能的第二个单词包括 “a”、“aaron” 等等。
   - 集束搜索会继续保留每个候选翻译的最有可能的选项，逐步构建出完整的句子。
   - 由于集束宽为3，并且词汇表中有10,000个单词，因此我们需要评估3乘以10,000，即30,000个可能的结果,即从这30000个里面选择最可能的3个。
   - **筛选候选词**：
     - 在集束搜索的过程中，如果最可能的前两个单词组合是 “in September”、“jane is” 和 “jane visits”，那么这意味着 “September” 作为第一个单词的选择被排除了。
     - 这时，我们的第一个单词的选择减少到了两个： “in” 和 “jane”。
   - **保持集束宽度**：
     - 尽管第一个单词的选择减少到了两个，但我们仍然保持集束宽度为3。
     - 这意味着我们会保留三个最可能的前两个单词组合，确保在每一步中考虑多个可能的选项，也即还是会输出3个项到底第三步。

在集束搜索的第二步中，我们更关心第一个和第二个单词对的联合概率，意味着我们要找到使第一个和第二个单词对的概率最大的组合，以此类推，集束搜索会在遇到句尾符号时终止，这表示句子的结束。

**条件概率的计算**：

- 根据条件概率的准则，第一个和第二个单词对的概率可以表示为第一个单词的概率乘以第二个单词在第一个单词已知情况下的条件概率。

$$
P(y^{1}, y^{2} | x) = P(y^{1} | x) \times P(y^{2} | y^{1}, x)
$$

![image-20240729203704062](images/image-20240729203704062.png)

### 定向集束搜索的改进

#### 长度规范化

​	在计算概率时，所有的概率值都是小于1的，通常远小于1。当多个小于1的数相乘时，结果会变得非常小，可能导致数值下溢（numerical underflow）。数值下溢是指数值太小，导致计算机的浮点表示不能精确地储存这些数值。为了避免数值下溢，我们会取概率的对数值。对数函数是严格单调递增的函数，这意味着最大化对数概率和与最大化概率乘积是等价的。通过取对数，我们将乘法转换为加法，这样可以避免数值下溢和四舍五入误差（rounding errors）。

**原始目标函数的问题**：

- 原始目标函数通过乘积概率来估计句子的概率：
  $$
  \underset{y}{\text{arg max}} \prod_{t=1}^{T_y} P(y^{t} | x, y^{1}, ..., y^{t-1})
  $$

- 由于概率值通常小于1，乘积会导致很小的数值，特别是对于长句子，这会导致数值下溢（numerical underflow）。

- 这种方法可能会倾向于生成较短的翻译，因为短句子的概率乘积不会那么小。

**对数概率的使用**：

- 为了避免数值下溢，我们使用对数概率和而不是概率乘积：
  $$
  \underset{y}{\text{arg max}} \sum_{t=1}^{T_y} \log P(y^{t} | x, y^{1}, ..., y^{t-1})
  $$

- 对数函数是单调递增的，最大化对数概率和与最大化概率乘积是等价的。

- 这种方法在数值上更稳定，避免了舍入误差和数值下溢。

**归一化目标函数**：

- 即使使用对数概率和，长句子的对数概率和仍然会变得非常负，因为加起来的项越多，结果越负。

- 为了减少对长句子的惩罚，我们可以将对数概率和归一化，通过除以翻译结果的单词数量 ( T_y )：
  $$
  \underset{y}{\text{arg max}} \frac{1}{T_y} \sum_{t=1}^{T_y} \log P(y^{t} | x, y^{1}, ..., y^{t-1})
  $$
  
- 这样，我们实际上是在最大化每个单词的对数概率的平均值。

**柔和归一化方法**：

- 在实践中，有时会使用更柔和的方法，即在 ( T_y ) 上加上指数 ( \alpha )：
  $$
  \underset{y}{\text{arg max}} \frac{1}{{T_y}^α} \sum_{t=1}^{T_y} \log P(y^{t} | x, y^{1}, ..., y^{t-1})
  $$
  
- 当 ( $\alpha = 1$ ) 时，相当于完全用长度来归一化；当 ( $\alpha = 0 $) 时，相当于没有归一化。

- 通过调整 ( \alpha ) 的值，可以在完全归一化和没有归一化之间找到一个平衡点，一般取0.7。

![image-20240729205403737](images/image-20240729205403737.png)

#### 误差分析

计算$p（y*|x）和p（\hat y|x）$来判断哪一个大以此来定向分析是谁的问题。

![image-20240729205546595](images/image-20240729205546595.png)

![image-20240729205549601](images/image-20240729205549601.png)

![image-20240729205552745](images/image-20240729205552745.png)

## 3.3Bleu分数

​	BLEU算法用于评估机器翻译的质量，通过比较机器翻译结果与人类翻译的参考结果，BLEU分数越高，表示机器翻译结果与人类翻译结果越相似。

1. **传统精确度（Precision）**：

   - 传统精确度是看机器翻译的每一个词是否出现在参考翻译中。

   - 例如，在你的例子中，机器翻译结果是 “the the the the the the the”，虽然这是一个非常糟糕的翻译结果，但因为每个词 “the” 都出现在参考翻译中，所以传统精确度是 7/7 = 1。
     $$
     \text{Precision} = \frac{\text{Number of words in MT output that appear in reference translations}}{\text{Total number of words in MT output}} 
     $$
     

2. **改良精确度（Modified Precision）**：

   - 传统精确度的一个问题是它没有考虑词的重复出现，这可能导致不准确的评估。
   - 改良精确度通过设定每个单词的计分上限来解决这个问题。计分上限是该词在参考翻译中出现的最多次数。
   - 例如，在参考翻译1中，“the” 出现了两次；在参考翻译2中，“the” 只出现了一次。所以 “the” 的计分上限是2。

3. **计算改良精确度**：

   - 在你的例子中，机器翻译结果是 “the the the the the the the”，虽然 “the” 出现了7次，但根据改良精确度的计算方法，“the” 的计分上限是2。
   - 因此，改良精确度是 2/7，因为在7个词中，我们最多只能给它2分。

$$
\text{Modified Precision} = \frac{\sum \text{Clipped Count of each word}}{\text{Total number of words in MT output}} 
$$



![image-20240729211757285](images/image-20240729211757285.png)

在BLEU评分中，我们不仅关注单词，还关注词组，特别是相邻的词对（bigrams）通过计算双词组的精确度，可以更准确地评估机器翻译的质量，考虑词的顺序和上下文。

- 参考翻译1： “The cat is on the mat.”
- 机器翻译输出： “The cat the cat on the mat.”
- 在这个例子中，一共有5种双词组的组合： “the cat”、“cat the”、“the cat”、“cat on” 和 “on the mat”。

1. **计算双词组的出现次数**：
   - 首先，数一下机器翻译输出中每个双词组的出现次数。
   - 然后，数一下这些双词组在参考翻译中出现的次数。
2. **改良双词组精确度（Modified Bigram Precision）**：
   - 计算每个双词组在参考翻译中出现的次数，并设定一个上限。
   - 例如，如果 “the cat” 在参考翻译中出现了2次，那么它的计分上限就是2。
   - 计算改良双词组精确度。

3. **例子中的计算**：
   - 在你的例子中，机器翻译输出中的双词组 “the cat” 出现了2次，但在参考翻译中最多出现2次，所以计分上限是2。
   - 其他双词组 “cat the”、“cat on” 和 “on the mat” 的计分分别是0、1和1。
   - 因此，改良双词组精确度是 4/6。

![image-20240729211800835](images/image-20240729211800835.png)

​	如果机器翻译的结果和参考翻译中的某一句完全相同，那么所有的精确度 ( P_1, P_2, …, P_n ) 都会等于1。

​	n元词组精确度：
$$
P_n = \frac{\sum \text{count}_{\text{clip}}(\text{n-gram})}{\sum \text{count}(\text{n-gram})} 
$$


![image-20240729211904091](images/image-20240729211904091.png)

- 最终的BLEU分数是所有n元词组精确度 ( P_n ) 的几何平均，然后取自然对数的指数。
- 公式表示为：

$$
\text{BLEU} = BP \cdot \exp\left(\frac{1}{N} \sum_{n=1}^{N} \log P_n\right) 
$$

**简短惩罚（Brevity Penalty, BP）**：

- 简短惩罚的作用是防止机器翻译输出非常短的句子，因为短句子容易得到高分。
- 当机器翻译的长度大于参考翻译的长度时，BP取值为1。
- 否则，BP会根据机器翻译和参考翻译的长度比值产生惩罚。
- 公式表示为：

$$
BP = \begin{cases} 
1 ; \text{if } \text{MT\_output\_length} > \text{reference\_output\_length} \\
\exp\left(1 - \frac{\text{reference\_output\_length}}{\text{MT\_output\_length}}\right) \text{otherwise}
\end{cases} 
$$

## 3.3注意力模型

​	传统的神经网络在处理长句子时，BLEU得分会随着句子的增长而快速下降。这是因为神经网络很难记住长句子的所有信息，导致翻译质量下降。人类在翻译长句子时，会先读一部分，然后翻译这一部分，直到整句翻译完毕。这种方式使得人类能够更好地处理长句子，保持翻译的连贯性和准确性。注意力机制模拟了人类的翻译方式，通过在每个时间步关注输入句子的不同部分，来处理长句子，这种机制使得模型能够在翻译过程中动态地选择重要的信息，从而提高翻译质量。注意力机制使得模型的BLEU得分不会随着句子的增长而下降。

![image-20240730190838869](images/image-20240730190838869.png)

**生成第一个词**：

- 当我们想要生成第一个词时，注意力机制会首先关注输入句子的第一个词及其周围的几个词。
- 注意力权重 ( $\alpha^{<1,1>}$ ) 表示在生成第一个词时，我们需要多少注意力放在第一个输入词上。
- 注意力机制会计算一系列注意力权重 ( $\alpha^{<1,2>}$ )，表示在生成第一个词时，我们需要多少注意力放在第二个输入词上，以此类推。
- 这些权重一起决定了我们应该放多少注意力在每个输入词上，从而形成上下文向量 ( C )。

**上下文向量的作用**：

- 上下文向量 ( $C $) 是注意力权重加权的输入特征值的加权和。
- 这个上下文向量 ( $C$ ) 是RNN网络的输入，用于生成第一个输出词。

**生成后续词**：

- 在生成第二个词时，注意力机制会计算新的注意力权重$ ( \alpha^{<2,1>} )、( \alpha^{<2,2>} )$ 等，表示在生成第二个词时应该放多少注意力在每个输入词上。
- 第一个生成的词 “Jane” 也会作为输入，结合新的上下文向量 ( C )，生成第二个词 “visits”。
- 类似地，生成第三个词时，注意力机制会计算新的注意力权重和上下文向量。

**公式化表示**：

- 上下文向量 ( C ) 的计算公式为：
  $$
  C^{t} = \sum_{t'} \alpha^{<t,t'>} a^{<t'>}
  $$
  
- 这里的 ($ \alpha^{<t,t’>}$ ) 是注意力权重， ($ a^{<t’>} $) 是输入特征值,且注意力权重非负且和为1。

![image-20240730192046018](images/image-20240730192046018.png)

​	$a^<t>$是时间步t上的特征向量（feature vector）。但是为了保持记号的一致性，我们用第二个，也就是t’，实际上我们将用t’来索引法语句子里面的词。
$$
a^{<t^{\prime}>}=(\vec{a}^{<t^{\prime}>},{a}^{ \leftarrow<t^{\prime}>})
$$
![image-20240730194225702](images/image-20240730194225702.png)

​	在神经机器翻译中，α^<t,t’> 表示在生成第 t 个输出词时，应该在第 t’ 个输入词上花费的注意力的数量。为了计算 α^<t,t’>，我们首先需要计算 e^<t,t’>，然后使用 softmax 函数来确保这些权重加起来等于 1。

​	计算 e^<t,t’> 的一种方式是使用一个小型的神经网络。具体来说，e^<t,t’> 是通过一个对齐模型（alignment model）计算的，这个模型通常是一个前馈神经网络。

### 计算 $e^{<t,t’>}$

1. **输入**：

   - $s^{t−1}$：上一个时间步的隐藏状态。
   - $a^{t^′}$：输入序列中第 $t’$ 个位置的特征。

2. **对齐模型**：

   - 对齐模型是一个小型的前馈神经网络，它接受$s^{t−1}$和$a^{t^′}$  作为输入，并输出一个标量 $e^{<t,t’>}$。

   - 公式：
     $$
     e^{<t,t’>}= \text{alignment model}(s^{t-1}, a^{t'})
     $$
     **依赖关系**：e^<t,t’> 依赖于上一个时间步的隐藏状态 st−1 和当前输入位置的特征 at′。这意味着模型在决定要花多少注意力在 t’ 位置时，会考虑上一个时间步的上下文信息。

     **训练过程**：通过反向传播和梯度下降算法，模型可以学习到一个合适的函数来计算 e^<t,t’>。这个过程是自动的，模型会根据训练数据调整参数，以优化注意力分布。

### **计算 $α^{<t,t’>}$**：

$$
\alpha^{<t,t’>} = \frac{\exp{(e^{<t,t’>})}}{\sum_{t''=1}^{T_x} \exp{(e^{<t,t’>})}}
$$

​	$T_x$ 是输入序列的长度

### 复杂度分析

- **时间复杂度**：$O(T_x * T_y)$，其中 $T_x$ 是输入序列的长度，$T_y $是输出序列的长度。
- **空间复杂度**：存储所有注意力权重也需要$ O(T_x * T_y) $的空间。

![image-20240730200017144](images/image-20240730200017144.png)

## 3.4语音识别

​	CTC（Connectionist Temporal Classification）损失函数是语音识别中的一种有效方法。专门用于处理未对齐的序列数据，如语音识别中的音频输入和文本输出。

**CTC 损失函数的基本思想**

1. **输入和输出序列长度不匹配**：
   - 语音识别中，输入序列（音频特征）的时间步通常远多于输出序列（文本）的时间步。例如，10秒的音频片段可能有1000个输入特征，但输出的文本可能只有几十个字符。
2. **允许重复和空白符**：
   - CTC 允许 RNN 生成重复的字符和特殊的空白符（blank character），用来对齐输入和输出。例如，“the quick brown fox” 可以被表示为 “t_h_e_ *q_u_i_c_k* *b_r_o_w_n* *f_o_x*”，其中下划线表示空白符。
3. **折叠规则**：
   - CTC 的一个基本规则是将空白符之间的重复字符折叠起来。例如，“t_h_e_ _q” 会被折叠成 “the q”。

![image-20240730201204043](images/image-20240730201204043.png)

## 3.5 Transformer

1. **并行处理**：
   - Transformer 能够在同一时间内处理整个句子，而不是逐词从左到右进行处理。这大大提高了计算效率。
2. **自注意力机制（Self-Attention）**：
   - 自注意力机制允许模型在处理每个词时，考虑句子中所有其他词的信息。这种机制使得模型能够捕捉到长距离的依赖关系。
3. **多头注意力机制（Multi-Head Attention）**：
   - 多头注意力机制通过并行计算多个注意力分数，捕捉到不同的语义信息。这使得模型能够更好地理解和生成复杂的句子结构。

![image-20240731190502599](images/image-20240731190502599.png)

### 自注意力机制

**词嵌入（Word Embeddings）**：

- 对于输入句子的每个单词，我们首先创建其词嵌入表示。例如，对于句子中的单词 “I’Afrique”，我们会根据上下文选择合适的词嵌入。

- 对于每个输入单词（例如 $X^{<3>}$ 表示 “I’African” 的词嵌入），我们需要计算其 Query、Key 和 Value 向量。
  $$
  q^{<i>} = W^Q x^{<i>}
  $$

  $$
  k^{<i>} = W^K x^{<i>}
  $$

  $$
  v^{<i>} = W^V x^{<i>}
  $$

  

  其中，$W^QW^KW^V$是需要学习的参数矩阵，$x^{<i>}$ 是输入单词的词嵌入。

**计算注意力权重**：

- 自注意力机制通过计算每个单词的 Query、Key 和 Value 向量来确定注意力权重。

- 公式如下：
  $$
  A(q, k, v) = \sum_{i} \frac{\exp(q \cdot k^{<i>})}{\sum_j \exp(q \cdot k^{<j>})} v^{<i>}
  $$
  

- 其中，$q$ 是 Query 向量，$k$ 是 Key 向量，$v $是 Value 向量。

- 通过**缩放点积注意力**，避免了数值爆炸问题，使得计算更加稳定。

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

​	其中，$d_k$是一个常量，$\sqrt{d_k}$这一项是用来防止点乘结果的数值过大的。它属于实现细节，对整个式子的意义不影响。抛去这一项的话，上式不过是之前那个公式的矩阵版本。这个公式在原论文里叫做Scaled Dot-Product Attention。

**上下文理解**：

- 自注意力机制允许模型在处理每个单词时，考虑句子中所有其他单词的信息。例如，对于 “I’Afrique”，模型会根据上下文决定其最合适的表达。

- **Query**：当前单词的查询信息，表示一个疑问，例如 “I’African” 的 Query 向量 q3 表示“那里发生了什么？”。

  **Key**：每个单词的键信息，用于回答 Query，例如 k1 表示“非洲发生了什么”，k2 表示“访问”。

  **Value**：每个单词的值信息，用于生成新的表示。

#### Query、Key 和 Value 

1. **Query 向量（Q）**：
   - Query 向量用于表示当前单词在句子中的查询信息。它用于与其他单词的 Key 向量进行点积运算，以计算注意力权重。
2. **Key 向量（K）**：
   - Key 向量用于表示每个单词的键信息。它与 Query 向量进行点积运算，以确定当前单词与其他单词的相关性。
3. **Value 向量（V）**：
   - Value 向量用于表示每个单词的值信息。它与计算出的注意力权重相乘，最终用于生成新的表示。

- **Query**：当前单词的查询信息，类似于在问“我应该关注哪些单词？”
- **Key**：每个单词的键信息，类似于在回答“我是否与当前单词相关？”
- **Value**：每个单词的值信息，表示实际的内容，用于生成新的表示。

![image-20240731191351796](images/image-20240731191351796.png)

![image-20240731191321693](images/image-20240731191321693.png)

### 多头注意力机制

**计算多个自注意力头**：

- 每个自注意力头（head）独立计算 Query、Key 和 Value 向量。对于每个头，使用不同的权重矩阵 $W_i^Q,W_i^K,W_i^V$。

$$
W^Q_i q^{<i>}
$$

$$
W^K_i k^{<i>}
$$

$$
W^V_i v^{<i>}
$$

**并行计算多个头**：

- 多头注意力机制并行计算多个自注意力头，每个头独立计算注意力权重和加权求和：
  $$
  Attention(W^Q_1Q, W^K_1K, W^V_1V)
  $$
  

- 其中，Attention 函数计算方式为：
  $$
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  $$

**串联和线性变换**：

- 将所有头的输出串联起来，形成一个新的向量：
  $$
  \text{MultiHead}(Q, K, V) = \text{concat}(\text{head}_1, \text{head}_2, \ldots, \text{head}_h)W^O 
  $$
  

- 其中，$W^O$ 是一个线性变换矩阵，用于将串联后的向量变换为最终的输出。

**多头注意力的优势**：

- 每个头可以关注输入序列的不同部分，捕捉到不同的语义信息。例如，一个头可能关注“发生了什么”，另一个头可能关注“什么时候发生的”。
- 并行计算多个头，提高了计算效率。

**并行计算**：

- 虽然在描述中可以将多头注意力看作是在一个大的 for 循环中计算，但实际操作中是并行计算的，因为各个头之间没有依赖关系。

![image-20240731192704728](images/image-20240731192704728.png)

### 构建TransFormer网络

#### 编码器（Encoder）

1. **多头注意力层**：
   - 输入句子被 token 化，并转换为词嵌入表示。
   - 输入的词嵌入作为 Query、Key 和 Value 向量输入到多头注意力层。
   - 多头注意力层计算多个自注意力头，每个头独立计算注意力权重和加权求和。
2. **前馈神经网络**：
   - 多头注意力层的输出传递到前馈神经网络中，进一步处理和提取特征。
   - 编码器重复 N 次（通常为 6 次）这样的操作。
3. **编码器输出**：
   - 编码器生成的输出包含了输入句子的丰富表示，这些表示将被传递到解码器中。

#### 解码器（Decoder）

1. **输入起始标识符**：
   - 解码器的第一个输入是句子的开头标识符 <SOS>（Start of Sentence）。
   - 将 <SOS> 输入到多头注意力区块，生成 Query、Key 和 Value 向量。
2. **多头注意力层**：
   - 解码器的多头注意力层有两个部分：
     - 第一部分使用解码器的前几个词生成 Query 向量。
     - 第二部分使用编码器的输出生成 Key 和 Value 向量。
3. **前馈神经网络**：
   - 多头注意力层的输出传递到前馈神经网络中，进一步处理和生成下一个单词的预测。
4. **重复操作**：
   - 解码器重复 N 次（通常为 6 次）这样的操作，每次生成一个新的单词。
   - 每次生成的新单词作为下一个时间步的输入，继续生成后续单词。

**解码器的输出**

线性层和 Softmax

- 解码器的输出通过一个线性层，然后通过 Softmax 函数预测下一个单词。
- 每次只能预测一个单词，逐步生成完整的句子。

- 例如，给定 <SOS> 和 “Jane”，解码器会预测下一个单词 “visits”。

![image-20240731193508953](images/image-20240731193508953.png)

### 优化transformer网络

#### 位置编码（Positional Encoding）

​		帮助模型识别单词在句子中的位置。由于自注意力机制本身不包含位置信息，位置编码通过正弦和余弦函数为每个单词生成一个独特的向量，从而保留单词的顺序信息。

位置编码的公式如下： 
$$
[ PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right) ] [ PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right) ]
$$

- **pos**：单词在句子中的位置。
- **i**：向量的维度索引。
- **d**：词嵌入向量的维度。

​	假设词嵌入向量的维度$ (d)$ 是 4，那么对于单词 “Jane”（位置 pos = 1），位置编码向量 $(P_1)$ 的计算如下，：

- 对于 (i = 0)：$ [ PE_{(1, 0)} = \sin\left(\frac{1}{10000^{0/4}}\right) = \sin(1) ] [ PE_{(1, 1)} = \cos\left(\frac{1}{10000^{0/4}}\right) = \cos(1) ]$
- 对于 (i = 1)： $[ PE_{(1, 2)} = \sin\left(\frac{1}{10000^{2/4}}\right) = \sin\left(\frac{1}{100}\right) ] [ PE_{(1, 3)} = \cos\left(\frac{1}{10000^{2/4}}\right) = \cos\left(\frac{1}{100}\right) ]$

​	然后填写到下图中的四维向量中。

**唯一性**：每个单词的位置编码向量都是唯一的，这确保了模型能够区分句子中不同位置的单词。

**结合词嵌入**：位置编码向量与词嵌入向量相加，使得单词的表示不仅包含其语义信息，还包含其在句子中的位置信息。

**正弦和余弦函数**：通过使用正弦和余弦函数，位置编码能够生成具有周期性和不同频率的向量，模型可以捕捉到单词之间的相对位置关系。

**位置影响**：位置编码向量直接添加到词嵌入向量上，使得单词的表示受到其在句子中位置的影响，提高翻译的准确性。

#### 残差连接（Residual Connections）

​	除了把位置编码添加到嵌入外还可以利用残差连接，将他们传递到网络中，这些残差连接与在ResNet中看到的类似，在这种情况下，我们的目的是通过整个架构传递位置信息，除了位置编码外，还使用了一个称为Add&Norm的层，类似于我们所熟悉的BatchNorm层，可以加快我们的学习速度，最后就解码器区块的输出而言实际上还存在一个线性层，和一个Softmax一起预测下一个单词，每次只能预测一个单词。

1. **目的**：
   - 残差连接的目的是在网络中传递位置信息，并帮助梯度更好地传播，从而加快模型的训练速度。
   - 这种连接方式类似于 ResNet 中的残差连接。
2. **实现**：
   - 在每个多头注意力层和前馈神经网络层之后，都会添加一个残差连接。
   - 输入直接加到输出上，然后传递到下一层。

#### Add & Norm 层

1. **功能**：
   - Add & Norm 层的作用是对残差连接后的输出进行归一化处理。
   - 这种归一化类似于 BatchNorm，可以加快模型的收敛速度。
2. **实现**：
   - 先进行加法操作（Add），将残差连接的输入和输出相加。
   - 然后进行归一化操作（Norm），通常使用 LayerNorm。

#### 掩码多头注意力机制

1. **训练过程中的掩码**：
   - 在训练过程中，模型可以访问完整的正确译文。这意味着我们可以一次性输入整个句子，而不需要逐词生成。
   - 掩码的作用是屏蔽句子的后半部分，以模拟测试时的预测过程。这样，模型只能看到已经生成的部分，而不能看到未来的单词。
2. **掩码的实现**：
   - 掩码多头注意力机制通过在计算注意力权重时，屏蔽掉未来的单词。具体来说，在计算注意力权重时，对于每个单词，只考虑它之前的单词，而不考虑它之后的单词。
   - 这种机制确保了模型在生成每个单词时，只能依赖已经生成的部分，而不能提前看到未来的单词。

- **模拟真实预测过程**：通过掩码，模型在训练过程中模拟了测试时的预测过程，提高了模型的泛化能力。
- **防止信息泄露**：掩码确保模型在生成每个单词时，只能依赖已经生成的部分，防止未来单词的信息泄露。

![image-20240731194920649](images/image-20240731194920649.png)
