# 0 Standard Attention

**标准注意力Standard Attention的两个问题：显存占用多、HBM读写次数多**	

​	transformer中注意力机制的计算过程为：

![image-20240822215157259](images/image-20240822215157259.png)



![image-20240822215207279](images/image-20240822215207279.png)

​	其中， ![image-20240822215228833](images/image-20240822215228833.png)，其中$N$是序列长度，$d$是每个注意力头的维度，输出可以记为![image-20240822215305936](images/image-20240822215305936.png)。

​	注意力机制的式子可以拆分为三步。

![image-20240822215405540](images/image-20240822215405540.png)

![image-20240822215426692](images/image-20240822215426692.png)

​	

​	在标准注意力实现中，![image-20240822215515021](images/image-20240822215515021.png)都要写回到HBM中，占用了 $O(N²)$的内存，通常![image-20240822215543528](images/image-20240822215543528.png)。

​	尽管已经有许多近似注意力的方法尝试减少attention的计算和内存要求。例如，稀疏近似和低秩近似的方法，将计算复杂度降低到了序列长度的线性或亚线性但这些近似注意力方法方法并没有得到广泛应用。因为这些方法过于关注FLOPS(浮点数计算次数)的减少，而忽略了IO读写的内存访问开销，导致这并没有效减少运行时间(wall-clock time)
​	总之，在现代GPU中，计算速度已经远超过了显存访问速度，transformer中的大部分计算操作的瓶颈是显存访问。对于显存受限的操作，IO感知是非常重要的，因为显存读写占用了大部分的运行时间,GPU的内存由多个不同大小和不同读写速度的内存组成。内存越小，读写速度越快。对于A100-40GB来说，内存分级图如下所示
![image-20240822214450417](images/image-20240822214450417.png)

- SRAM内存分布在108个流式多处理器上，每个处理器的大小为192K，合计为$192*108KB=20736KM = 20MB$即计算块，但内存小。
- 高带宽内存HBM（High Bandwidth Memory），也就是我们常说的显存，大小为40GB。SRAM的读写速度为19TB/s，而HBM的读写速度只有1.5TB/s，不到SRAM的1/10,相当于计算慢，但内存大。

即标准注意力实现存在两个问题：

​	1.显存占用多，过程中由于实例化了完整的注意力矩阵![image-20240822214806196](images/image-20240822214806196.png)，导致了![image-20240822214813139](images/image-20240822214813139.png)的内存要求。
HBM读写次数多，减慢了运行时间(wall-clock time)
​	2.接下来下文的Memory-efficient Attention、Flash Attention，便是要分别解决上述这两个问题。

# 1 Memory-efficient Attention

​	其作用就是：**把显存复杂度从平方降低到线性，但HBM访问次数仍是平方**。

​	在注意力计算过程中，节省显存的主要挑战是softmax与$K,V$的列是耦合的，其方法是**单独计算softmax的归一化因子**，来实现解耦。

​	1. 记$Q$的第$i$列为![image-20240822220157561](images/image-20240822220157561.png)，$K$的第$j$列为![image-20240822220220233](images/image-20240822220220233.png)，则有![image-20240822220227323](images/image-20240822220227323.png)。

​		定义softmax的归一化因子为：![image-20240822220250653](images/image-20240822220250653.png)。

​	2. 记![image-20240822220313917](images/image-20240822220313917.png)为V的第$j$个列向量，则输出$O$的第$i$个列向量$o_i$为：![image-20240822220403358](images/image-20240822220403358.png)

​	3. 这样在得到归一化因子$L_i$后，就可以通过反复累加![image-20240822220443694](images/image-20240822220443694.png)来获得$O_i$。

节省显存的注意力机制将显存复杂度从$O(N²)$降低到了$O(N)$。