# 训练、开发、测试集

1. **训练集 (Training Set)**:

   - **作用**：训练集用于训练机器学习模型。模型通过观察训练集中的样本来学习数据的模式、特征和关系。
   - **示例**：假设构建一个垃圾邮件分类器。训练集将包含已标记为“垃圾邮件”或“非垃圾邮件”的电子邮件样本。模型将使用这些样本来学习如何区分垃圾邮件和非垃圾邮件。

2. **验证集 (Validation Set)**:

   - **作用**：验证集用于调整模型的超参数（例如学习率、正则化参数等）。它帮助我们选择最佳的模型配置，以避免过拟合或欠拟合。
   - **示例**：在训练过程中，使用验证集来评估不同超参数设置的性能。例如，可以尝试不同的隐藏层大小、学习率等，并选择在验证集上表现最好的模型。

3. **测试集 (Test Set)**:

   - **作用**：测试集用于评估模型的泛化能力。是模型在未见过的数据上的性能指标。

   - **示例**：在训练和验证之后，使用测试集来评估模型的准确性、召回率、精确度等指标。测试集中的样本与训练集和验证集中的样本不重复，以确保模型在新数据上的表现。

# 偏差、方差

对于下方照片，第一个是高偏差的情况，中间为适度拟合，右边为高度拟合。![image-20240605200531746](images/image-20240605200531746.png)

理解偏差和方差的两个关键数据是训练集误差和验证集误差。

**高方差：**即高度拟合，训练集效果好，验证集效果差，如训练集的误差为1%而验证集的误差为11%。

**高偏差：**即训练集的错误率高，也即欠拟合，和验证集的结果却较为合理。如训练集的误差为15%而验证集的误差为16%。

**高偏差且高方差：**即训练集的错误率高的同时，验证集的结果也不合理。如训练集的误差为15%而验证集的误差为30%。

如训练集的误差为0.5%，验证集的误差为1%，则是低偏差低方差，为模型优秀结果。

**最优误差(基本误差)：**如果假设人类的识别程度的错误率为15%，那么验证集的错误率15%也为合理情况。

如果出现高偏差的情况无法拟合训练集，则选择新的网络；如果偏差适合的情况下，方差高则最好的解决办法就是采用更多数据或者正则化来减少拟合。

## 2.1-正则化

​	一般用于高方差的情况下；**正则化**（Regularization）：用于控制模型的复杂度，防止模型在训练数据上过度拟合（overfitting）。当模型过度拟合时，它会学习到训练数据中的噪声和细微变化，导致在新数据上的性能下降。

![image-20240605205134599](images/image-20240605205134599.png)

**L1 正则化**:

- **数学形式**

  : L1 正则化通过在损失函数中添加权重参数的绝对值之和作为惩罚项。这可以表示为损失函数 ![image-20240605205320014](images/image-20240605205320014.png)加上正则化项![image-20240605205344967](images/image-20240605205344967.png)

  **效果**: L1 正则化倾向于产生稀疏的权重矩阵，即许多权重会变为零。这意味着它可以用于特征选择，因为它会自动去除不重要的特征（权重为零的特征）。
  
  **L1范数**（也称为曼哈顿距离）：
  
  - **定义**：L1范数是向量中各个元素绝对值之和。
  - **作用**：L1范数倾向于产生稀疏的权重矩阵，即许多权重会变为零。因此，它可以用于特征选择。
  - **示例**：在对用户的电影爱好进行分类时，L1范数可以过滤掉无用的特征，只保留对分类有用的特征

**L2 正则化**（权重衰减）:

- **数学形式**:

   L2 正则化通过在损失函数中添加权重参数的平方和作为惩罚项。这可以表示为损失函数![image-20240605205320014](images/image-20240605205320014.png)加上正则化项![image-20240605205523676](images/image-20240605205523676.png)

  **效果**: L2 正则化会使权重值变得较小，但不会将它们推向零，这意味着它不会产生稀疏模型。L2 正则化有助于控制模型的复杂度，但不具备特征选择的功能。

  **L2范数**（弗罗贝尼乌斯范数）（也称为欧氏距离）：

  - **定义**：L2范数是向量元素平方和再开平方。
  - **作用**：L2范数不会将权重推向零，但会使权重变得较小，从而防止过拟合。
  - **示例**：在模型优化中，L2范数通常用作正则化项，以提高模型的泛化能力。

![image-20240605214643543](images/image-20240605214643543.png)

​	当λ足够大时，它会对权重矩阵施加很大的惩罚，导致权重的值减小，甚至趋于0。神经网络会越来越接近逻辑回归，但神经网络的隐藏单元依旧存在，只不过影响变小了。

​	损失函数中加入L2正则化项时，算法会尝试最小化损失函数和正则化项的总和（成本函数）。由于正则化项是权重的平方和，增加λ会使得优化算法在减少损失的同时，也倾向于将权重值减小，以降低正则化项的值。如果λ设置得非常大，那么优化算法在减少正则化项的过程中，会将权重推向。

​	当权重值减小或趋于0时，隐藏单元与输入和输出之间的连接变得非常弱。意味着隐藏单元对模型的输出贡献减少，从而减少了它们的影响力。虽然隐藏单元仍然存在，但对最终结果的影响也会变得很小，使得整个网络的行为更像是一个简单的逻辑回归分类器。

![image-20240607122958629](images/image-20240607122958629.png)

### **为什么正则化可以减少过拟合？**

​	即正则化通过添加一个与权重相关的惩罚项到损失函数中，鼓励模型学习更小的权重，而这样会使得模型倾向于学习更简单的、泛化能力更强的模式，而不是复杂的模式，这些复杂的模式可能只在训练数据上有效，但在未见过的数据上则不一定有效（过拟合）。

​	以tanh激活函数为例，当正则化参数λ增大，导致权重w减小，因为z是输入x和权重w的线性组合，所以z的值也会减小。当z的值较小时，tanh函数的输出接近线性关系（因为tanh函数在原点附近是近似线性的，如图所示）。这意味着，随着λ的增加，神经网络的非线性效应减弱，模型的复杂度降低，从而减少了过拟合的风险。

tanh函数可以表示为：
$$
\text{tanh}(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
$$
当z的值较小（即接近0）时，tanh函数可以近似为：
$$
\text{tanh}(z) \approx z
$$
![image-20240607123827795](images/image-20240607123827795.png)

## 2.2-dropout(随机失活)正则化

dropout会遍历网络的每一层，并设置消除神经网络中节点的概率，假设我们设置每一层的消除概率为0.5，即理论上有一半的隐藏单元会被消除，那么这个消除后的神经网络既是正则化后的神经网络。

​	通过在训练过程中随机地丢弃（即将权重设置为0）网络中的一部分神经元来减少过拟合。在每一层设置一个丢弃概率。如0.5，则在每次训练迭代中，每个神经元都有50%的概率不被包括在前向和后向传播中。这样，网络的每次训练迭代都是在一种略微不同的网络架构下完成的。

​	这种方法的效果类似于训练多个不同的网络，并将它们的预测结果进行平均。因为每次迭代都随机丢弃不同的神经元，所以实际上是在训练多个“子网络”。这些子网络共享权重，因此dropout可以被看作是一种非常高效的模型集成方法。

![image-20240607130819065](images/image-20240607130819065.png)

### inverted dropout（反向随机失活）

​	keep-prob是一个概率，意味为1的概率是多少，此处为0.8即80%为1，20%为0，后将a3(第三层输出)与d3相乘，然后将a3中对应于d3中为0的位置置1，也即是完成了消除。

​	在使用dropout正则化时，除以keep-prob（保留概率）是为了保证激活值的期望不变。在dropout过程中，一部分神经元会被随机丢弃，从而减少了网络的有效容量。如果不进行调整，那么在训练阶段，神经元的输出激活值的期望会因为dropout而减少。

​	例如，假设keep-prob为0.8，这意味着有20%的神经元会被丢弃。剩下的神经元的输出激活值的总和将只有原来的80%，这会影响到后续层的学习过程。我们需要将激活值除以keep-prob，这样在期望上，激活值的总和仍然保持不变。假设一个神经元的输出激活值为 ( a )，在不使用dropout的情况下，其期望值为 ( a )。如果使用了dropout，神经元的输出激活值的期望变为 ：
$$
\text{keep-prob} \times a
$$
​	为了使期望值与不使用dropout时一致，我们需要将激活值除以keep-prob，即：
$$
\frac{\text{keep-prob} \times a}{\text{keep-prob}} = a
$$
![image-20240607132538222](images/image-20240607132538222.png)

​	而在测试阶段不用使用dropout。其原因是，我们希望模型能够利用其全部学到的知识来进行预测，而不是依赖于训练时的随机性。Dropout是一种正则化技术，它通过在训练过程中随机地关闭一部分神经元来防止模型过拟合。这种随机性有助于模型学习到更加鲁棒（鲁棒性（Robustness）指的是系统在面对各种变化、干扰或不确定性时，仍能保持正常运行和正确输出结果的能力。）的特征表示，因为它不能依赖于任何特定的神经元激活模式。

​	在测试阶段，我们需要模型给出稳定的预测结果。如果在测试时也使用dropout，那么每次运行模型时都可能得到不同的结果，因为不同的神经元会被随机关闭。这会导致模型的预测性能下降，因为我们无法保证模型每次都能使用最佳的神经元组合来做出决策。由于在训练阶段使用了dropout，模型的权重已经适应了在一定比例的神经元被关闭的情况下进行预测。因此，在测试阶段，我们可以使用所有的神经元（即不使用dropout），但权重可能会乘以保留概率（keep-prob），以保持激活值的总和与训练阶段相似。这样做可以确保模型的输出反映了训练时学到的所有知识，同时保持了预测的一致性。

### 理解dropout

​	Dropout作为一种正则化技术，会随机地关闭网络中的一些单元，这样做的目的是为了防止网络对于训练数据的过度拟合。当一个单元被关闭时，它在当前训练迭代中不会对前向传播和反向传播产生影响。这迫使网络不会对任何一个输入特征赋予过多的权重，因为在训练过程中，这些特征可能会被随机删除。因此，网络必须学习更加分散的权重，以便在某些输入特征缺失时仍能做出准确的预测。

​	Dropout的效果类似于L2正则化，因为它倾向于减小权重的大小，但它们的工作原理有所不同。L2正则化通过惩罚权重的平方和来工作，导致所有权重都均匀地缩小，特别是对于那些较大的权重。而Dropout则是通过随机地关闭单元来实现正则化，这种方法不是均匀地缩小权重，而是增加了权重配置的多样性。

​	对于不同层应用不同的keep-prob（保留概率），因为不同的层可能对过拟合的敏感度不同。例如，如果某个权重矩阵特别大，可能需要更高的dropout率来防止过拟合；而对于那些我们认为不太可能过拟合的层，可以设置较高的keep-prob，甚至可以是1，意味着这些层不使用dropout。

​	Dropout主要用于计算机视觉领域，在这个领域中，模型往往非常深且复杂，容易过拟合。在其他类型的问题中，可能不需要使用dropout，或者使用其他形式的正则化就足够了。这取决于具体问题的复杂性、数据的数量和质量，以及模型的结构。

![image-20240607150642455](images/image-20240607150642455.png)

## 2.3-其他正则化的方法

通过改变图片的方式来伪增加图片集。![image-20240607151407146](images/image-20240607151407146.png)

#### Early stopping

​	Early stopping是一种防止神经网络过拟合的正则化技术。它的基本思想是在训练过程中，当模型在验证集上的性能不再提升时停止训练。这样做的目的是为了避免权重w变得过大，从而导致模型在训练数据上过度拟合。

​	在神经网络的训练过程中，权重w通常是从较小的随机值开始的。随着训练的进行，权重w会逐渐增大，以最小化训练集上的损失函数。如果训练时间过长，权重w可能会变得非常大，这时模型可能会开始学习训练数据中的噪声，而不是潜在的、更一般的模式，即过拟合现象。

Early stopping通过在验证集上监控性能来解决这个问题。当连续多个epoch性能没有提升时，就会停止训练。这通常意味着模型在验证集上的误差开始增加，表明模型可能开始过拟合。通过在这个“中间点”停止训练，我们可以得到一个大小适中的权重w，有助于保持模型的泛化能力，同时避免过拟合。

![image-20240607153026046](images/image-20240607153026046.png)
