# numpyåŸºç¡€

## 1-ä½¿ç”¨numpyæ„å»ºåŸºæœ¬å‡½æ•°

### 1.1-sigmoid functionå’Œnp.expï¼ˆï¼‰

â€‹	åœ¨ä½¿ç”¨np.expï¼ˆï¼‰ä¹‹å‰ï¼Œä½ å°†ä½¿ç”¨math.expï¼ˆï¼‰å®ç°Sigmoidå‡½æ•°ã€‚ç„¶åï¼Œä½ å°†çŸ¥é“ä¸ºä»€ä¹ˆnp.expï¼ˆï¼‰æ¯”math.expï¼ˆï¼‰æ›´å¯å–ã€‚

**ç»ƒä¹ **ï¼šæ„å»ºä¸€ä¸ªè¿”å›å®æ•°xçš„sigmoidçš„å‡½æ•°ã€‚å°†math.expï¼ˆxï¼‰ç”¨äºæŒ‡æ•°å‡½æ•°ã€‚

![image-20240523132329935](images/image-20240523132329935.png)

â€‹	ä¹Ÿç§°ä¸ºé€»è¾‘å‡½æ•°ã€‚ä¸€ç§éçº¿æ€§å‡½æ•°ï¼Œå³å¯ç”¨äºæœºå™¨å­¦ä¹ ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼Œä¹Ÿèƒ½ç”¨äºæ·±åº¦å­¦ä¹ ã€‚

â€‹	æ·±åº¦å­¦ä¹ ä¸­ä¸»è¦ä½¿ç”¨çš„æ˜¯çŸ©é˜µå’Œå‘é‡ï¼Œå› æ­¤numpyæ›´ä¸ºå®ç”¨ï¼Œå¯¹äºä½¿ç”¨mathå†™çš„sigmidå‡½æ•°ï¼Œå¦‚æœä¼ å…¥å‘é‡æˆ–è€…çŸ©é˜µä¼šæŠ¥ç±»å‹ä¸åŒ¹é…çš„é”™è¯¯ï¼Œå› æ­¤ä½¿ç”¨np.exp()ã€‚

â€‹	å¦‚æœ![image-20240523133219046](images/image-20240523133219046.png)æ˜¯è¡Œå‘é‡ï¼Œåˆ™![image-20240523133230569](images/image-20240523133230569.png)ä¼šå°†æŒ‡æ•°å‡½æ•°åº”ç”¨äºxçš„æ¯ä¸ªå…ƒç´ ã€‚è¾“å‡ºä¸ºï¼š![image-20240523133247952](images/image-20240523133247952.png)

```python
import math 
import numpy as np

# example of np.exp
x = np.array([1, 2, 3])
print(np.exp(x)) # result is (exp(1), exp(2), exp(3))
```

output:

```python
[ 2.71828183  7.3890561  20.08553692]
```

å¦‚æœxæ˜¯å‘é‡ï¼Œåˆ™ğ‘ =ğ‘¥+3æˆ–![image-20240523133848886](images/image-20240523133848886.png)ä¹‹ç±»çš„Pythonè¿ç®—å°†è¾“å‡ºä¸xç»´åº¦å¤§å°ç›¸åŒçš„å‘é‡sã€‚

```PY
x = np.array([1, 2, 3])
print (x + 3)
```

outputï¼š

```python
[4 5 6]
```

ç»¼ä¸Šï¼Œå¯¹äºnp.exp()æ‰€å®ç°çš„sigmoidå‡½æ•°ï¼Œå¯ä»¥è¾“å…¥çŸ©é˜µï¼Œå‘é‡ç­‰ï¼Œè¾“å‡ºè¾“å…¥å¦‚ä¸‹æ‰€ç¤ºã€‚

![image-20240523134217046](images/image-20240523134217046.png)

```python
def sigmoid(x):

    s = 1 / (1 + np.exp(-x))
    
    return s
```



### 1.2- Sigmoid gradient

**ç»ƒä¹ **ï¼šåˆ›å»ºå‡½æ•°sigmoid_gradï¼ˆï¼‰è®¡ç®—sigmoidå‡½æ•°ç›¸å¯¹äºå…¶è¾“å…¥xçš„æ¢¯åº¦ã€‚ å…¬å¼ä¸ºï¼š

â€‹	æ­¤å¤„å¯¹åº”äºäºŒåˆ†æ³•ï¼Œxä¸€èˆ¬ä¸ºw^T*x+bï¼Œä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ—¶å€™ä¼šä»æˆæœ¬å‡½æ•°ä¸€æ­¥æ­¥å‘å‰æ±‚åå¯¼ï¼Œä»¥æ‰¾åˆ°æˆæœ¬å‡½æ•°æœ€å°çš„å€¼ä¸ç‚¹ï¼Œåˆç†åˆ©ç”¨é“¾å¼æ³•åˆ™ä»¥æ±‚å¾—ç›¸åº”çš„å¯¼æ•°ã€‚

![image-20240523134357263](images/image-20240523134357263.png)

```python
def sigmoid_derivative(x):
    s = sigmoid(x)
    ds = s * (1-s)
    return ds

x = np.array([1, 2, 3])
print ("sigmoid_derivative(x) = " + str(sigmoid_derivative(x)))
```

output:

```PYTHON
sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]
```

### 1.3- é‡å¡‘æ•°ç»„

æ·±åº¦å­¦ä¹ ä¸­ä¸¤ä¸ªå¸¸ç”¨çš„numpyå‡½æ•°æ˜¯np.shape()å’Œnp.reshape()ã€‚
-X.shapeç”¨äºè·å–çŸ©é˜µ/å‘é‡Xçš„shapeï¼ˆç»´åº¦ï¼‰ã€‚
-X.reshapeï¼ˆ...ï¼‰ç”¨äºå°†Xé‡å¡‘ä¸ºå…¶ä»–å°ºå¯¸ã€‚

ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå›¾åƒç”±shapeä¸º(ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„,â„ğ‘’ğ‘–ğ‘”â„ğ‘¡,ğ‘‘ğ‘’ğ‘ğ‘¡â„=3)çš„3Dæ•°ç»„è¡¨ç¤ºï¼Œï¼ˆé•¿ï¼Œå®½ï¼ŒRGB=3ï¼‰ä½†æ˜¯ï¼Œå½“ä½ è¯»å–å›¾åƒä½œä¸ºç®—æ³•çš„è¾“å…¥æ—¶ï¼Œä¼šå°†å…¶è½¬æ¢ä¸ºç»´åº¦ä¸º(ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„âˆ—â„ğ‘’ğ‘–ğ‘”â„ğ‘¡âˆ—3,1)çš„å‘é‡ã€‚æ¢å¥è¯è¯´ï¼Œå°†3Dé˜µåˆ—â€œå±•å¼€â€æˆ–é‡å¡‘ä¸º1Då‘é‡ã€‚

![image-20240523135829704](images/image-20240523135829704.png)

**ç»ƒä¹ **ï¼šå®ç°`image2vector()` ,è¯¥è¾“å…¥é‡‡ç”¨ç»´åº¦ä¸º(length, height, 3)çš„è¾“å…¥ï¼Œå¹¶è¿”å›ç»´åº¦ä¸º(length\*height\*3 , 1)çš„å‘é‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³å°†å½¢ä¸ºï¼ˆaï¼Œbï¼Œcï¼‰çš„æ•°ç»„vé‡å¡‘ä¸ºç»´åº¦ä¸º(a*b, 3)çš„å‘é‡ï¼Œåˆ™å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```python
v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c
```

-è¯·ä¸è¦å°†å›¾åƒçš„å°ºå¯¸ç¡¬ç¼–ç ä¸ºå¸¸æ•°ã€‚è€Œæ˜¯é€šè¿‡image.shape [0]ç­‰æ¥æŸ¥æ‰¾æ‰€éœ€çš„æ•°é‡ã€‚

```python
def image2vector(image):

    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)

    return v



image = np.array([[[ 0.67826139,  0.29380381],
        [ 0.90714982,  0.52835647],
        [ 0.4215251 ,  0.45017551]],

       [[ 0.92814219,  0.96677647],
        [ 0.85304703,  0.52351845],
        [ 0.19981397,  0.27417313]],

       [[ 0.60659855,  0.00533165],
        [ 0.10820313,  0.49978937],
        [ 0.34144279,  0.94630077]]])

print ("image2vector(image) = " + str(image2vector(image)))
```

output:

```python
image2vector(image) = [[0.67826139]
 [0.29380381]
 [0.90714982]
 [0.52835647]
 [0.4215251 ]
 [0.45017551]
 [0.92814219]
 [0.96677647]
 [0.85304703]
 [0.52351845]
 [0.19981397]
 [0.27417313]
 [0.60659855]
 [0.00533165]
 [0.10820313]
 [0.49978937]
 [0.34144279]
 [0.94630077]]
```

### 1.4-è¡Œæ ‡å‡†åŒ–

â€‹	å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰ã€‚ ç”±äºå½’ä¸€åŒ–åæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œé€šå¸¸ä¼šè¡¨ç°å‡ºæ›´å¥½çš„æ•ˆæœã€‚ é€šè¿‡å½’ä¸€åŒ–ï¼Œä¹Ÿå°±æ˜¯å°†xæ›´æ”¹ä¸º![image-20240523141832233](images/image-20240523141832233.png)ï¼ˆå°†xçš„æ¯ä¸ªè¡Œå‘é‡é™¤ä»¥å…¶èŒƒæ•°ï¼ˆæ¨¡å€¼ï¼‰)ã€‚

ä¾‹å¦‚ï¼š

![image-20240523141947128](images/image-20240523141947128.png)

then

![image-20240523141954325](images/image-20240523141954325.png)

and

![image-20240523141958558](images/image-20240523141958558.png)

**ç»ƒä¹ **ï¼šæ‰§è¡Œ normalizeRowsï¼ˆï¼‰æ¥æ ‡å‡†åŒ–çŸ©é˜µçš„è¡Œã€‚ å°†æ­¤å‡½æ•°åº”ç”¨äºè¾“å…¥çŸ©é˜µxä¹‹åï¼Œxçš„æ¯ä¸€è¡Œåº”ä¸ºå•ä½é•¿åº¦ï¼ˆå³é•¿åº¦ä¸º1ï¼‰å‘é‡ã€‚

```python
#linalg = linearï¼ˆçº¿æ€§ï¼‰+ algebraï¼ˆä»£æ•°ï¼‰ï¼Œnormåˆ™è¡¨ç¤ºèŒƒæ•°ã€‚
#x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False)
```

- x: è¡¨ç¤ºçŸ©é˜µï¼ˆä¹Ÿå¯ä»¥æ˜¯ä¸€ç»´ï¼‰

- ordï¼šèŒƒæ•°ç±»å‹

  ![image-20240523142909611](images/image-20240523142909611.png)

- axis, axis=0 è¡¨ç¤ºæŒ‰åˆ—å‘é‡æ¥è¿›è¡Œå¤„ç†ï¼Œæ±‚å¤šä¸ªåˆ—å‘é‡çš„èŒƒæ•°; axis =1 è¡¨ç¤ºæŒ‰è¡Œå‘é‡æ¥è¿›è¡Œå¤„ç†ï¼Œæ±‚å¤šä¸ªè¡Œå‘é‡çš„èŒƒæ•°

- keepdimsï¼šè¡¨ç¤ºæ˜¯å¦ä¿æŒçŸ©é˜µçš„äºŒç»´ç‰¹æ€§ï¼ŒTrueè¡¨ç¤ºä¿æŒï¼ŒFalseè¡¨ç¤ºä¸ä¿æŒï¼Œé»˜è®¤ä¸ºFalse

```python
def normalizeRows(x):
    x_norm = np.linalg.norm(x, axis = 1, keepdims = True)
    # Divide x by its norm.
    x = x / x_norm
    ### END CODE HERE ###

    return x

x = np.array([
    [0, 3, 4],
    [1, 6, 4]])
print("normalizeRows(x) = " + str(normalizeRows(x)))
```

outputï¼š

```PYTHON
normalizeRows(x) = [[0.         0.6        0.8       ]
 [0.13736056 0.82416338 0.54944226]]
```

**æ³¨æ„**ï¼š
åœ¨normalizeRowsï¼ˆï¼‰ä¸­ï¼Œå°è¯•printæŸ¥çœ‹ x_normå’Œxçš„ç»´åº¦ï¼Œ ä¼šå‘ç°å®ƒä»¬å…·æœ‰ä¸åŒçš„wç»´åº¦ã€‚ é‰´äºx_normé‡‡ç”¨xçš„æ¯ä¸€è¡Œçš„èŒƒæ•°ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚ å› æ­¤ï¼Œx_normå…·æœ‰ç›¸åŒçš„è¡Œæ•°ï¼Œä½†åªæœ‰1åˆ—ã€‚ é‚£ä¹ˆï¼Œå½“ä½ å°†xé™¤ä»¥x_normæ—¶ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ è¿™å°±æ˜¯æ‰€è°“çš„å¹¿æ’­broadcastingï¼Œæˆ‘ä»¬ç°åœ¨å°†è®¨è®ºå®ƒï¼

æ­¤å¤„å¯¹åº”çš„æ˜¯ç¬”è®°ä¸­çš„å¹¿æ’­ï¼Œé€šä¿—æ¥è®²å°±æ˜¯ä¼šæ™ºèƒ½åŒ–è§£å†³ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜ã€‚

### 1.5- å¹¿æ’­å’Œsoftmaxå‡½æ•°

åœ¨numpyä¸­è¦ç†è§£çš„ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µæ˜¯â€œå¹¿æ’­â€ã€‚ è¿™å¯¹äºåœ¨ä¸åŒå½¢çŠ¶çš„æ•°ç»„ä¹‹é—´æ‰§è¡Œæ•°å­¦è¿ç®—éå¸¸æœ‰ç”¨ã€‚

**ç»ƒä¹ **: ä½¿ç”¨numpyå®ç°softmaxå‡½æ•°ã€‚ ä½ å¯ä»¥å°†softmaxç†è§£ä¸ºç®—æ³•éœ€è¦å¯¹ä¸¤ä¸ªæˆ–å¤šä¸ªç±»è¿›è¡Œåˆ†ç±»æ—¶ä½¿ç”¨çš„æ ‡å‡†åŒ–å‡½æ•°ã€‚

â€‹	ç®€è€Œè¨€ä¹‹ï¼Œè¾“å…¥ä¸€ä¸ªçŸ©é˜µï¼Œå–eæŒ‡æ•°åæ¯è¡Œè¿›è¡Œå•ä½åŒ–ï¼Œå¯¹äºè¿™ä¸ªå‡½æ•°æœ‰ä»€ä¹ˆä½œç”¨å°†åœ¨æœ¬ä¸“ä¸šçš„ç¬¬äºŒé—¨è¯¾ä¸­äº†è§£æœ‰å…³softmaxçš„æ›´å¤šä¿¡æ¯ã€‚

![image-20240523144022058](images/image-20240523144022058.png)

```python
def softmax(x):

    x_exp = np.exp(x)

    x_sum = np.sum(x_exp, axis = 1, keepdims = True)

    s = x_exp / x_sum
    
    return s


x = np.array([
    [9, 2, 5, 0, 0],
    [7, 5, 0, 0 ,0]])
print("softmax(x) = " + str(softmax(x)))
```

output:

```PYTHON
softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04
  1.21052389e-04]
 [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04
  8.01252314e-04]]
```

## 2-å‘é‡åŒ–

â€‹	åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸éœ€è¦å¤„ç†éå¸¸å¤§çš„æ•°æ®é›†ã€‚ å› æ­¤ï¼Œéè®¡ç®—æœ€ä½³å‡½æ•°å¯èƒ½ä¼šæˆä¸ºç®—æ³•ä¸­çš„å·¨å¤§ç“¶é¢ˆï¼Œå¹¶å¯èƒ½ä½¿æ¨¡å‹è¿è¡Œä¸€æ®µæ—¶é—´ã€‚ ä¸ºäº†ç¡®ä¿ä»£ç çš„é«˜æ•ˆè®¡ç®—ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‘é‡åŒ–ã€‚ ä¾‹å¦‚ï¼Œå°è¯•åŒºåˆ†ç‚¹/å¤–éƒ¨/å…ƒç´ ä¹˜ç§¯ä¹‹é—´çš„åŒºåˆ«ã€‚

â€‹	ä¸‹æ–¹è¿ç®—æ—¶é—´ä¸º0ï¼Œdicã€docç›¸åŒï¼Œç”µè„‘è¿ç®—å¤ªå¿«ï¼Œä»£ç å¤ªå°‘ï¼Œformatå‡½æ•°ç²¾ç¡®åˆ°åå…«ä½éƒ½æ˜¯0ã€‚

â€‹	ä½†å¤§è¿ç®—é‡é¿å…ä½¿ç”¨æ˜¾ç¤ºforï¼Œä½¿ç”¨npå‡½æ•°åº“ã€‚

### **ç‚¹ç§¯(dot product)**

åˆå«æ ‡é‡ç§¯ã€æ•°é‡ç§¯ã€‚

â€‹	![image-20240523163946065](images/image-20240523163946065.png)

forç‰ˆæœ¬ï¼š

```python
import time

x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###
tic = time.process_time()
dot = 0
for i in range(len(x1)):
    dot+= x1[i]*x2[i]
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

npç‰ˆæœ¬ï¼š

```PYTHON
x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]
x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]

### VECTORIZED DOT PRODUCT OF VECTORS ###
tic = time.process_time()
dot = np.dot(x1,x2)
toc = time.process_time()
print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```



output:

```PYTHON
dot = 278
 ----- Computation time = 0.0ms
```



### **å‰ç§¯(corss product)**

â€‹	æˆ–å‘é‡ç§¯(vector product )

![image-20240523164123958](images/image-20240523164123958.png)

### **å¤–ç§¯(outer product)**

åœ¨çº¿æ€§ä»£æ•°ä¸­ï¼Œä¸¤ä¸ªåæ ‡å‘é‡çš„å¤–ç§¯æ˜¯ä¸€ä¸ªçŸ©é˜µã€‚å¦‚æœè¿™ä¸¤ä¸ªå‘é‡çš„ç»´åº¦æ˜¯*n*å’Œ*m*ï¼Œé‚£ä¹ˆå®ƒä»¬çš„å¤–ç§¯æ˜¯ä¸€ä¸ª*n* Ã— *m*çŸ©é˜µã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œç»™å®šä¸¤ä¸ªå¼ é‡ï¼ˆå¤šç»´æ•°å­—æ•°ç»„ï¼‰ï¼Œå®ƒä»¬çš„å¤–ç§¯æ˜¯å¼ é‡ã€‚å¼ é‡çš„å¤–ç§¯ä¹Ÿç§°ä¸ºå¼ é‡ç§¯ï¼Œå¯ç”¨äºå®šä¹‰å¼ é‡ä»£æ•°ã€‚

![image-20240523164405819](images/image-20240523164405819.png)

forç‰ˆæœ¬:

```PYTHON
### CLASSIC OUTER PRODUCT IMPLEMENTATION ###
tic = time.process_time()
outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros
for i in range(len(x1)):
    for j in range(len(x2)):
        outer[i,j] = x1[i]*x2[j]
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```

npç‰ˆæœ¬ï¼š

```PYTHON
### VECTORIZED OUTER PRODUCT ###
tic = time.process_time()
outer = np.outer(x1,x2)
toc = time.process_time()
print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")
```



output:

```PYTHON
outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
 ----- Computation time = 0.0ms
```

â€‹	ä¸åŒäº`np.multiply()`å’Œ`*` æ“ä½œç¬¦ï¼ˆç›¸å½“äºMatlab / Octaveä¸­çš„ `.*`ï¼‰æ‰§è¡Œé€å…ƒç´ çš„ä¹˜æ³•ï¼Œ`np.dot()`æ‰§è¡Œçš„æ˜¯çŸ©é˜µ-çŸ©é˜µæˆ–çŸ©é˜µå‘é‡ä¹˜æ³•ï¼Œ

### 2.1-å®ç°L1å’ŒL2æŸå¤±å‡½æ•°

**ç»ƒä¹ **ï¼šå®ç°L1æŸå¤±å‡½æ•°çš„Numpyå‘é‡åŒ–ç‰ˆæœ¬ã€‚ æˆ‘ä»¬ä¼šå‘ç°å‡½æ•°absï¼ˆxï¼‰ï¼ˆxçš„ç»å¯¹å€¼ï¼‰å¾ˆæœ‰ç”¨ã€‚

**æç¤º**ï¼š
-æŸå¤±å‡½æ•°ç”¨äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ æŸå¤±è¶Šå¤§ï¼Œé¢„æµ‹![image-20240523170837643](images/image-20240523170837643.png) ä¸çœŸå®å€¼![image-20240523170844510](images/image-20240523170844510.png)çš„å·®å¼‚ä¹Ÿå°±è¶Šå¤§ã€‚ åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¸å¦‚Gradient Descentä¹‹ç±»çš„ä¼˜åŒ–ç®—æ³•æ¥è®­ç»ƒæ¨¡å‹å¹¶æœ€å¤§ç¨‹åº¦åœ°é™ä½æˆæœ¬ã€‚

- L1æŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š

![image-20240523170721050](images/image-20240523170721050.png)

```PYTHON
def L1(yhat, y):
    loss = np.sum(np.abs(y - yhat))
    return loss

yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L1 = " + str(L1(yhat,y)))
```

output:

```python
L1 = 1.1
```

**ç»ƒä¹ **ï¼šå®ç°L2æŸå¤±å‡½æ•°çš„Numpyå‘é‡åŒ–ç‰ˆæœ¬ã€‚ æœ‰å¥½å‡ ç§æ–¹æ³•å¯ä»¥å®ç°L2æŸå¤±å‡½æ•°ï¼Œä½†æ˜¯è¿˜æ˜¯np.dotï¼ˆï¼‰å‡½æ•°æ›´å¥½ç”¨ã€‚ æé†’ä¸€ä¸‹ï¼Œå¦‚æœğ‘¥=[ğ‘¥1,ğ‘¥2,...,ğ‘¥ğ‘›]ï¼Œåˆ™`np.dotï¼ˆxï¼Œxï¼‰`=![image-20240523171230965](images/image-20240523171230965.png)

- L2æŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š

![image-20240523171322543](images/image-20240523171322543.png)

æ­¤ç§ç®—æ³•æ˜¯ä¸Šè¿°æåˆ°çš„çŸ©é˜µçš„ç‚¹ç§¯ç®—æ³•ã€‚

```python
def L2(yhat, y):
    loss = np.dot((y - yhat),(y - yhat).T)
    return loss


yhat = np.array([.9, 0.2, 0.1, .4, .9])
y = np.array([1, 0, 0, 1, 1])
print("L2 = " + str(L2(yhat,y)))
```

output:

```python
L2 = 0.43
```

# ç”¨ç¥ç»ç½‘ç»œæ€æƒ³å®ç°Logisticå›å½’

## 1- å®‰è£…åŒ…

**ä½ å°†å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š**

- å»ºç«‹å­¦ä¹ ç®—æ³•çš„ä¸€èˆ¬æ¶æ„ï¼ŒåŒ…æ‹¬ï¼š
  - åˆå§‹åŒ–å‚æ•°
  - è®¡ç®—æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦
  - ä½¿ç”¨ä¼˜åŒ–ç®—æ³•ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
- æŒ‰æ­£ç¡®çš„é¡ºåºå°†ä»¥ä¸Šæ‰€æœ‰ä¸‰ä¸ªåŠŸèƒ½é›†æˆåˆ°ä¸€ä¸ªä¸»æ¨¡å‹ä¸Šã€‚

```python
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from lr_utils import load_dataset #è‡ªå»ºåº“ï¼Œä¸å¤§å¾ˆå°ï¼Œç½‘ä¸Šå¯æ‰¾åˆ°ã€‚
```

## 2- æ•°æ®é›†

**é—®é¢˜è¯´æ˜**ï¼šä½ å°†è·å¾—ä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„æ•°æ®é›†ï¼ˆ"data.h5"ï¼‰ï¼š

-    æ ‡è®°ä¸ºcatï¼ˆy = 1ï¼‰æˆ–écatï¼ˆy = 0ï¼‰çš„**m_train**è®­ç»ƒå›¾åƒé›†
-    æ ‡è®°ä¸ºcatæˆ–non-catçš„**m_test**æµ‹è¯•å›¾åƒé›†
-    å›¾åƒç»´åº¦ä¸ºï¼ˆnum_pxï¼Œnum_pxï¼Œ3ï¼‰ï¼Œå…¶ä¸­3è¡¨ç¤º3ä¸ªé€šé“ï¼ˆRGBï¼‰ã€‚ å› æ­¤ï¼Œæ¯ä¸ªå›¾åƒéƒ½æ˜¯æ­£æ–¹å½¢ï¼ˆé«˜åº¦= num_pxï¼‰å’Œï¼ˆå®½åº¦= num_pxï¼‰ã€‚

ä½ å°†æ„å»ºä¸€ä¸ªç®€å•çš„å›¾åƒè¯†åˆ«ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥å°†å›¾ç‰‡æ­£ç¡®åˆ†ç±»ä¸ºçŒ«å’ŒéçŒ«ã€‚
è®©æˆ‘ä»¬ç†Ÿæ‚‰ä¸€ä¸‹æ•°æ®é›†å§ï¼Œ é¦–å…ˆé€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç æ¥åŠ è½½æ•°æ®ã€‚

```python
# Loading the data (cat/non-cat)
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

print("train_set_x shape: " + str(train_set_x_orig.shape))
print("train_set_y shape: " + str(train_set_y.shape))
print("test_set_x shape: " + str(test_set_x_orig.shape))
print("test_set_y shape: " + str(test_set_y.shape))
```

æˆ‘ä»¬åœ¨å›¾åƒæ•°æ®é›†ï¼ˆè®­ç»ƒå’Œæµ‹è¯•ï¼‰çš„æœ«å°¾æ·»åŠ äº†"_orig"ï¼Œä»¥ä¾¿å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ã€‚ é¢„å¤„ç†åï¼Œæˆ‘ä»¬å°†å¾—åˆ°train_set_xå’Œtest_set_xï¼ˆæ ‡ç­¾train_set_yå’Œtest_set_yä¸éœ€è¦ä»»ä½•é¢„å¤„ç†ï¼‰ã€‚

- **train_set_x_orig** ï¼šä¿å­˜çš„æ˜¯è®­ç»ƒé›†é‡Œé¢çš„å›¾åƒæ•°æ®ï¼ˆæœ¬è®­ç»ƒé›†æœ‰209å¼ 64x64çš„å›¾åƒï¼‰ã€‚
- **train_set_y_orig** ï¼šä¿å­˜çš„æ˜¯è®­ç»ƒé›†çš„å›¾åƒå¯¹åº”çš„åˆ†ç±»å€¼ï¼ˆã€0 | 1ã€‘ï¼Œ0è¡¨ç¤ºä¸æ˜¯çŒ«ï¼Œ1è¡¨ç¤ºæ˜¯çŒ«ï¼‰ã€‚
- **test_set_x_orig** ï¼šä¿å­˜çš„æ˜¯æµ‹è¯•é›†é‡Œé¢çš„å›¾åƒæ•°æ®ï¼ˆæœ¬è®­ç»ƒé›†æœ‰50å¼ 64x64çš„å›¾åƒï¼‰ã€‚
- **test_set_y_orig** ï¼š ä¿å­˜çš„æ˜¯æµ‹è¯•é›†çš„å›¾åƒå¯¹åº”çš„åˆ†ç±»å€¼ï¼ˆã€0 | 1ã€‘ï¼Œ0è¡¨ç¤ºä¸æ˜¯çŒ«ï¼Œ1è¡¨ç¤ºæ˜¯çŒ«ï¼‰ã€‚
- **classes** ï¼š ä¿å­˜çš„æ˜¯ä»¥bytesç±»å‹ä¿å­˜çš„ä¸¤ä¸ªå­—ç¬¦ä¸²æ•°æ®ï¼Œæ•°æ®ä¸ºï¼š[bâ€™non-catâ€™ bâ€™catâ€™]ã€‚

train_set_x_origå’Œtest_set_x_origçš„æ¯ä¸€è¡Œéƒ½æ˜¯ä»£è¡¨å›¾åƒçš„æ•°ç»„ã€‚ ä½ å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç æ¥å¯è§†åŒ–ç¤ºä¾‹ã€‚ è¿˜å¯ä»¥éšæ„æ›´æ”¹`index`å€¼å¹¶é‡æ–°è¿è¡Œä»¥æŸ¥çœ‹å…¶ä»–å›¾åƒã€‚

```python
# Example of a picture
index = 5
plt.imshow(train_set_x_orig[index])
plt.show()  #ä½¿ç”¨vscodeæ—¶ï¼Œæ·»åŠ è¿™ä¸ªæ‰ä¼šæ˜¾ç¤ºï¼Œå¹¶éåœ¨ipythonnoteä¸­è¿è¡Œã€‚
print ("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") +  "' picture.")
```

outputï¼š

```python
y = [0], it's a 'non-cat' picture.
```

![image-20240523175255830](images/image-20240523175255830.png)

æ·±åº¦å­¦ä¹ ä¸­çš„è®¸å¤šæŠ¥é”™éƒ½æ¥è‡ªäºçŸ©é˜µ/å‘é‡å°ºå¯¸ä¸åŒ¹é…ã€‚ å¦‚æœä½ å¯ä»¥ä¿æŒçŸ©é˜µ/å‘é‡çš„å°ºå¯¸ä¸å˜ï¼Œé‚£ä¹ˆå°†æ¶ˆé™¤å¤§å¤šé”™è¯¯ã€‚

**ç»ƒä¹ ï¼š** æŸ¥æ‰¾ä»¥ä¸‹å„é¡¹çš„å€¼ï¼š

-    m_trainï¼ˆè®­ç»ƒé›†ç¤ºä¾‹æ•°é‡ï¼‰
-    m_testï¼ˆæµ‹è¯•é›†ç¤ºä¾‹æ•°é‡ï¼‰
-    num_pxï¼ˆ=è®­ç»ƒå›¾åƒçš„é«˜åº¦=è®­ç»ƒå›¾åƒçš„å®½åº¦ï¼‰

â€œ train_set_x_origâ€æ˜¯ä¸€ä¸ªç»´åº¦ä¸ºï¼ˆm_trainï¼Œnum_pxï¼Œnum_pxï¼Œ3ï¼‰çš„numpyæ•°ç»„ã€‚

```python
### START CODE HERE ### (â‰ˆ 3 lines of code)
m_train = train_set_x_orig.shape[0]
m_test = test_set_x_orig.shape[0]
num_px = train_set_x_orig.shape[1]
### END CODE HERE ###

print ("Number of training examples: m_train = " + str(m_train))
print ("Number of testing examples: m_test = " + str(m_test))
print ("Height/Width of each image: num_px = " + str(num_px))
print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
print ("train_set_x shape: " + str(train_set_x_orig.shape))
print ("train_set_y shape: " + str(train_set_y.shape))
print ("test_set_x shape: " + str(test_set_x_orig.shape))
print ("test_set_y shape: " + str(test_set_y.shape))
```

outputï¼š

```python
Number of training examples: m_train = 209
Number of testing examples: m_test = 50
Height/Width of each image: num_px = 64
Each image is of size: (64, 64, 3)
train_set_x shape: (209, 64, 64, 3)
train_set_y shape: (1, 209)
test_set_x shape: (50, 64, 64, 3)
test_set_y shape: (1, 50)
```

ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œä½ ç°åœ¨åº”è¯¥ä»¥ç»´åº¦(num_px âˆ— num_px âˆ— 3, 1)çš„numpyæ•°ç»„é‡å¡‘ç»´åº¦ï¼ˆnum_pxï¼Œnum_pxï¼Œ3ï¼‰çš„å›¾åƒã€‚ æ­¤åï¼Œæˆ‘ä»¬çš„è®­ç»ƒï¼ˆå’Œæµ‹è¯•ï¼‰æ•°æ®é›†æ˜¯ä¸€ä¸ªnumpyæ•°ç»„ï¼Œå…¶ä¸­æ¯åˆ—ä»£è¡¨ä¸€ä¸ªå±•å¹³çš„å›¾åƒã€‚ åº”è¯¥æœ‰m_trainï¼ˆå’Œm_testï¼‰åˆ—ã€‚

## 3-é¢„å¤„ç†æ•°æ®é›†

**ç»ƒä¹ ï¼š** é‡å¡‘è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ï¼Œä»¥ä¾¿å°†å¤§å°ï¼ˆnum_pxï¼Œnum_pxï¼Œ3ï¼‰çš„å›¾åƒå±•å¹³ä¸ºå•ä¸ªå½¢çŠ¶çš„å‘é‡(num_px âˆ— num_px âˆ— 3, 1)ã€‚

å°†ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼Œcï¼Œdï¼‰çš„çŸ©é˜µXå±•å¹³ä¸ºå½¢çŠ¶ä¸º(bâˆ—câˆ—d, a)çš„çŸ©é˜µX_flattenæ—¶çš„ä¸€ä¸ªæŠ€å·§æ˜¯ï¼š

â€‹	åœ¨æŒ‡å®šç›®æ ‡ shape æ—¶å­˜åœ¨ä¸€äº›æŠ€å·§ï¼š

> - 1. -1 è¡¨ç¤ºè¿™ä¸ªç»´åº¦çš„å€¼æ˜¯ä» x çš„å…ƒç´ æ€»æ•°å’Œå‰©ä½™ç»´åº¦æ¨æ–­å‡ºæ¥çš„ã€‚å› æ­¤ï¼Œæœ‰ä¸”åªæœ‰ä¸€ä¸ªç»´åº¦å¯ä»¥è¢«è®¾ç½®ä¸º-1,åˆ©ç”¨å˜æ¢å‰åçš„å…ƒç´ æ•°å€¼ç›¸ç­‰æ¨æµ‹å‡ºæ¥ã€‚
> - 1. 0 è¡¨ç¤ºå®é™…çš„ç»´æ•°æ˜¯ä» x çš„å¯¹åº”ç»´æ•°ä¸­å¤åˆ¶å‡ºæ¥çš„ï¼Œå› æ­¤ shape ä¸­ 0 çš„ç´¢å¼•å€¼ä¸èƒ½è¶…è¿‡ x çš„ç»´åº¦ï¼Œç›´æ¥å¤åˆ¶å³å¯ã€‚

è¿™é‡Œæœ‰ä¸€äº›ä¾‹å­æ¥è§£é‡Šå®ƒä»¬ï¼š

> - 1ã€‚ç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º[2,4,6]çš„ä¸‰ç»´ Tensor xï¼Œç›®æ ‡å½¢çŠ¶ä¸º[6,8]ï¼Œåˆ™å°† x å˜æ¢ä¸ºå½¢çŠ¶ä¸º[6,8]çš„ 2-D Tensorï¼Œä¸” x çš„æ•°æ®ä¿æŒä¸å˜ã€‚
> - 2ã€‚ç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º[2,4,6]çš„ä¸‰ç»´ Tensor xï¼Œç›®æ ‡å½¢çŠ¶ä¸º[2,3,-1,2]ï¼Œåˆ™å°† x å˜æ¢ä¸ºå½¢çŠ¶ä¸º[2,3,4,2]çš„ 4-D Tensorï¼Œä¸” x çš„æ•°æ®ä¿æŒä¸å˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡å½¢çŠ¶çš„ä¸€ä¸ªç»´åº¦è¢«è®¾ç½®ä¸º-1ï¼Œè¿™ä¸ªç»´åº¦çš„å€¼æ˜¯ä» x çš„å…ƒç´ æ€»æ•°å’Œå‰©ä½™ç»´åº¦æ¨æ–­å‡ºæ¥çš„ã€‚
> - 3ã€‚ç»™å®šä¸€ä¸ªå½¢çŠ¶ä¸º[2,4,6]çš„ä¸‰ç»´ Tensor xï¼Œç›®æ ‡å½¢çŠ¶ä¸º[-1,0,3,2]ï¼Œåˆ™å°† x å˜æ¢ä¸ºå½¢çŠ¶ä¸º[2,4,3,2]çš„ 4-D Tensorï¼Œä¸” x çš„æ•°æ®ä¿æŒä¸å˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ0 å¯¹åº”ä½ç½®çš„ç»´åº¦å€¼å°†ä» x çš„å¯¹åº”ç»´æ•°ä¸­å¤åˆ¶ï¼Œ-1 å¯¹åº”ä½ç½®çš„ç»´åº¦å€¼ç”± x çš„å…ƒç´ æ€»æ•°å’Œå‰©ä½™ç»´åº¦æ¨æ–­å‡ºæ¥ã€‚

```python
X_flatten = X.reshape(X.shape [0]ï¼Œ-1).T     # å…¶ä¸­X.Tæ˜¯Xçš„è½¬ç½®çŸ©é˜µ
```

å¤šæ•°æœºå™¨å­¦ä¹ ä»¥è¡Œä¸ºç‰¹å¾åˆ—ä¸ºæ ·æœ¬æ•°ã€‚

```python
# Reshape the training and test examples

### START CODE HERE ### (â‰ˆ 2 lines of code)
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T
### END CODE HERE ###

print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))
print ("train_set_y shape: " + str(train_set_y.shape))
print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))
print ("test_set_y shape: " + str(test_set_y.shape))
print ("sanity check after reshaping: " + str(train_set_x_flatten[0:5,0]))
```

outputï¼š

```python
train_set_x_flatten shape: (12288, 209)
train_set_y shape: (1, 209)
test_set_x_flatten shape: (12288, 50)
test_set_y shape: (1, 50)
sanity check after reshaping: [17 31 56 22 33] #â€œç†æ™ºæ£€æŸ¥â€ï¼ˆsanity checkï¼‰ï¼Œç”¨äºéªŒè¯æ•°æ®æ˜¯å¦æŒ‰é¢„æœŸè¿›è¡Œäº†é‡å¡‘ã€‚ é€šè¿‡æ‰“å°æ•°ç»„ä¸€å°éƒ¨åˆ†ï¼Œé€šè¿‡è§†è§‰æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸï¼Œé€‰å–å‰äº”ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªç‰¹å¾å€¼ã€‚
```

æœºå™¨å­¦ä¹ ä¸­ä¸€ä¸ªå¸¸è§çš„é¢„å¤„ç†æ­¥éª¤æ˜¯å¯¹æ•°æ®é›†è¿›è¡Œå±…ä¸­å’Œæ ‡å‡†åŒ–ï¼Œè¿™æ„å‘³ç€ä½ è¦ä»æ¯ä¸ªç¤ºä¾‹ä¸­å‡å»æ•´ä¸ªnumpyæ•°ç»„çš„å‡å€¼ï¼Œç„¶åé™¤ä»¥æ•´ä¸ªnumpyæ•°ç»„çš„æ ‡å‡†å·®ã€‚ä½†æ˜¯å›¾ç‰‡æ•°æ®é›†åˆ™æ›´ä¸ºç®€å•æ–¹ä¾¿ï¼Œå¹¶ä¸”åªè¦å°†æ•°æ®é›†çš„æ¯ä¸€è¡Œé™¤ä»¥255ï¼ˆåƒç´ é€šé“çš„æœ€å¤§å€¼ï¼‰ï¼Œæ•ˆæœä¹Ÿå·®ä¸å¤šã€‚

åœ¨è®­ç»ƒæ¨¡å‹æœŸé—´ï¼Œå°†è¦ä¹˜ä»¥æƒé‡å¹¶å‘ä¸€äº›åˆå§‹è¾“å…¥æ·»åŠ åå·®ä»¥è§‚å¯Ÿç¥ç»å…ƒçš„æ¿€æ´»ã€‚ç„¶åï¼Œä½¿ç”¨åå‘æ¢¯åº¦ä¼ æ’­ä»¥è®­ç»ƒæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè®©ç‰¹å¾å…·æœ‰ç›¸ä¼¼çš„èŒƒå›´ä»¥è‡³æ¸å˜ä¸ä¼šçˆ†ç‚¸æ˜¯éå¸¸é‡è¦çš„ï¼ˆåç»­ç« èŠ‚ï¼‰ã€‚

```python
train_set_x = train_set_x_flatten/255.
test_set_x = test_set_x_flatten/255.
```

é¢„å¤„ç†æ•°æ®é›†çš„å¸¸è§æ­¥éª¤æ˜¯ï¼š

- æ‰¾å‡ºæ•°æ®çš„å°ºå¯¸å’Œç»´åº¦ï¼ˆm_trainï¼Œm_testï¼Œnum_pxç­‰ï¼‰
- é‡å¡‘æ•°æ®é›†ï¼Œä»¥ä½¿æ¯ä¸ªç¤ºä¾‹éƒ½æ˜¯å¤§å°ä¸ºï¼ˆnum_px \*num_px \* 3ï¼Œ1ï¼‰çš„å‘é‡
- â€œæ ‡å‡†åŒ–â€æ•°æ®

## 4-ç®—æ³•çš„ä¸€èˆ¬æ¡†æ¶

ä½¿ç”¨ç¥ç»ç½‘ç»œæ€ç»´æ–¹å¼å»ºç«‹Logisticå›å½’ã€‚ ä¸‹å›¾è¯´æ˜äº†ä¸ºä»€ä¹ˆâ€œé€»è¾‘å›å½’å®é™…ä¸Šæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œï¼â€

![image-20240523183605947](images/image-20240523183605947.png)

**ç®—æ³•çš„æ•°å­¦è¡¨è¾¾å¼**ï¼š

â€‹	å‰æ–¹éƒ½æ˜¯åœ¨é¢„æµ‹ï¼Œä¼¼ä¹åªæœ‰æŸå¤±å‡½æ•°å’Œæˆæœ¬å‡½æ•°æ˜¯ä¸è®­ç»ƒæœ‰å…³ç³»ï¼Œæˆæœ¬å‡½æ•°æœ€ç»ˆæ˜¯è¦æ±‚åå¯¼æˆwå’Œbçš„å‡½æ•°ï¼Œå› æ­¤å°±æ˜¯é€šè¿‡æˆæœ¬å‡½æ•°å’ŒæŸå¤±å‡½æ•°æ¥æ‰¾å¯»ä½¿å¾—Jæœ€å°çš„wå’Œbå‡½æ•°ã€‚

For one example![image-20240523183922055](images/image-20240523183922055.png)ï¼š

![image-20240523183932138](images/image-20240523183932138.png)

The cost is then computed by summing over all training examples:

![image-20240523184026470](images/image-20240523184026470.png)

**å…³é”®æ­¥éª¤**ï¼š

-    åˆå§‹åŒ–æ¨¡å‹å‚æ•°
-    é€šè¿‡æœ€å°åŒ–æŸå¤±æ¥å­¦ä¹ æ¨¡å‹çš„å‚æ•°
-    ä½¿ç”¨å­¦ä¹ åˆ°çš„å‚æ•°è¿›è¡Œé¢„æµ‹ï¼ˆåœ¨æµ‹è¯•é›†ä¸Šï¼‰
-    åˆ†æç»“æœå¹¶å¾—å‡ºç»“è®º

## 5-æ„å»ºç®—æ³•

å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸»è¦æ­¥éª¤æ˜¯ï¼š
1.å®šä¹‰æ¨¡å‹ç»“æ„ï¼ˆä¾‹å¦‚è¾“å…¥ç‰¹å¾çš„æ•°é‡ï¼‰
2.åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
3.å¾ªç¯ï¼š

-    è®¡ç®—å½“å‰æŸå¤±ï¼ˆæ­£å‘ä¼ æ’­ï¼‰
-    è®¡ç®—å½“å‰æ¢¯åº¦ï¼ˆå‘åä¼ æ’­ï¼‰
-    æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

æ„å»º1-3ï¼Œé›†æˆåˆ°ä¸€ä¸ªç§°ä¸ºâ€œ modelï¼ˆï¼‰â€çš„å‡½æ•°ä¸­ã€‚

### 5.1- sigmoidå‡½æ•°(è¾…åŠ©å‡½æ•°)

```python
def sigmoid(z):
    s = 1 / (1 + np.exp(-z))
    return s
```

### 5.2- åˆå§‹åŒ–å‚æ•°

**ç»ƒä¹ ï¼š** å®ç°å‚æ•°åˆå§‹åŒ–ã€‚ ä½ å¿…é¡»å°†wåˆå§‹åŒ–ä¸ºé›¶çš„å‘é‡ä½¿ç”¨np.zerosï¼ˆï¼‰ã€‚

```python
def initialize_with_zeros(dim):
    w = np.zeros((dim, 1))
    b = 0

    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
    
    return w, b

dim = 2
w, b = initialize_with_zeros(dim)
print ("w = " + str(w))
print ("b = " + str(b))
```

outputï¼š

```PYTHON
w = [[0.]
 [0.]]
b = 0
```

### 5.3 å‰å‘å’Œåå‘ä¼ æ’­å‡½æ•°

**ç»ƒä¹ ï¼š** å®ç°å‡½æ•°propagateï¼ˆï¼‰æ¥è®¡ç®—æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚

æ­£å‘ä¼ æ’­ï¼š

- å¾—åˆ°X
- è®¡ç®—![image-20240523190723683](images/image-20240523190723683.png)
- è®¡ç®—æŸå¤±å‡½æ•°ï¼š![image-20240523190732001](images/image-20240523190732001.png)

ä½¿ç”¨åˆ°ä»¥ä¸‹ä¸¤ä¸ªå…¬å¼ï¼šå¯ä»ç¬”è®°ä¸­ä¸€æ­¥æ­¥æ¨å‡ºæ¥ã€‚

![image-20240523190754111](images/image-20240523190754111.png)

```python
def propagate(w, b, X, Y):
    m = X.shape[1]
    A = sigmoid(np.dot(w.T, X) + b)         
    cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
    dw = 1 / m * np.dot(X, (A - Y).T)
    db = 1 / m * np.sum(A - Y)
    assert(dw.shape == w.shape)
    assert(db.dtype == float)
    cost = np.squeeze(cost) #ä»æ•°ç»„çš„å½¢çŠ¶ä¸­ç§»é™¤å•ç»´åº¦çš„æ¡ç›®ã€‚å¦‚ä¸€äº›æ•°ç»„ä¸º[2,2ï¼Œ1]ï¼Œå…¶å®è´¨å°±ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œä½†è¿™æ ·å†™ä¼šè¡¨ç°ä¸ºä¸‰ç»´æ•°ç»„ï¼Œå› æ­¤å¯ä»¥ç”¨squeezeå‡½æ•°å°†å…¶å˜ä¸º[2,2]
    assert(cost.shape == ())
    
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost

w, b, X, Y = np.array([[1],[2]]), 2, np.array([[1,2],[3,4]]), np.array([[1,0]])
#w, b, X, Y = np.array([[1],[2],[3]]), 2, np.array([[1,2,3],[3,4,3],[5,6,3]]), np.array([[1,0,1]])
grads, cost = propagate(w, b, X, Y)
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
print ("cost = " + str(cost))
```

outputï¼š

```python
dw = [[0.99993216]
 [1.99980262]]
db = 0.49993523062470574
cost = 6.000064773192205
```

### 5.4-ä¼˜åŒ–å‡½æ•°

- åˆå§‹åŒ–å‚æ•°ã€‚
- è®¡ç®—æŸå¤±å‡½æ•°åŠå…¶æ¢¯åº¦ã€‚
- ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°å‚æ•°ã€‚

**ç»ƒä¹ ï¼š** é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•° ğ½ æ¥å­¦ä¹  ğ‘¤ å’Œ ğ‘ã€‚ å¯¹äºå‚æ•°ğœƒï¼Œæ›´æ–°è§„åˆ™ä¸ºğœƒ=ğœƒâˆ’ğ›¼ ğ‘‘ğœƒï¼Œå…¶ä¸­ğ›¼æ˜¯å­¦ä¹ ç‡ã€‚

```python
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    
    costs = []
    
    for i in range(num_iterations): #è¿­ä»£æ¬¡æ•°
        

        grads, cost = propagate(w, b, X, Y)


        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

        if i % 100 == 0:#è®°å½•æ¯éš”ä¸€å®šæ­¥éª¤ï¼ˆæ¯ 100 æ­¥ï¼‰çš„æŸå¤±å€¼ã€‚
            costs.append(cost)#å°†æŸå¤±å€¼ï¼ˆcostï¼‰æ·»åŠ åˆ°åä¸ºcostsçš„åˆ—è¡¨ä¸­ã€‚

        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
    
    params = {"w": w,
              "b": b}
    
    grads = {"dw": dw,
             "db": db}
    
    return params, grads, costs

params, grads, costs = optimize(w, b, X, Y, num_iterations= 101, learning_rate = 0.009, print_cost = False)

print ("w = " + str(params["w"]))
print ("b = " + str(params["b"]))
print ("dw = " + str(grads["dw"]))
print ("db = " + str(grads["db"]))
print(costs)
```

outputï¼š

```PYTHON
w = [[0.10440664]
 [0.21535171]]
b = 1.5554725342883116
dw = [[0.89458411]
 [1.74622645]]
db = 0.4258211729530607
[6.000064773192205, 1.4313999565615696]
```

**ç»ƒä¹ ï¼š** ä¸Šä¸€ä¸ªå‡½æ•°å°†è¾“å‡ºå­¦ä¹ åˆ°çš„wå’Œbã€‚ æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨wå’Œbæ¥é¢„æµ‹æ•°æ®é›†Xçš„æ ‡ç­¾ã€‚å®ç°`predictï¼ˆï¼‰`å‡½æ•°ã€‚ é¢„æµ‹åˆ†ç±»æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼š
1.è®¡ç®—![image-20240523193405123](images/image-20240523193405123.png)
2.å°†açš„é¡¹è½¬æ¢ä¸º0ï¼ˆå¦‚æœæ¿€æ´»<= 0.5ï¼‰æˆ–1ï¼ˆå¦‚æœæ¿€æ´»> 0.5)ï¼Œå¹¶å°†é¢„æµ‹ç»“æœå­˜å‚¨åœ¨å‘é‡â€œ Y_predictionâ€ä¸­ã€‚ å¦‚æœæ„¿æ„ï¼Œå¯ä»¥åœ¨forå¾ªç¯ä¸­ä½¿ç”¨if / elseè¯­å¥ã€‚

```python
def predict(w, b, X):
    #æ­¤å¤„çš„Xå·²ç»æ˜¯è¢«é¢„å¤„ç†è¿‡åçš„ï¼Œå³å¤§å°ä¸º(X.shape[0],X.shape[1])è€Œä¸å†æ˜¯RGBä¸‰ç»´æ•°ç»„ã€‚
    m = X.shape[1]  #è·å–æ ·æœ¬æ•°
    Y_prediction = np.zeros((1,m)) #1è¡Œmåˆ—
    w = w.reshape(X.shape[0], 1)#wä¸ºxå¯¹åº”çš„è¡Œï¼Œ1åˆ—ï¼ˆåç»­è¦è½¬ç½®ï¼‰
    A = sigmoid(np.dot(w.T, X) + b)#y_hat

    for i in range(A.shape[1]):
        if A[0, i] <= 0.5:
            Y_prediction[0, i] = 0 #ä¸æ˜¯çŒ«
        else:
            Y_prediction[0, i] = 1 #çŒ«
    
    assert(Y_prediction.shape == (1, m)) 
    
    return Y_prediction

print ("predictions = " + str(predict(w, b, X)))
```

outputï¼š

```python
predictions = [[1. 1.]] #ç»“æœè¡¨ç¤ºæµ‹è¯•äº†ä¸¤å¼ å›¾ç‰‡ï¼Œéƒ½ä¸ºçŒ«ã€‚
```

## 6- å°†æ‰€æœ‰åŠŸèƒ½åˆå¹¶åˆ°æ¨¡å‹ä¸­

ç°åœ¨ï¼Œå°†æ‰€æœ‰æ„ä»¶ï¼ˆåœ¨ä¸Šä¸€éƒ¨åˆ†ä¸­å®ç°çš„åŠŸèƒ½ï¼‰ä»¥æ­£ç¡®çš„é¡ºåºæ”¾åœ¨ä¸€èµ·ï¼Œä»è€Œå¾—åˆ°æ•´ä½“çš„model()å‡½æ•°ã€‚

**ç»ƒä¹ ï¼š** å®ç°å‡½æ•°åŠŸèƒ½ï¼Œä½¿ç”¨ä»¥ä¸‹ç¬¦å·ï¼š

-    Y_predictionå¯¹æµ‹è¯•é›†çš„é¢„æµ‹
-    Y_prediction_trainå¯¹è®­ç»ƒé›†çš„é¢„æµ‹
-    wï¼ŒæŸå¤±ï¼Œoptimizeï¼ˆï¼‰è¾“å‡ºçš„æ¢¯åº¦

```python
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):

    w, b = initialize_with_zeros(X_train.shape[0]) #å¯¹wå’Œbè¿›è¡Œä»¥Xè¡Œä¸ºæ•°çš„åˆå§‹åŒ–[dimï¼Œ1]

    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    w = parameters["w"]
    b = parameters["b"]#è®­ç»ƒåçš„

    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)

    print("train accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) #100-ç»å¯¹è¯¯å·®ï¼ˆæ­¤å¤„è¯¯å·®å½’ä¸€åŒ–äº†ï¼Œå› æ­¤ä¹˜ä»¥100ï¼‰
    print("test accuracy: {} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    
    d = {"costs": costs,
         "Y_prediction_test": Y_prediction_test, 
         "Y_prediction_train" : Y_prediction_train, 
         "w" : w, #wä»0é€šè¿‡æ¢¯åº¦æ¥è®¡ç®—æƒé‡çŸ©é˜µï¼Œdwçš„å€¼é€šè¿‡ç…§ç‰‡è®¡ç®—ä¸ä¸€æ ·ã€‚
         "b" : b,
         "learning_rate" : learning_rate,
         "num_iterations": num_iterations}
    
    return d

```

â€‹	é€šè¿‡è®­ç»ƒé›†å¾—åˆ°æ¯å¼ ç…§ç‰‡dwï¼Œç„¶åè®¡ç®—æŸå¤±å‡½æ•°ï¼Œè¿›è€Œå¾—åˆ°æˆæœ¬å‡½æ•°ï¼Œdwçš„ä¸åŒå¯¼è‡´äº†æŸå¤±å‡½æ•°ä¸åŒï¼Œä»è€Œä½¿å¾—æ›´æ–°çš„wæƒé‡çŸ©é˜µä¸åŒã€‚

## 7- å­¦ä¹ ç‡çš„é€‰æ‹©

ä¸ºäº†ä½¿æ¢¯åº¦ä¸‹é™èµ·ä½œç”¨ï¼Œä½ å¿…é¡»æ˜æ™ºåœ°é€‰æ‹©å­¦ä¹ ç‡ã€‚ å­¦ä¹ ç‡ğ›¼å†³å®šæˆ‘ä»¬æ›´æ–°å‚æ•°çš„é€Ÿåº¦ã€‚ å¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šâ€œè¶…å‡ºâ€æœ€ä½³å€¼ã€‚ åŒæ ·ï¼Œå¦‚æœå¤ªå°ï¼Œå°†éœ€è¦æ›´å¤šçš„è¿­ä»£æ‰èƒ½æ”¶æ•›åˆ°æœ€ä½³å€¼ã€‚ è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆè°ƒæ•´å¥½å­¦ä¹ ç‡è‡³å…³é‡è¦ã€‚

è®©æˆ‘ä»¬å°†æ¨¡å‹çš„å­¦ä¹ æ›²çº¿ä¸é€‰æ‹©çš„å‡ ç§å­¦ä¹ ç‡è¿›è¡Œæ¯”è¾ƒã€‚ è¿è¡Œä¸‹é¢çš„å•å…ƒæ ¼ã€‚ è¿™å¤§çº¦éœ€è¦1åˆ†é’Ÿã€‚ è¿˜å¯ä»¥å°è¯•ä¸æˆ‘ä»¬åˆå§‹åŒ–è¦åŒ…å«çš„â€œ learning_ratesâ€å˜é‡çš„ä¸‰ä¸ªå€¼ä¸åŒçš„å€¼ï¼Œç„¶åçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

```python
learning_rates = [0.01, 0.001, 0.0001]
models = {}
for i in learning_rates:
    print ("learning rate is: " + str(i))
    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)
    print ('\n' + "-------------------------------------------------------" + '\n')

for i in learning_rates:
    plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]))

plt.ylabel('cost')
plt.xlabel('iterations')

legend = plt.legend(loc='upper center', shadow=True)
frame = legend.get_frame()
frame.set_facecolor('0.90')
plt.show()
```

outputï¼š

```python
learning rate is: 0.01
train accuracy: 99.52153110047847 %
test accuracy: 68.0 %

-------------------------------------------------------

learning rate is: 0.001
train accuracy: 88.99521531100478 %
test accuracy: 64.0 %

-------------------------------------------------------

learning rate is: 0.0001
train accuracy: 68.42105263157895 %
test accuracy: 36.0 %

-------------------------------------------------------
```

![image-20240523212116574](images/image-20240523212116574.png)

## 8-ä½¿ç”¨è‡ªå·±çš„å›¾åƒè¿›è¡Œæµ‹è¯•

ç¥è´ºä½ å®Œæˆæ­¤ä½œä¸šã€‚ ä½ å¯ä»¥ä½¿ç”¨è‡ªå·±çš„å›¾åƒå¹¶æŸ¥çœ‹æ¨¡å‹çš„é¢„æµ‹è¾“å‡ºã€‚ è¦åšåˆ°è¿™ä¸€ç‚¹ï¼š
   1.å•å‡»æ­¤ç¬”è®°æœ¬ä¸Šéƒ¨æ ä¸­çš„ "File"ï¼Œç„¶åå•å‡»"Open" ä»¥åœ¨Coursera Hubä¸Šè¿è¡Œã€‚
   2.å°†å›¾åƒæ·»åŠ åˆ°Jupyter Notebookçš„ç›®å½•ä¸­ï¼Œåœ¨"images"æ–‡ä»¶å¤¹ä¸­
   3.åœ¨ä»¥ä¸‹ä»£ç ä¸­æ›´æ”¹å›¾åƒçš„åç§°
   4.è¿è¡Œä»£ç ï¼Œæ£€æŸ¥ç®—æ³•æ˜¯å¦æ­£ç¡®ï¼ˆ1 = catï¼Œ0 = non-catï¼‰ï¼

```python
fname = '/home/kesci/input/deeplearningai17761/cat_in_iran.jpg'#æ–‡ä»¶è·¯å¾„
image = np.array(plt.imread(fname))
my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T
my_predicted_image = predict(d["w"], d["b"], my_image)

plt.imshow(image)
print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")
```

# å•éšå±‚çš„ç¥ç»ç½‘ç»œåˆ†ç±»äºŒç»´æ•°æ®

## 1-å®‰è£…åŒ…

- [numpy](https://www.heywhale.com/api/notebooks/5e85d6bf95b029002ca7e7e6/www.numpy.org)æ˜¯Pythonç§‘å­¦è®¡ç®—çš„åŸºæœ¬åŒ…ã€‚

- [sklearn](http://scikit-learn.org/stable/)æä¾›äº†ç”¨äºæ•°æ®æŒ–æ˜å’Œåˆ†æçš„ç®€å•æœ‰æ•ˆçš„å·¥å…·ã€‚

- [matplotlib](http://matplotlib.org/) æ˜¯åœ¨Pythonä¸­å¸¸ç”¨çš„ç»˜åˆ¶å›¾å½¢çš„åº“ã€‚

- testCasesæä¾›äº†ä¸€äº›æµ‹è¯•ç¤ºä¾‹ç”¨ä»¥è¯„ä¼°å‡½æ•°çš„æ­£ç¡®æ€§

- planar_utilsæä¾›äº†æ­¤ä½œä¸šä¸­ä½¿ç”¨çš„å„ç§å‡½æ•°

  å¯¼å…¥ä»¥ä¸‹ä¾èµ–åº“ã€‚

```python
# Package imports
import numpy as np
import matplotlib.pyplot as plt
from testCases import *
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

%matplotlib inline

np.random.seed(1) # set a seed so that the results are consistent
```

## 2-æ•°æ®é›†

`plt.scatter` æ˜¯ Matplotlib åº“ä¸­ç”¨äºç»˜åˆ¶æ•£ç‚¹å›¾çš„å‡½æ•°ã€‚å®ƒçš„è¯­æ³•å’Œå‚æ•°å¦‚ä¸‹ï¼š

```python
plt.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, edgecolors=None, plotnonfinite=False, data=None, **kwargs)
```

å°†â€œflowerâ€ 2åˆ†ç±»æ•°æ®é›†åŠ è½½åˆ°å˜é‡ `X` å’Œ `Y`ä¸­ã€‚

  \- åŒ…å«ç‰¹å¾ï¼ˆx1ï¼Œx2ï¼‰çš„numpyæ•°ç»„ï¼ˆçŸ©é˜µï¼‰X
  \- åŒ…å«æ ‡ç­¾ï¼ˆçº¢è‰²ï¼š0ï¼Œè“è‰²ï¼š1ï¼‰çš„numpyæ•°ç»„ï¼ˆå‘é‡ï¼‰Yã€‚

è¿™é‡Œçš„Xæ˜¯[2,N]çŸ©é˜µï¼Œç¬¬ä¸€è¡Œä¸ºæ¨ªåæ ‡ï¼Œç¬¬äºŒè¡Œä¸ºçºµåæ ‡ã€‚

```python
X, Y = load_planar_dataset() 
```

ä½¿ç”¨matplotlibå¯è§†åŒ–æ•°æ®é›†ã€‚ æ•°æ®çœ‹èµ·æ¥åƒæ˜¯å¸¦æœ‰ä¸€äº›çº¢è‰²ï¼ˆæ ‡ç­¾y = 0ï¼‰å’Œä¸€äº›è“è‰²ï¼ˆy = 1ï¼‰ç‚¹çš„â€œèŠ±â€ã€‚ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªé€‚åˆè¯¥æ•°æ®çš„åˆ†ç±»æ¨¡å‹ã€‚

- `X[0, :]` å’Œ `X[1, :]`ï¼šè¿™ä¸¤ä¸ªæ˜¯æ•£ç‚¹å›¾ä¸­ç‚¹çš„æ¨ªçºµåæ ‡ã€‚`X[0, :]` æ˜¯æ‰€æœ‰ç‚¹çš„æ¨ªåæ ‡é›†åˆï¼Œ`X[1, :]` æ˜¯æ‰€æœ‰ç‚¹çš„çºµåæ ‡é›†åˆã€‚
- `c=Y.reshape(X[0,:].shape)`ï¼šè¿™é‡Œçš„ `c` å‚æ•°ä»£è¡¨é¢œè‰²ã€‚`Y` æ˜¯ä¸€ä¸ªé¢œè‰²æ ‡ç­¾æ•°ç»„ï¼Œé€šè¿‡ `reshape` æ–¹æ³•å°†å…¶å½¢çŠ¶è°ƒæ•´ä¸ºä¸ `X[0,:]` ç›¸åŒï¼Œè¿™æ ·æ¯ä¸ªç‚¹éƒ½ä¼šæ ¹æ® `Y` ä¸­çš„æ ‡ç­¾è¢«ç€è‰²ã€‚
- `s=40`ï¼šè¿™æ˜¯è®¾ç½®æ•£ç‚¹çš„å¤§å°ï¼Œè¿™é‡Œè®¾ç½®ä¸º40ã€‚
- `cmap=plt.cm.Spectral`ï¼š`cmap` å‚æ•°æŒ‡å®šäº†ä¸€ä¸ªé¢œè‰²æ˜ å°„ï¼Œ`plt.cm.Spectral` æ˜¯ä¸€ä¸ªé¢œè‰²æ˜ å°„çš„åç§°ï¼Œå®ƒåŒ…å«äº†ä»çº¢è‰²åˆ°è“è‰²çš„ä¸€ç³»åˆ—é¢œè‰²ï¼Œç”¨äºæ ¹æ® `c` å‚æ•°ç»™ç‚¹ç€è‰²ã€‚

```python
# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y.reshape(X[0,:].shape), s=40, cmap=plt.cm.Spectral)
```

outputï¼š

<img src="images/image-20240528193307428.png" alt="image-20240528193307428" style="zoom:50%;" />

äº†è§£ä¸€ä¸‹æˆ‘ä»¬çš„æ•°æ®ã€‚

```PYTHON
### START CODE HERE ### (â‰ˆ 3 lines of code)
shape_X = X.shape
shape_Y = Y.shape

m = shape_X[1]  # training set size
### END CODE HERE ###

print ('The shape of X is: ' + str(shape_X))
print ('The shape of Y is: ' + str(shape_Y))
print ('I have m = %d training examples!' % (m))
```

outputï¼š

```PYTHON
The shape of X is: (2, 400)
The shape of Y is: (1, 400)
I have m = 400 training examples!
```

## 3-ç®€å•Logisticå›å½’

â€‹	åœ¨æ„å»ºå®Œæ•´çš„ç¥ç»ç½‘ç»œä¹‹å‰ï¼Œé¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹é€»è¾‘å›å½’åœ¨æ­¤é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚ ä½ å¯ä»¥ä½¿ç”¨sklearnçš„å†…ç½®å‡½æ•°æ¥æ‰§è¡Œæ­¤æ“ä½œã€‚ è¿è¡Œä»¥ä¸‹ä»£ç ä»¥åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨ã€‚

```PYTHON
# Train the logistic regression classifier
clf = sklearn.linear_model.LogisticRegressionCV();
clf.fit(X.T, Y.T);
```

è¿è¡Œä¸‹é¢çš„ä»£ç ä»¥ç»˜åˆ¶æ­¤æ¨¡å‹çš„å†³ç­–è¾¹ç•Œï¼š

```PYTHON
# Plot the decision boundary for logistic regression
plot_decision_boundary(lambda x: clf.predict(x), X, Y)
plt.title("Logistic Regression")

# Print accuracy
LR_predictions = clf.predict(X.T)
print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +
       '% ' + "(percentage of correctly labelled datapoints)")
```

outputï¼š

```PYTHON
Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)
```



<img src="images/image-20240528195612442.png" alt="image-20240528195612442" style="zoom: 67%;" />

â€‹	å¯ä»¥çœ‹åˆ°åˆ†ç±»çš„æ•ˆæœå¹¶ä¸å¥½ï¼Œè¿™æ˜¯ç”±äºæ•°æ®é›†ä¸æ˜¯çº¿æ€§å¯åˆ†ç±»çš„ï¼Œå› æ­¤é€»è¾‘å›å½’æ•ˆæœä¸ä½³ã€‚

## 4-ç¥ç»ç½‘ç»œ

<img src="https://cdn.kesci.com/upload/image/q17ipqoyrg.png?imageView2/0/w/960/h/960" alt="Image Name" style="zoom:80%;" />

**æ•°å­¦åŸç†**ï¼Œa<sup>[1]</sup><sup>(1)</sup><sub>1</sub>(ä¸Šæ ‡ï¼š[ç¬¬ä¸€å±‚]ï¼ˆç¬¬ä¸€ä¸ªæ ·æœ¬ï¼‰ä¸‹æ ‡ï¼šç¬¬ä¸€ä¸ªéšè—å±‚ç¥ç»å…ƒ)ï¼›å…¶ä½™ä»¥æ­¤ç±»æ¨ã€‚
$$
z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}
$$

$$
a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}
$$

$$
z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}
$$

$$
\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}
$$

$$
y^{(i)}_{prediction} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}\tag{5}
$$

æŸå¤±å‡½æ•°ğ½:
$$
J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}
$$
å»ºç«‹ç¥ç»ç½‘ç»œçš„ä¸€èˆ¬æ–¹æ³•æ˜¯ï¼š
1.å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„ï¼ˆè¾“å…¥å•å…ƒæ•°ï¼Œéšè—å•å…ƒæ•°ç­‰ï¼‰ã€‚
2.åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
3.å¾ªç¯ï¼š

- å®ç°å‰å‘ä¼ æ’­
- è®¡ç®—æŸå¤±
- åå‘ä¼ æ’­ä»¥è·å¾—æ¢¯åº¦
- æ›´æ–°å‚æ•°ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰

é€šå¸¸ä¼šæ„å»ºè¾…åŠ©å‡½æ•°æ¥è®¡ç®—ç¬¬1-3æ­¥ï¼Œç„¶åå°†å®ƒä»¬åˆå¹¶ä¸º`nn_model()`å‡½æ•°ã€‚ä¸€æ—¦æ„å»ºäº†`nn_model()`å¹¶å­¦ä¹ äº†æ­£ç¡®çš„å‚æ•°ï¼Œå°±å¯ä»¥å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚

### 4.1 å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„

**ç»ƒä¹ **ï¼šå®šä¹‰ä¸‰ä¸ªå˜é‡ï¼š
   \- n_xï¼šè¾“å…¥å±‚çš„å¤§å°
   \- n_hï¼šéšè—å±‚çš„å¤§å°ï¼ˆå°†å…¶è®¾ç½®ä¸º4ï¼‰
   \- n_yï¼šè¾“å‡ºå±‚çš„å¤§å°

**æç¤º**ï¼šä½¿ç”¨shapeæ¥æ‰¾åˆ°n_xå’Œn_yã€‚ å¦å¤–ï¼Œå°†éšè—å±‚å¤§å°ç¡¬ç¼–ç ä¸º4ã€‚

```python
# GRADED FUNCTION: layer_sizes
# ä¸€èˆ¬X,Yä¼šé¢„å¤„ç†ä¸ºä¸€ä¸ª(N,1)çŸ©é˜µã€‚
def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """
    ### START CODE HERE ### (â‰ˆ 3 lines of code)
    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    ### END CODE HERE ###
    return (n_x, n_h, n_y)

X_assess, Y_assess = layer_sizes_test_case()
(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess)
print("The size of the input layer is: n_x = " + str(n_x))
print("The size of the hidden layer is: n_h = " + str(n_h))
print("The size of the output layer is: n_y = " + str(n_y))
```

outputï¼š

```PYTHON
The size of the input layer is: n_x = 5
The size of the hidden layer is: n_h = 4
The size of the output layer is: n_y = 2
```

### 4.2éšæœºåˆå§‹åŒ–å‚æ•°

â€‹	å¦‚ç¬”è®°ä¸­æ‰€è¯´ï¼Œå•éšå±‚ç¥ç»ç½‘ç»œéœ€è¦éšæœºåˆå§‹åŒ–å‚æ•°ï¼Œé¿å…è®­ç»ƒæ— æ•ˆæœã€‚

**ç»ƒä¹ **ï¼šå®ç°å‡½æ•° `initialize_parameters()`ã€‚

**è¯´æ˜**ï¼š

- è¯·ç¡®ä¿å‚æ•°å¤§å°æ­£ç¡®ã€‚ å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯å‚è€ƒä¸Šé¢çš„ç¥ç»ç½‘ç»œå›¾ã€‚
- ä½¿ç”¨éšæœºå€¼åˆå§‹åŒ–æƒé‡çŸ©é˜µã€‚
     \- ä½¿ç”¨ï¼š`np.random.randnï¼ˆaï¼Œbï¼‰* 0.01`éšæœºåˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰çš„çŸ©é˜µã€‚
- å°†åå·®å‘é‡åˆå§‹åŒ–ä¸ºé›¶ã€‚
     \- ä½¿ç”¨ï¼š`np.zeros((a,b))` åˆå§‹åŒ–ç»´åº¦ä¸ºï¼ˆaï¼Œbï¼‰é›¶çš„çŸ©é˜µã€‚

```python
# GRADED FUNCTION: initialize_parameters

def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """
    
    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.
    
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = np.random.randn(n_h,n_x) * 0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y,n_h) * 0.01
    b2 = np.zeros((n_y,1))
    ### END CODE HERE ###
    #ç¬”è®°ä¸­æœ‰è®°è½½ï¼ŒW1çŸ©é˜µæ˜¯ä¸€ä¸ªæ˜¯ä¸€ä¸ª[n_h,n_x]çš„çŸ©é˜µ,è¿™æ˜¯å‘é‡åŒ–åçš„è®¡ç®—å¼ï¼Œå®é™…å„åˆ†å¼å­åº”å‚ç…§ç¬”è®°ä¸­å¯ä»¥ç›´è§‚çœ‹å‡ºã€‚
    assert (W1.shape == (n_h, n_x))#ä¸Xï¼ˆå‘é‡åŒ–åï¼‰ç›¸ä¹˜åï¼Œå‡ºç°ä¸€ä¸ª[n_h,1]çš„çŸ©é˜µã€‚
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters


n_x, n_h, n_y = initialize_parameters_test_case()

parameters = initialize_parameters(n_x, n_h, n_y)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```python
W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
```

### 4.3å¾ªç¯

**é—®é¢˜**ï¼šå®ç°`forward_propagationï¼ˆï¼‰`ã€‚

**è¯´æ˜**ï¼š

- åœ¨ä¸Šæ–¹æŸ¥çœ‹åˆ†ç±»å™¨çš„æ•°å­¦è¡¨ç¤ºå½¢å¼ã€‚
- ä½ å¯ä»¥ä½¿ç”¨å†…ç½®åœ¨ç¬”è®°æœ¬ä¸­çš„`sigmoid()`å‡½æ•°ã€‚
- ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨numpyåº“ä¸­çš„`np.tanhï¼ˆï¼‰`å‡½æ•°ã€‚
- å¿…é¡»æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
     1.ä½¿ç”¨`parameters [â€œ ..â€]`ä»å­—å…¸â€œ parametersâ€ï¼ˆè¿™æ˜¯`initialize_parametersï¼ˆï¼‰`çš„è¾“å‡ºï¼‰ä¸­æ£€ç´¢å‡ºæ¯ä¸ªå‚æ•°ã€‚
     2.å®ç°æ­£å‘ä¼ æ’­ï¼Œè®¡ç®—ğ‘[1],ğ´[1],ğ‘[2] å’Œ ğ´[2] ï¼ˆæ‰€æœ‰è®­ç»ƒæ•°æ®çš„é¢„æµ‹ç»“æœå‘é‡ï¼‰ã€‚
- å‘åä¼ æ’­æ‰€éœ€çš„å€¼å­˜å‚¨åœ¨`cache`ä¸­ï¼Œ `cache`å°†ä½œä¸ºåå‘ä¼ æ’­å‡½æ•°çš„è¾“å…¥ã€‚

```python
# GRADED FUNCTION: forward_propagation

def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- The sigmoid output of the second activation
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"
    """
    # Retrieve each parameter from the dictionary "parameters"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    ### END CODE HERE ###
    
    # Implement Forward Propagation to calculate A2 (probabilities)
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    ### END CODE HERE ###
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache

X_assess, parameters = forward_propagation_test_case()

A2, cache = forward_propagation(X_assess, parameters)

# Note: we use the mean here just to make sure that your output matches ours. 
print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2']))
```

outputï¼š

```PYTHON
-0.0004997557777419913 -0.000496963353231779 0.00043818745095914653 0.500109546852431
```

ç°åœ¨ï¼Œä½ å·²ç»è®¡ç®—äº†åŒ…å«æ¯ä¸ªç¤ºä¾‹çš„![image-20240528204452259](images/image-20240528204452259.png)çš„![image-20240528204457680](images/image-20240528204457680.png)ï¼ˆåœ¨Pythonå˜é‡â€œ`A2`â€ä¸­)ï¼Œå…¶ä¸­ï¼Œä½ å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š
$$
J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}
$$
**ç»ƒä¹ **ï¼šå®ç°`compute_costï¼ˆï¼‰`ä»¥è®¡ç®—æŸå¤±ğ½çš„å€¼ã€‚

**è¯´æ˜**ï¼š

- æœ‰å¾ˆå¤šç§æ–¹æ³•å¯ä»¥å®ç°äº¤å‰ç†µæŸå¤±ã€‚ æˆ‘ä»¬ä¸ºä½ æä¾›äº†å®ç°æ–¹æ³•ï¼š
  $$
  - \sum\limits_{i=0}^{m}  y^{(i)}\log(a^{[2](i)})
  $$

  ```python
  logprobs = np.multiply(np.log(A2),Y)  
  cost = - np.sum(logprobs)                # no need to use a for loop!
  ```

ï¼ˆä½ ä¹Ÿå¯ä»¥ä½¿ç”¨np.multiply()ç„¶åä½¿ç”¨np.sum()æˆ–ç›´æ¥ä½¿ç”¨np.dot()ï¼‰ã€‚

```python
# GRADED FUNCTION: compute_cost

def compute_cost(A2, Y, parameters):
    """
    Computes the cross-entropy cost given in equation (13)
    
    Arguments:
    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    parameters -- python dictionary containing your parameters W1, b1, W2 and b2
    
    Returns:
    cost -- cross-entropy cost given equation (13)
    """
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
     ### START CODE HERE ### (â‰ˆ 2 lines of code)
    logprobs = Y*np.log(A2) + (1-Y)* np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. 
                                # E.g., turns [[17]] into 17 
    assert(isinstance(cost, float))
    
    return cost

A2, Y_assess, parameters = compute_cost_test_case()

print("cost = " + str(compute_cost(A2, Y_assess, parameters)))
```

outputï¼š

```PYTHON
cost = 0.6929198937761265
```

ç°åœ¨ï¼Œé€šè¿‡ä½¿ç”¨åœ¨æ­£å‘ä¼ æ’­æœŸé—´è®¡ç®—çš„ç¼“å­˜ï¼Œä½ å¯ä»¥å®ç°åå‘ä¼ æ’­ã€‚

**é—®é¢˜**ï¼šå®ç°å‡½æ•°`backward_propagationï¼ˆï¼‰`ã€‚

**è¯´æ˜**ï¼š
åå‘ä¼ æ’­é€šå¸¸æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€éš¾ï¼ˆæœ€æ•°å­¦ï¼‰çš„éƒ¨åˆ†ã€‚ä¸ºäº†å¸®åŠ©ä½ æ›´å¥½åœ°äº†è§£ï¼Œæˆ‘ä»¬æä¾›äº†åå‘ä¼ æ’­è¯¾ç¨‹çš„å¹»ç¯ç‰‡ã€‚ä½ å°†è¦ä½¿ç”¨æ­¤å¹»ç¯ç‰‡å³ä¾§çš„å…­ä¸ªæ–¹ç¨‹å¼ä»¥æ„å»ºå‘é‡åŒ–å®ç°ã€‚

è¿™äº›éƒ½æ˜¯é€šè¿‡åˆç†è¿ç”¨å¯¼æ•°å’Œé“¾å¼æ³•åˆ™æ¥è®¡ç®—ã€‚

![Image Name](https://cdn.kesci.com/upload/image/q17hcd4yra.png?imageView2/0/w/960/h/960)

- âˆ— è¡¨ç¤ºå…ƒç´ ä¹˜æ³•ï¼ˆç”±é“¾å¼æ³•åˆ™å¾—æ¥ï¼‰ã€‚
- æ·±åº¦å­¦ä¹ ä¸­å¾ˆå¸¸è§çš„ç¼–ç è¡¨ç¤ºæ–¹æ³•ï¼š
  - dW1 =![image-20240528205543061](images/image-20240528205543061.png)
  - db1 = ![image-20240528205547650](images/image-20240528205547650.png)
  - dW2 = ![image-20240528205551000](images/image-20240528205551000.png)
  - db2 = ![image-20240528205554231](images/image-20240528205554231.png)
- æç¤ºï¼š
    -è¦è®¡ç®—dZ1ï¼Œä½ é¦–å…ˆéœ€è¦è®¡ç®—![image-20240528205602032](images/image-20240528205602032.png)ã€‚ç”±äº![image-20240528205607729](images/image-20240528205607729.png)æ˜¯tanhæ¿€æ´»å‡½æ•°ï¼Œå› æ­¤å¦‚æœ![image-20240528205616540](images/image-20240528205616540.png)ğ‘§) åˆ™![image-20240528205622711](images/image-20240528205622711.png)ã€‚æ‰€ä»¥ä½ å¯ä»¥ä½¿ç”¨`(1 - np.power(A1, 2))`è®¡ç®—![image-20240528205632961](images/image-20240528205632961.png)ã€‚

```python
# GRADED FUNCTION: backward_propagation

def backward_propagation(parameters, cache, X, Y):
    """
    Implement the backward propagation using the instructions above.
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
    X -- input data of shape (2, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    grads -- python dictionary containing your gradients with respect to different parameters
    """
    m = X.shape[1]
    
    # First, retrieve W1 and W2 from the dictionary "parameters".
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    ### END CODE HERE ###
        
    # Retrieve also A1 and A2 from dictionary "cache".
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A1 = cache["A1"]
    A2 = cache["A2"]
    ### END CODE HERE ###
    
    # Backward propagation: calculate dW1, db1, dW2, db2. 
    ### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)
    ### END CODE HERE ###
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads

parameters, cache, X_assess, Y_assess = backward_propagation_test_case()

grads = backward_propagation(parameters, cache, X_assess, Y_assess)
print ("dW1 = "+ str(grads["dW1"]))
print ("db1 = "+ str(grads["db1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("db2 = "+ str(grads["db2"]))
```

outputï¼š

```PYTHON
dW1 = [[ 0.01018708 -0.00708701]
 [ 0.00873447 -0.0060768 ]
 [-0.00530847  0.00369379]
 [-0.02206365  0.01535126]]
db1 = [[-0.00069728]
 [-0.00060606]
 [ 0.000364  ]
 [ 0.00151207]]
dW2 = [[ 0.00363613  0.03153604  0.01162914 -0.01318316]]
db2 = [[0.06589489]]
```

**é—®é¢˜**ï¼šå®ç°å‚æ•°æ›´æ–°ã€‚ ä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼Œä½ å¿…é¡»ä½¿ç”¨ï¼ˆdW1ï¼Œdb1ï¼ŒdW2ï¼Œdb2ï¼‰æ‰èƒ½æ›´æ–°ï¼ˆW1ï¼Œb1ï¼ŒW2ï¼Œb2ï¼‰ã€‚

**ä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™è§„åˆ™**ï¼š![image-20240528205907166](images/image-20240528205907166.png)å…¶ä¸­ğ›¼æ˜¯å­¦ä¹ ç‡ï¼Œè€Œğœƒ ä»£è¡¨ä¸€ä¸ªå‚æ•°ã€‚

**å›¾ç¤º**ï¼šå…·æœ‰è‰¯å¥½çš„å­¦ä¹ é€Ÿç‡ï¼ˆæ”¶æ•›ï¼‰å’Œè¾ƒå·®çš„å­¦ä¹ é€Ÿç‡ï¼ˆå‘æ•£ï¼‰çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚ å›¾ç‰‡ç”±Adam Harleyæä¾›ã€‚

![Image Name](https://cdn.kesci.com/upload/image/q17hh4otzu.gif?imageView2/0/w/960/h/960)

![Image Name](https://cdn.kesci.com/upload/image/q17hharbth.gif?imageView2/0/w/960/h/960)

```python
# GRADED FUNCTION: update_parameters

def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    Updates parameters using the gradient descent update rule given above
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients 
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """
    # Retrieve each parameter from the dictionary "parameters"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    ### END CODE HERE ###
    
    # Retrieve each gradient from the dictionary "grads"
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    ## END CODE HERE ###
    
    # Update rule for each parameter
    ### START CODE HERE ### (â‰ˆ 4 lines of code)
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    ### END CODE HERE ###
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters

parameters, grads = update_parameters_test_case()
parameters = update_parameters(parameters, grads)

print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
```

### 4.4nn_model()é›†æˆ

```python
# GRADED FUNCTION: nn_model

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- Number of iterations in gradient descent loop
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- parameters learnt by the model. They can then be used to predict.
    """
    
    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".
    ### START CODE HERE ### (â‰ˆ 5 lines of code)
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    ### END CODE HERE ###
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
         
        ### START CODE HERE ### (â‰ˆ 4 lines of code)
        # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".
        A2, cache = forward_propagation(X, parameters)
        
        # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".
        cost = compute_cost(A2, Y, parameters)
 
        # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".
        grads = backward_propagation(parameters, cache, X, Y)
 
        # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".
        parameters = update_parameters(parameters, grads)
        
        ### END CODE HERE ###
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters

X_assess, Y_assess = nn_model_test_case()

parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=False)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[-4.18503197  5.33214315]
 [-7.52988635  1.24306559]
 [-4.19302427  5.32627154]
 [ 7.52984762 -1.24308746]]
b1 = [[ 2.32926944]
 [ 3.79460252]
 [ 2.33002498]
 [-3.79466751]]
W2 = [[-6033.83668723 -6008.12983227 -6033.10091631  6008.06624417]]
b2 = [[-52.66610924]]
```

### 4.5- é¢„æµ‹[Â¶](https://www.heywhale.com/api/notebooks/5e85d6bf95b029002ca7e7e6/RenderedContent?cellcomment=1&cellbookmark=1#4.5--é¢„æµ‹)

**é—®é¢˜**ï¼šä½¿ç”¨ä½ çš„æ¨¡å‹é€šè¿‡æ„å»ºpredict()å‡½æ•°è¿›è¡Œé¢„æµ‹ã€‚
ä½¿ç”¨æ­£å‘ä¼ æ’­æ¥é¢„æµ‹ç»“æœã€‚

**æç¤º**ï¼š 
$$
y_{prediction} = \mathbb 1 \text{{activation > 0.5}} = \begin{cases}  
      1 & \text{if}\ activation > 0.5 \\  
      0 & \text{otherwise}  
    \end{cases}
$$

ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³åŸºäºé˜ˆå€¼å°†çŸ©é˜µXè®¾ä¸º0å’Œ1ï¼Œåˆ™å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š `X_new = (X > threshold)`

```python
# GRADED FUNCTION: predict

def predict(parameters, X):
    """
    Using the learned parameters, predicts a class for each example in X
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- input data of size (n_x, m)
    
    Returns
    predictions -- vector of predictions of our model (red: 0 / blue: 1)
    """
    
    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.
  ### START CODE HERE ### (â‰ˆ 2 lines of code)
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)#æ­¤å‡½æ•°çš„ä½œç”¨æ˜¯å°†è¾“å…¥æ•°ç»„ä¸­çš„å…ƒç´ å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„æ•´æ•°æˆ–æŒ‡å®šçš„å°æ•°ä½æ•°ã€‚
    ### END CODE HERE ###
    
    return predictions


parameters, X_assess = predict_test_case()

predictions = predict(parameters, X_assess)
print("predictions mean = " + str(np.mean(predictions)))#è®¡ç®— predictions æ•°ç»„ä¸­æ‰€æœ‰å…ƒç´ çš„å¹³å‡å€¼ï¼Œå³å„ä¸ªç‰¹å¾æ‰€ç»™å‡ºçš„é¢„æµ‹å€¼çš„å¹³å‡å€¼ã€‚
```

outputï¼š

```PYTHON
predictions mean = 0.6666666666666666
```

ç°åœ¨è¿è¡Œæ¨¡å‹ä»¥æŸ¥çœ‹å…¶å¦‚ä½•åœ¨äºŒç»´æ•°æ®é›†ä¸Šè¿è¡Œã€‚ è¿è¡Œä»¥ä¸‹ä»£ç ä»¥ä½¿ç”¨å«æœ‰![image-20240528211628033](images/image-20240528211628033.png)éšè—å•å…ƒçš„å•ä¸ªéšè—å±‚æµ‹è¯•æ¨¡å‹ã€‚

```python
# Build a model with a n_h-dimensional hidden layer
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)

# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)#ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
plt.title("Decision Boundary for hidden layer size " + str(4))
```

outputï¼š

```python
Cost after iteration 0: 0.693048
Cost after iteration 1000: 0.288083
Cost after iteration 2000: 0.254385
Cost after iteration 3000: 0.233864
Cost after iteration 4000: 0.226792
Cost after iteration 5000: 0.222644
Cost after iteration 6000: 0.219731
Cost after iteration 7000: 0.217504
Cost after iteration 8000: 0.219467
Cost after iteration 9000: 0.218561
```

![image-20240528211828808](images/image-20240528211828808.png)

```python
# Print accuracy
predictions = predict(parameters, X)
print ('Accuracy: %d' % float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100) + '%')
```

outputï¼š

```python
Accuracy: 90%
```

#### lambdaå‡½æ•°

- **Lambdaå‡½æ•°**ï¼Œä¹Ÿç§°ä¸º**åŒ¿åå‡½æ•°**ï¼Œæ˜¯Pythonä¸­ä¸€ç§ç®€æ´çš„å‡½æ•°å½¢å¼ã€‚å®ƒå…è®¸æ‚¨åœ¨éœ€è¦å‡½æ•°ä½œä¸ºå‚æ•°æˆ–è¿”å›å€¼çš„åœ°æ–¹å¿«é€Ÿå®šä¹‰ä¸€ä¸ªç®€çŸ­çš„å‡½æ•°ã€‚ä¸‹é¢è®©æˆ‘è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼š

  1. **Lambdaå‡½æ•°çš„è¯­æ³•**ï¼š

     - Lambdaå‡½æ•°çš„è¯­æ³•åªåŒ…å«ä¸€ä¸ªè¡¨è¾¾å¼ï¼Œå½¢å¼å¦‚ä¸‹ï¼š

       ```
       lambda [arg1 [, arg2, ...]]: expression
       ```

       

     - å…¶ä¸­ï¼Œ`lambda` æ˜¯Pythonçš„å…³é”®å­—ï¼Œ`[arg...]` å’Œ `expression` ç”±ç”¨æˆ·è‡ªå®šä¹‰ã€‚

  2. **Lambdaå‡½æ•°çš„ç‰¹ç‚¹**ï¼š

     - **åŒ¿åæ€§**ï¼šLambdaå‡½æ•°æ²¡æœ‰åå­—ï¼Œé€šå¸¸ç”¨äºç®€å•çš„æ“ä½œã€‚
     - **è¾“å…¥å’Œè¾“å‡º**ï¼šè¾“å…¥æ˜¯ä¼ å…¥åˆ°å‚æ•°åˆ—è¡¨çš„å€¼ï¼Œè¾“å‡ºæ˜¯æ ¹æ®è¡¨è¾¾å¼è®¡ç®—å¾—åˆ°çš„å€¼ã€‚
     - **å‘½åç©ºé—´**ï¼šLambdaå‡½æ•°æ‹¥æœ‰è‡ªå·±çš„å‘½åç©ºé—´ï¼Œä¸èƒ½è®¿é—®å‚æ•°åˆ—è¡¨ä¹‹å¤–æˆ–å…¨å±€å‘½åç©ºé—´ä¸­çš„å‚æ•°ã€‚

  3. **å¸¸è§çš„Lambdaå‡½æ•°ç¤ºä¾‹**ï¼š

     - `lambda x, y: x * y`ï¼šè¾“å…¥æ˜¯xå’Œyï¼Œè¾“å‡ºæ˜¯å®ƒä»¬çš„ç§¯ã€‚
     - `lambda: None`ï¼šæ²¡æœ‰è¾“å…¥å‚æ•°ï¼Œè¾“å‡ºæ˜¯Noneã€‚
     - `lambda *args: sum(args)`ï¼šè¾“å…¥æ˜¯ä»»æ„ä¸ªæ•°çš„å‚æ•°ï¼Œè¾“å‡ºæ˜¯å®ƒä»¬çš„å’Œã€‚
     - `lambda **kwargs: 1`ï¼šè¾“å…¥æ˜¯ä»»æ„é”®å€¼å¯¹å‚æ•°ï¼Œè¾“å‡ºæ˜¯1ã€‚

### 4.6- è°ƒæ•´éšè—å±‚å¤§å°

```python
# This may take about 2 minutes to run

plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 10, 20]
for i, n_h in enumerate(hidden_layer_sizes):#enumerate(hidden_layer_sizes) åˆ›å»ºäº†ä¸€ä¸ªå¯è¿­ä»£çš„å¯¹è±¡ï¼Œå®ƒä¼šè¿”å›ä¸€ä¸ª (index, value) çš„å…ƒç»„ã€‚
#åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œi æ˜¯ç´¢å¼•ï¼Œn_h æ˜¯å¯¹åº”çš„éšè—å±‚å¤§å°ã€‚
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))
```

outputï¼š

```python
Accuracy for 1 hidden units: 67.5 %
Accuracy for 2 hidden units: 67.25 %
Accuracy for 3 hidden units: 90.75 %
Accuracy for 4 hidden units: 90.5 %
Accuracy for 5 hidden units: 91.25 %
Accuracy for 10 hidden units: 90.25 %
Accuracy for 20 hidden units: 90.5 %
```

![image-20240528213230431](images/image-20240528213230431.png)

![image-20240528213240650](images/image-20240528213240650.png)![image-20240528213247686](images/image-20240528213247686.png)

**è¯´æ˜**ï¼š

- è¾ƒå¤§çš„æ¨¡å‹ï¼ˆå…·æœ‰æ›´å¤šéšè—çš„å•å…ƒï¼‰èƒ½å¤Ÿæ›´å¥½åœ°æ‹Ÿåˆè®­ç»ƒé›†ï¼Œç›´åˆ°æœ€ç»ˆæœ€å¤§çš„æ¨¡å‹è¿‡æ‹Ÿåˆæ•°æ®ä¸ºæ­¢ã€‚
- éšè—å±‚çš„æœ€ä½³å¤§å°ä¼¼ä¹åœ¨n_h = 5å·¦å³ã€‚çš„ç¡®ï¼Œæ­¤å€¼ä¼¼ä¹å¾ˆå¥½åœ°æ‹Ÿåˆäº†æ•°æ®ï¼Œè€Œåˆä¸ä¼šå¼•èµ·æ˜æ˜¾çš„è¿‡åº¦æ‹Ÿåˆã€‚
- ç¨åä½ è¿˜å°†å­¦ä¹ æ­£åˆ™åŒ–ï¼Œå¸®åŠ©æ„å»ºæ›´å¤§çš„æ¨¡å‹ï¼ˆä¾‹å¦‚n_h = 50ï¼‰è€Œä¸ä¼šè¿‡åº¦æ‹Ÿåˆã€‚

# æ·±åº¦ç¥ç»ç½‘ç»œ

- åœ¨æ­¤ä½œä¸šä¸­ï¼Œä½ å°†å®ç°æ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œæ‰€éœ€çš„æ‰€æœ‰å‡½æ•°ã€‚

**å®Œæˆæ­¤ä»»åŠ¡åï¼Œä½ å°†èƒ½å¤Ÿï¼š**

- ä½¿ç”¨ReLUç­‰éçº¿æ€§å•ä½æ¥æ”¹å–„æ¨¡å‹
- å»ºç«‹æ›´æ·±çš„ç¥ç»ç½‘ç»œï¼ˆå…·æœ‰1ä¸ªä»¥ä¸Šçš„éšè—å±‚ï¼‰
- å®ç°ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ç¥ç»ç½‘ç»œç±»

## ç¬¦å·è¯´æ˜

**ç¬¦å·è¯´æ˜**ï¼š

- ä¸Šæ ‡[ğ‘™] è¡¨ç¤ºä¸![image-20240601104701164](images/image-20240601104701164.png)å±‚ç›¸å…³çš„æ•°é‡ã€‚
    \- ç¤ºä¾‹ï¼š![image-20240601104706889](images/image-20240601104706889.png)æ˜¯![image-20240601104712654](images/image-20240601104712654.png)å±‚çš„æ¿€æ´»ã€‚![image-20240601104720503](images/image-20240601104720503.png)å’Œ![image-20240601104725684](images/image-20240601104725684.png)æ˜¯![image-20240601104715769](images/image-20240601104715769.png)å±‚å‚æ•°ã€‚
- ä¸Šæ ‡(ğ‘–) è¡¨ç¤ºä¸![image-20240601104733097](images/image-20240601104733097.png)ç¤ºä¾‹ç›¸å…³çš„æ•°é‡ã€‚
    \- ç¤ºä¾‹ï¼š![image-20240601104738064](images/image-20240601104738064.png)æ˜¯ç¬¬![image-20240601104742332](images/image-20240601104742332.png) çš„è®­ç»ƒæ•°æ®ã€‚
- ä¸‹æ ‡ğ‘– è¡¨ç¤º![image-20240601104747965](images/image-20240601104747965.png)çš„å‘é‡ã€‚
    \- ç¤ºä¾‹ï¼š![image-20240601104805504](images/image-20240601104805504.png) è¡¨ç¤º![image-20240601104810415](images/image-20240601104810415.png)å±‚æ¿€æ´»çš„![image-20240601104814768](images/image-20240601104814768.png) è¾“å…¥ã€‚

## 1-å®‰è£…åŒ…

```PYTHON
import numpy as np
import h5py
import matplotlib.pyplot as plt
from lib.testCases_v2 import *
from lib.dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward

plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'


np.random.seed(1)
```

- åˆå§‹åŒ–ä¸¤å±‚çš„ç½‘ç»œå’Œğ¿å±‚çš„ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚
- å®ç°æ­£å‘ä¼ æ’­æ¨¡å—ï¼ˆåœ¨ä¸‹å›¾ä¸­ä»¥ç´«è‰²æ˜¾ç¤ºï¼‰ã€‚
     \- å®Œæˆæ¨¡å‹æ­£å‘ä¼ æ’­æ­¥éª¤çš„LINEARéƒ¨åˆ†ï¼ˆ![image-20240601110505018](images/image-20240601110505018.png)ï¼‰ã€‚
     \- æä¾›ä½¿ç”¨çš„ACTIVATIONå‡½æ•°ï¼ˆrelu / Sigmoidï¼‰ã€‚
     \- å°†å‰ä¸¤ä¸ªæ­¥éª¤åˆå¹¶ä¸ºæ–°çš„[LINEAR-> ACTIVATION]å‰å‘å‡½æ•°ã€‚
     \- å †å [LINEAR-> RELU]æ­£å‘å‡½æ•°L-1æ¬¡ï¼ˆç¬¬1åˆ°L-1å±‚ï¼‰ï¼Œå¹¶åœ¨æœ«å°¾æ·»åŠ [LINEAR-> SIGMOID]ï¼ˆæœ€åçš„ğ¿å±‚)ã€‚è¿™åˆæˆäº†ä¸€ä¸ªæ–°çš„L_model_forwardå‡½æ•°ã€‚
- è®¡ç®—æŸå¤±ã€‚
- å®ç°åå‘ä¼ æ’­æ¨¡å—ï¼ˆåœ¨ä¸‹å›¾ä¸­ä»¥çº¢è‰²è¡¨ç¤ºï¼‰ã€‚
    \- å®Œæˆæ¨¡å‹åå‘ä¼ æ’­æ­¥éª¤çš„LINEARéƒ¨åˆ†ã€‚
    \- æä¾›çš„ACTIVATEå‡½æ•°çš„æ¢¯åº¦ï¼ˆrelu_backward / sigmoid_backwardï¼‰
    \- å°†å‰ä¸¤ä¸ªæ­¥éª¤ç»„åˆæˆæ–°çš„[LINEAR-> ACTIVATION]åå‘å‡½æ•°ã€‚
    \- å°†[LINEAR-> RELU]å‘åå †å L-1æ¬¡ï¼Œå¹¶åœ¨æ–°çš„L_model_backwardå‡½æ•°ä¸­åå‘æ·»åŠ [LINEAR-> SIGMOID]
- æœ€åæ›´æ–°å‚æ•°ã€‚

![image-20240601110810268](images/image-20240601110810268.png)

**æ³¨æ„**ï¼šå¯¹äºæ¯ä¸ªæ­£å‘å‡½æ•°ï¼Œéƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„åå‘å‡½æ•°ã€‚ è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨æ­£å‘ä¼ æ’­æ¨¡å—çš„æ¯ä¸€æ­¥éƒ½å°†ä¸€äº›å€¼å­˜å‚¨åœ¨ç¼“å­˜ä¸­çš„åŸå› ã€‚ç¼“å­˜çš„å€¼å¯ç”¨äºè®¡ç®—æ¢¯åº¦ã€‚ ç„¶åï¼Œåœ¨åå‘ä¼ å¯¼æ¨¡å—ä¸­ï¼Œä½ å°†ä½¿ç”¨ç¼“å­˜çš„å€¼æ¥è®¡ç®—æ¢¯åº¦ã€‚ æ­¤ä½œä¸šå°†æŒ‡å¯¼è¯´æ˜å¦‚ä½•æ‰§è¡Œè¿™äº›æ­¥éª¤ã€‚

## 2-åˆå§‹åŒ–

â€‹	é¦–å…ˆç¼–å†™ä¸¤ä¸ªè¾…åŠ©å‡½æ•°ç”¨æ¥åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°ã€‚ ç¬¬ä¸€ä¸ªå‡½æ•°å°†ç”¨äºåˆå§‹åŒ–ä¸¤å±‚æ¨¡å‹çš„å‚æ•°ã€‚ ç¬¬äºŒä¸ªå°†æŠŠåˆå§‹åŒ–è¿‡ç¨‹æ¨å¹¿åˆ°ğ¿å±‚æ¨¡å‹ä¸Šã€‚

### 2.1-ä¸¤å±‚ç¥ç»ç½‘ç»œ

åˆ›å»ºå¹¶åˆå§‹åŒ–2å±‚ç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚

**è¯´æ˜**ï¼š

- æ¨¡å‹çš„ç»“æ„ä¸ºï¼š*LINEAR -> RELU -> LINEAR -> SIGMOID*ã€‚
- éšæœºåˆå§‹åŒ–æƒé‡çŸ©é˜µã€‚ ç¡®ä¿å‡†ç¡®çš„ç»´åº¦ï¼Œä½¿ç”¨`np.random.randnï¼ˆshapeï¼‰* 0.01`ã€‚
- å°†åå·®åˆå§‹åŒ–ä¸º0ã€‚ ä½¿ç”¨`np.zerosï¼ˆshapeï¼‰`ã€‚

```python
# GRADED FUNCTION: initialize_parameters

def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    parameters -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """
    
    np.random.seed(1)
    
### START CODE HERE ### (â‰ˆ 4 lines of code)
#np.random.randn()å‡½æ•°ç”¨äºç”Ÿæˆä¸€ä¸ªç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰çš„éšæœºæ ·æœ¬æ•°ç»„ã€‚è¿™é‡Œçš„randnå‡½æ•°åé¢çš„å‚æ•°n_hå’Œn_xæŒ‡å®šäº†æ•°ç»„çš„å½¢çŠ¶ï¼Œå³ç”Ÿæˆä¸€ä¸ªn_hè¡Œn_xåˆ—çš„äºŒç»´æ•°ç»„ã€‚
    W1 = np.random.randn(n_h, n_x)*0.01
    b1 = np.zeros((n_h,1))
    W2 = np.random.randn(n_y, n_h)*0.01
    b2 = np.zeros((n_y,1))
    ### END CODE HERE ###
    
    
    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters    
parameters = initialize_parameters(2,2,1)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

outputï¼š

```PYTHON
W1 = [[ 0.01624345 -0.00611756]
 [-0.00528172 -0.01072969]]
b1 = [[0.]
 [0.]]
W2 = [[ 0.00865408 -0.02301539]]
b2 = [[0.]]
```

### 2.2-Lå±‚ç¥ç»ç½‘ç»œ

å¯¹äºLå±‚æ·±åº¦ç¥ç»ç½‘ç»œçš„åˆå§‹åŒ–å› ä¸ºå­˜åœ¨æ›´å¤šçš„æƒé‡çŸ©é˜µå’Œåå·®å‘é‡ã€‚ å®Œæˆ `initialize_parameters_deep`åï¼Œåº”ç¡®ä¿å„å±‚ä¹‹é—´çš„ç»´åº¦åŒ¹é…ã€‚ ![image-20240601112143301](images/image-20240601112143301.png)æ˜¯ğ‘™å±‚ä¸­çš„ç¥ç»å…ƒæ•°é‡ã€‚ å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬è¾“å…¥çš„ ğ‘‹ çš„å¤§å°ä¸º(12288,209)ï¼ˆä»¥ğ‘š=209ä¸ºä¾‹)ï¼Œåˆ™ï¼š

![image-20240601112456025](images/image-20240601112456025.png)

å½“æˆ‘ä»¬åœ¨pythonä¸­è®¡ç®—ğ‘Šğ‘‹+ğ‘æ—¶ï¼Œä½¿ç”¨å¹¿æ’­ï¼Œæ¯”å¦‚ï¼š
$$
W = \begin{bmatrix}      j  & k  & l\\      m  & n & o \\      p  & q & r     \end{bmatrix}\;\;\; X = \begin{bmatrix}      a  & b  & c\\      d  & e & f \\      g  & h & i    \end{bmatrix} \;\;\; b =\begin{bmatrix}      s  \\      t  \\      u   \end{bmatrix}\tag{2}
$$
Then ğ‘Šğ‘‹+ğ‘ will be:
$$
WX + b = \begin{bmatrix}      (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\      (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\      (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u   \end{bmatrix}\tag{3}
$$
**ç»ƒä¹ **ï¼šå®ç°Lå±‚ç¥ç»ç½‘ç»œçš„åˆå§‹åŒ–ã€‚

**è¯´æ˜**ï¼š

- æ¨¡å‹çš„ç»“æ„ä¸º *[LINEAR -> RELU] Ã— (L-1) -> LINEAR -> SIGMOID*ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œğ¿âˆ’1å±‚ä½¿ç”¨ReLUä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œæœ€åä¸€å±‚é‡‡ç”¨sigmoidæ¿€æ´»å‡½æ•°è¾“å‡ºã€‚

- éšæœºåˆå§‹åŒ–æƒé‡çŸ©é˜µã€‚ä½¿ç”¨`np.random.randï¼ˆshapeï¼‰* 0.01`ã€‚

- é›¶åˆå§‹åŒ–åå·®ã€‚ä½¿ç”¨`np.zerosï¼ˆshapeï¼‰`ã€‚

- æˆ‘ä»¬å°†åœ¨ä¸åŒçš„layer_dimså˜é‡ä¸­å­˜å‚¨ğ‘›[ğ‘™]ï¼Œå³ä¸åŒå±‚ä¸­çš„ç¥ç»å…ƒæ•°ã€‚ä¾‹å¦‚ï¼Œä¸Šå‘¨â€œäºŒç»´æ•°æ®åˆ†ç±»æ¨¡å‹â€çš„`layer_dims`ä¸º[2,4,1]ï¼šå³æœ‰ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªéšè—å±‚åŒ…å«4ä¸ªéšè—å•å…ƒï¼Œä¸€ä¸ªè¾“å‡ºå±‚åŒ…å«1ä¸ªè¾“å‡ºå•å…ƒã€‚å› æ­¤ï¼Œ`W1`çš„ç»´åº¦ä¸ºï¼ˆ4,2ï¼‰ï¼Œ`b1`çš„ç»´åº¦ä¸ºï¼ˆ4,1ï¼‰ï¼Œ`W2`çš„ç»´åº¦ä¸ºï¼ˆ1,4ï¼‰ï¼Œè€Œ`b2`çš„ç»´åº¦ä¸ºï¼ˆ1,1ï¼‰ã€‚ç°åœ¨ä½ å°†æŠŠå®ƒåº”ç”¨åˆ°ğ¿å±‚ï¼

- è¿™æ˜¯

  ğ¿=1ï¼ˆä¸€å±‚ç¥ç»ç½‘ç»œï¼‰çš„å®ç°ã€‚ä»¥å¯å‘ä½ å¦‚ä½•å®ç°é€šç”¨çš„ç¥ç»ç½‘ç»œï¼ˆLå±‚ç¥ç»ç½‘ç»œï¼‰ã€‚

  ```python
  if L == 1:  
        parameters["W" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01  
        parameters["b" + str(L)] = np.zeros((layer_dims[1], 1))
  ```

```PYTHON
# GRADED FUNCTION: initialize_parameters_deep

def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the dimensions of each layer in our network
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims)            # number of layers in the network

    for l in range(1, L):
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))
        ### END CODE HERE ###
        
        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))
        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))

        
    return parameters
parameters = initialize_parameters_deep([5,4,3])
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))
```

output:

```PYTHON
W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]
 [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]
 [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]
 [-0.01023785 -0.00712993  0.00625245 -0.00160513]
 [-0.00768836 -0.00230031  0.00745056  0.01976111]]
b2 = [[0.]
 [0.]
 [0.]]
```

## 3-æ­£å‘ä¼ æ’­æ¨¡å—

### 3.1-çº¿æ€§æ­£å‘

æ¥ä¸‹æ¥å°†æ‰§è¡Œæ­£å‘ä¼ æ’­æ¨¡å—ã€‚ é¦–å…ˆå®ç°ä¸€äº›åŸºæœ¬å‡½æ•°ï¼Œç”¨äºç¨åçš„æ¨¡å‹å®ç°ã€‚æŒ‰ä»¥ä¸‹é¡ºåºå®Œæˆä¸‰ä¸ªå‡½æ•°ï¼š

- LINEAR
- LINEAR -> ACTIVATIONï¼Œå…¶ä¸­æ¿€æ´»å‡½æ•°é‡‡ç”¨ReLUæˆ–Sigmoidã€‚
- [LINEAR -> RELU] Ã— (L-1) -> LINEAR -> SIGMOIDï¼ˆæ•´ä¸ªæ¨¡å‹ï¼‰

çº¿æ€§æ­£å‘æ¨¡å—ï¼ˆåœ¨æ‰€æœ‰æ•°æ®ä¸­å‡è¿›è¡Œå‘é‡åŒ–ï¼‰çš„è®¡ç®—æŒ‰ç…§ä»¥ä¸‹å…¬å¼ï¼š
$$
Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}
$$
å…¶ä¸­![image-20240601114525732](images/image-20240601114525732.png)

è¯¥å•å…ƒçš„æ•°å­¦è¡¨ç¤ºä¸º ![image-20240601114550590](images/image-20240601114550590.png)ï¼Œä½ å¯èƒ½ä¼šå‘ç°`np.dotï¼ˆï¼‰`æœ‰ç”¨ã€‚ å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ™å¯ä»¥printï¼ˆ`W.shape`)æŸ¥çœ‹ä¿®æ”¹ã€‚

```PYTHON
# GRADED FUNCTION: linear_forward

def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter 
    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """
    
    ### START CODE HERE ### (â‰ˆ 1 line of code)
    Z = np.dot(W,A) + b
    ### END CODE HERE ###
    
    assert(Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)
    
    return Z, cache

A, W, b = linear_forward_test_case()

Z, linear_cache = linear_forward(A, W, b)
print("Z = " + str(Z))
```

outputï¼š

```PYTHON
Z = [[ 3.26295337 -1.23429987]]
```

### 3.2-æ­£å‘çº¿æ€§æ¿€æ´»

ä½¿ç”¨ä¸¤ä¸ªæ¿€æ´»å‡½æ•°ï¼š

**Sigmoid**ï¼š![image-20240601115454788](images/image-20240601115454788.png) ã€‚ è¯¥å‡½æ•°è¿”å›**ä¸¤é¡¹å€¼**ï¼šæ¿€æ´»å€¼"`a`"å’ŒåŒ…å«"`Z`"çš„"`cache`"ï¼ˆè¿™æ˜¯æˆ‘ä»¬å°†é¦ˆå…¥åˆ°ç›¸åº”çš„åå‘å‡½æ•°çš„å†…å®¹)ã€‚ 

```python
A, activation_cache = sigmoid(Z)
```

**ReLU**ï¼šReLuçš„æ•°å­¦å…¬å¼ä¸º![image-20240601115539959](images/image-20240601115539959.png)ã€‚æˆ‘ä»¬ä¸ºä½ æä¾›äº†`relu`å‡½æ•°ã€‚ è¯¥å‡½æ•°è¿”å›**ä¸¤é¡¹å€¼**ï¼šæ¿€æ´»å€¼â€œ`A`â€å’ŒåŒ…å«â€œ`Z`â€çš„â€œ`cache`â€ï¼ˆè¿™æ˜¯æˆ‘ä»¬å°†é¦ˆå…¥åˆ°ç›¸åº”çš„åå‘å‡½æ•°çš„å†…å®¹)ã€‚ ä½ å¯ä»¥æŒ‰ä¸‹è¿°æ–¹å¼å¾—åˆ°ä¸¤é¡¹å€¼ï¼š

```python
A, activation_cache = relu(Z)
```

ä¸ºäº†æ›´åŠ æ–¹ä¾¿ï¼Œæˆ‘ä»¬æŠŠä¸¤ä¸ªå‡½æ•°ï¼ˆçº¿æ€§å’Œæ¿€æ´»ï¼‰ç»„åˆä¸ºä¸€ä¸ªå‡½æ•°ï¼ˆLINEAR-> ACTIVATIONï¼‰ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå‡½æ•°ç”¨ä»¥æ‰§è¡ŒLINEARæ­£å‘æ­¥éª¤å’ŒACTIVATIONæ­£å‘æ­¥éª¤ã€‚

**ç»ƒä¹ **ï¼šå®ç° *LINEAR->ACTIVATION* å±‚çš„æ­£å‘ä¼ æ’­ã€‚ æ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š![image-20240601115658695](images/image-20240601115658695.png)ï¼Œå…¶ä¸­æ¿€æ´»"g" å¯ä»¥æ˜¯sigmoidï¼ˆï¼‰æˆ–reluï¼ˆï¼‰ã€‚ ä½¿ç”¨linear_forwardï¼ˆ)å’Œæ­£ç¡®çš„æ¿€æ´»å‡½æ•°ã€‚

```python
# GRADED FUNCTION: linear_activation_forward

def linear_activation_forward(A_prev, W, b, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer

    Arguments:
    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)
    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    Returns:
    A -- the output of the activation function, also called the post-activation value 
    cache -- a python dictionary containing "linear_cache" and "activation_cache";
             stored for computing the backward pass efficiently
    """
    
    if activation == "sigmoid":
        # Inputs: "A_prev, W, b". Outputs: "A, activation_cache".
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        Z, linear_cache = linear_forward(A_prev,W,b)
        A, activation_cache = sigmoid(Z)
        ### END CODE HERE ###
    
    elif activation == "relu":
        # Inputs: "A_prev, W, b". Outputs: "A, activation_cache".
        ### START CODE HERE ### (â‰ˆ 2 lines of code)
        Z, linear_cache = linear_forward(A_prev,W,b)
        A, activation_cache = relu(Z)
        ### END CODE HERE ###
    
    assert (A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache

A_prev, W, b = linear_activation_forward_test_case()

A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = "sigmoid")
print("With sigmoid: A = " + str(A))

A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = "relu")
print("With ReLU: A = " + str(A))
```

outputï¼š

```PYTHON
With sigmoid: A = [[0.96890023 0.11013289]]
With ReLU: A = [[3.43896131 0.        ]]
```

### 3.3-Lå±‚æ¨¡å‹

ä¸ºäº†æ–¹ä¾¿å®ç°ğ¿å±‚ç¥ç»ç½‘ç»œï¼Œä½ å°†éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥å¤åˆ¶å‰ä¸€ä¸ªå‡½æ•°ï¼ˆä½¿ç”¨RELUçš„`linear_activation_forward`ï¼‰ğ¿âˆ’1æ¬¡ï¼Œä»¥åŠå¤åˆ¶å¸¦æœ‰SIGMOIDçš„`linear_activation_forward`ã€‚

*[LINEAR -> RELU] Ã— (L-1) -> LINEAR -> SIGMOID* æ¨¡å‹

![image-20240601120522108](images/image-20240601120522108.png)

**è¯´æ˜**ï¼šåœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œå˜é‡`AL`è¡¨ç¤º![image-20240601120801407](images/image-20240601120801407.png)ï¼ˆæœ‰æ—¶ä¹Ÿç§°ä¸º`Yhat`ï¼Œå³ğ‘Œ^ã€‚)

**æç¤º**ï¼š

- ä½¿ç”¨ä½ å…ˆå‰ç¼–å†™çš„å‡½æ•°
- ä½¿ç”¨forå¾ªç¯å¤åˆ¶[LINEAR-> RELU]ï¼ˆL-1ï¼‰æ¬¡
- ä¸è¦å¿˜è®°åœ¨â€œcacheâ€åˆ—è¡¨ä¸­æ›´æ–°ç¼“å­˜ã€‚ è¦å°†æ–°å€¼ `c`æ·»åŠ åˆ°`list`ä¸­ï¼Œå¯ä»¥ä½¿ç”¨`list.append(c)`ã€‚

```python
# GRADED FUNCTION: L_model_forward

def L_model_forward(X, parameters):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    
    Arguments:
    X -- data, numpy array of shape (input size, number of examples)
    parameters -- output of initialize_parameters_deep()
    
    Returns:
    AL -- last post-activation value
    caches -- list of caches containing:
                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """

    caches = []
    A = X
    L = len(parameters) // 2                  # number of layers in the neural network
    
    # Implement [LINEAR -> RELU]*(L-1). Add "cache" to the "caches" list.
    #è¿™è¡Œä»£ç ä¼šæ‰§è¡Œ L-1 æ¬¡å¾ªç¯ï¼ŒåŒ…æ‹¬1ä¸åŒ…æ‹¬Lã€‚
    for l in range(1, L):
        A_prev = A 
         ### START CODE HERE ### (â‰ˆ 2 lines of code)
        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation = "relu")
        caches.append(cache)
        ### END CODE HERE ###
    
    # Implement LINEAR -> SIGMOID. Add "cache" to the "caches" list.
    ### START CODE HERE ### (â‰ˆ 2 lines of code)
    #æœ€åä¸€å±‚æ¿€æ´»å‡½æ•°ä½¿ç”¨sigmoidå‡½æ•°ã€‚
    #æ­¤å¤„ä¼ å…¥çš„Aæ˜¯ä¸Šä¸€å±‚çš„è¾“å‡ºã€‚
    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],activation = "sigmoid")
    caches.append(cache)
    ### END CODE HERE ###
    
    assert(AL.shape == (1,X.shape[1]))
            
    return AL, caches

X, parameters = L_model_forward_test_case()
AL, caches = L_model_forward(X, parameters)
print("AL = " + str(AL))
print("Length of caches list = " + str(len(caches)))
```

outputï¼š

```python
AL = [[0.17007265 0.2524272 ]]
Length of caches list = 2
```

## 4-æŸå¤±å‡½æ•°

è®¡ç®—æŸå¤±ï¼Œä»¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨å­¦ä¹ ã€‚

**ç»ƒä¹ **ï¼šä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—äº¤å‰ç†µæŸå¤±ğ½ï¼š
$$
-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}
$$

```python
# GRADED FUNCTION: compute_cost

def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    """
    
    m = Y.shape[1]

    # Compute loss from aL and y.
    ### START CODE HERE ### (â‰ˆ 1 lines of code)
    #æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚å¯¹è¡Œæ±‚å’Œçš„ç›®çš„æ˜¯ä¸ºäº†è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œç„¶åå°†è¿™äº›æŸå¤±åŠ èµ·æ¥å¾—åˆ°æ•´ä¸ªæ‰¹é‡çš„æ€»æŸå¤±ã€‚
    #å› æ­¤æ­¤å¤„æ˜¯axisä¸º1ï¼Œè¡¨ç¤ºå¯¹è¡Œæ±‚å’Œï¼Œå³è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œå…¶costä¹Ÿæ˜¯ä¸€ä¸ªçŸ©é˜µã€‚
    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL),axis=1,keepdims=True)
    #costæ˜¯ä¸€ä¸ªåˆ—å‘é‡
    ### END CODE HERE ###
    
    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
    assert(cost.shape == ())
    
    return cost
Y, AL = compute_cost_test_case()

print("cost = " + str(compute_cost(AL, Y)))
```

outputï¼š

```PYTHON
cost = 0.41493159961539694
```
